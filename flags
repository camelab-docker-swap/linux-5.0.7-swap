Binary file .git/index matches
Documentation/DMA-API.txt:size is the size (and should be a page-sized multiple).
Documentation/acpi/enumeration.txt:				"page-size, 32
Documentation/admin-guide/mm/hugetlbpage.rst:which function as described above for the default huge page-sized case.
Documentation/admin-guide/mm/hugetlbpage.rst:``hugepage-shm``
Documentation/admin-guide/mm/hugetlbpage.rst:	see tools/testing/selftests/vm/hugepage-shm.c
Documentation/admin-guide/mm/hugetlbpage.rst:``hugepage-mmap``
Documentation/admin-guide/mm/hugetlbpage.rst:	see tools/testing/selftests/vm/hugepage-mmap.c
Documentation/admin-guide/mm/idle_page_tracking.rst:The page-types tool in the tools/vm directory can be used to assist in this.
Documentation/admin-guide/mm/pagemap.rst:The page-types tool in the tools/vm directory can be used to query the
Documentation/admin-guide/mm/pagemap.rst:The page-types tool in the tools/vm directory can be used to query the
Documentation/admin-guide/mm/pagemap.rst:Before Linux 3.11 pagemap bits 55-60 were used for "page-shift" (which is
Documentation/admin-guide/mm/userfaultfd.rst:Vmas are not suitable for page- (or hugepage) granular fault tracking
Documentation/admin-guide/tainted-kernels.rst: 6)  ``B`` if a page-release function has found a bad page reference or
Documentation/block/biodoc.txt: Christoph Hellwig had some code that uses bios for page-io (rather than
Documentation/blockdev/zram.txt: invalid_io       the number of non-page-size-aligned I/O requests
Documentation/cgroup-v1/memcg_test.txt:	of troubles here. This is because shmem is page-cache but can be
Documentation/core-api/cachetlb.rst:	that dirty data in that page at the page->virtual mapping
Documentation/core-api/cachetlb.rst:        There is a bit set aside in page->flags (PG_arch_1) as
Documentation/core-api/cachetlb.rst:	page->mapping->i_mmap is an empty tree, just mark the architecture
Documentation/core-api/mm-api.rst:.. kernel-doc:: mm/page-writeback.c
Documentation/devicetree/bindings/display/ssd1307fb.txt:  - solomon,page-offset: Offset of pages (band of 8 pixels) that the screen is
Documentation/devicetree/bindings/eeprom/at25.txt:- at25,page-size : size of the eeprom page
Documentation/devicetree/bindings/memory-controllers/arm,pl172.txt:- mpmc,async-page-mode:	Enable asynchronous page mode.
Documentation/devicetree/bindings/memory-controllers/arm,pl172.txt:- mpmc,page-mode-read-delay:	Delay for asynchronous page mode sequential
Documentation/devicetree/bindings/memory-controllers/arm,pl172.txt:		mpmc,page-mode-read-delay = <70>;
Documentation/devicetree/bindings/memory-controllers/atmel,ebi.txt:- atmel,smc-page-mode		enable page mode if present. The provided value
Documentation/devicetree/bindings/memory-controllers/exynos-srom.txt:- samsung,srom-page-mode : if page mode is set, 4 data page mode will be configured,
Documentation/devicetree/bindings/memory-controllers/exynos-srom.txt:			samsung,srom-page-mode;
Documentation/devicetree/bindings/memory-controllers/omap-gpmc.txt: - gpmc,page-burst-access-ns: 	Multiple access word delay
Documentation/devicetree/bindings/mips/cavium/bootbus.txt:- cavium,page-mode: Optional.  If present, page mode (PAGEM) is selected.
Documentation/devicetree/bindings/mtd/gpmc-nor.txt:		gpmc,page-burst-access-ns = <6>;
Documentation/devicetree/bindings/net/gpmc-eth.txt:		gpmc,page-burst-access-ns = <6>;
Documentation/dontdiff:hugepage-mmap
Documentation/dontdiff:hugepage-shm
Documentation/dontdiff:page-types
Documentation/fb/udlfb.txt:defio support enabled, to support a page-fault based detection mechanism
Documentation/filesystems/Locking:VM_PFNMAP or VM_MIXEDMAP with a page-less entry. Expected return is
Documentation/filesystems/caching/fscache.txt:     attribute objects themselves have page-array contents.
Documentation/filesystems/orangefs.txt:At startup userspace allocates two page-size-aligned (posix_memalign)
Documentation/filesystems/proc.txt:the storage layer. This is done at page-dirtying time.
Documentation/filesystems/squashfs.txt:the page-cache in the normal way.  The cache is used to temporarily cache
Documentation/intel_txt.txt:   page-level protection.
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInReceives: https://tools.ietf.org/html/rfc1213#page-26
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInDelivers: https://tools.ietf.org/html/rfc1213#page-28
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipOutRequests: https://tools.ietf.org/html/rfc1213#page-28
Documentation/networking/snmp_counter.rst:.. _Explicit Congestion Notification: https://tools.ietf.org/html/rfc3168#page-6
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInHdrErrors: https://tools.ietf.org/html/rfc1213#page-27
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInAddrErrors: https://tools.ietf.org/html/rfc1213#page-27
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInUnknownProtos: https://tools.ietf.org/html/rfc1213#page-27
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipInDiscards: https://tools.ietf.org/html/rfc1213#page-28
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipOutDiscards: https://tools.ietf.org/html/rfc1213#page-28
Documentation/networking/snmp_counter.rst:.. _RFC1213 ipOutNoRoutes: https://tools.ietf.org/html/rfc1213#page-29
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInMsgs: https://tools.ietf.org/html/rfc1213#page-41
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutMsgs: https://tools.ietf.org/html/rfc1213#page-43
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInDestUnreachs: https://tools.ietf.org/html/rfc1213#page-41
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInTimeExcds: https://tools.ietf.org/html/rfc1213#page-41
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInParmProbs: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInSrcQuenchs: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInRedirects: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInEchos: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInEchoReps: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInTimestamps: https://tools.ietf.org/html/rfc1213#page-42
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInTimestampReps: https://tools.ietf.org/html/rfc1213#page-43
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInAddrMasks: https://tools.ietf.org/html/rfc1213#page-43
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInAddrMaskReps: https://tools.ietf.org/html/rfc1213#page-43
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutDestUnreachs: https://tools.ietf.org/html/rfc1213#page-44
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutTimeExcds: https://tools.ietf.org/html/rfc1213#page-44
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutParmProbs: https://tools.ietf.org/html/rfc1213#page-44
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutSrcQuenchs: https://tools.ietf.org/html/rfc1213#page-44
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutRedirects: https://tools.ietf.org/html/rfc1213#page-44
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutEchos: https://tools.ietf.org/html/rfc1213#page-45
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutEchoReps: https://tools.ietf.org/html/rfc1213#page-45
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutTimestamps: https://tools.ietf.org/html/rfc1213#page-45
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutTimestampReps: https://tools.ietf.org/html/rfc1213#page-45
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutAddrMasks: https://tools.ietf.org/html/rfc1213#page-45
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutAddrMaskReps: https://tools.ietf.org/html/rfc1213#page-46
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpInErrors: https://tools.ietf.org/html/rfc1213#page-41
Documentation/networking/snmp_counter.rst:.. _RFC1213 icmpOutErrors: https://tools.ietf.org/html/rfc1213#page-43
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpInSegs: https://tools.ietf.org/html/rfc1213#page-48
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpOutSegs: https://tools.ietf.org/html/rfc1213#page-48
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpActiveOpens: https://tools.ietf.org/html/rfc1213#page-47
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpPassiveOpens: https://tools.ietf.org/html/rfc1213#page-47
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpEstabResets: https://tools.ietf.org/html/rfc1213#page-48
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpAttemptFails: https://tools.ietf.org/html/rfc1213#page-48
Documentation/networking/snmp_counter.rst:.. _RFC1213 tcpOutRsts: https://tools.ietf.org/html/rfc1213#page-52
Documentation/networking/snmp_counter.rst:.. _RFC2525 2.17 section: https://tools.ietf.org/html/rfc2525#page-50
Documentation/networking/snmp_counter.rst:.. _RFC of PAWS: https://tools.ietf.org/html/rfc1323#page-17
Documentation/networking/snmp_counter.rst:.. _RFC 5961 section 3.2: https://tools.ietf.org/html/rfc5961#page-7
Documentation/networking/snmp_counter.rst:.. _RFC 5961 section 4.2: https://tools.ietf.org/html/rfc5961#page-9
Documentation/networking/snmp_counter.rst:.. _RFC 5961 section 5.2: https://tools.ietf.org/html/rfc5961#page-11
Documentation/nommu-mmap.txt:     page-aligned.  This is because XIP may take place, and the data may not be
Documentation/process/submit-checklist.rst:21) Has been checked with injection of at least slab and page-allocation
Documentation/sound/alsa-configuration.rst:Note: snd-page-alloc module does the job which snd-hammerfall-mem
Documentation/sound/alsa-configuration.rst:allocation sure, load snd-page-alloc module in the early
Documentation/sound/alsa-configuration.rst:Note: snd-page-alloc module does the job which snd-hammerfall-mem
Documentation/sound/alsa-configuration.rst:allocation sure, load snd-page-alloc module in the early
Documentation/sound/alsa-configuration.rst:PCM buffers by loading snd-page-alloc module and write commands to its
Documentation/sound/alsa-configuration.rst:Reading the proc file /proc/drivers/snd-page-alloc shows the current
Documentation/sound/alsa-configuration.rst:commands to the snd-page-alloc driver:
Documentation/static-keys.txt:               487 page-faults               #    0.001 M/sec                    ( +-  0.02% )
Documentation/static-keys.txt:               487 page-faults               #    0.001 M/sec                    ( +-  0.05% )
Documentation/sysctl/vm.txt:- page-cluster
Documentation/sysctl/vm.txt:page-cluster
Documentation/sysctl/vm.txt:page-cluster controls the number of pages up to which consecutive pages
Documentation/trace/ring-buffer-design.txt:  next_page = temp_page->next
Documentation/virtual/kvm/mmu.txt:    The page pointed to by spt will have its page->private pointing back
Documentation/virtual/kvm/ppc-pv.txt:mfmsr	rX		ld	rX, magic_page->msr
Documentation/virtual/kvm/ppc-pv.txt:mfsprg	rX, 0		ld	rX, magic_page->sprg0
Documentation/virtual/kvm/ppc-pv.txt:mfsprg	rX, 1		ld	rX, magic_page->sprg1
Documentation/virtual/kvm/ppc-pv.txt:mfsprg	rX, 2		ld	rX, magic_page->sprg2
Documentation/virtual/kvm/ppc-pv.txt:mfsprg	rX, 3		ld	rX, magic_page->sprg3
Documentation/virtual/kvm/ppc-pv.txt:mfsrr0	rX		ld	rX, magic_page->srr0
Documentation/virtual/kvm/ppc-pv.txt:mfsrr1	rX		ld	rX, magic_page->srr1
Documentation/virtual/kvm/ppc-pv.txt:mfdar	rX		ld	rX, magic_page->dar
Documentation/virtual/kvm/ppc-pv.txt:mfdsisr	rX		lwz	rX, magic_page->dsisr
Documentation/virtual/kvm/ppc-pv.txt:mtmsr	rX		std	rX, magic_page->msr
Documentation/virtual/kvm/ppc-pv.txt:mtsprg	0, rX		std	rX, magic_page->sprg0
Documentation/virtual/kvm/ppc-pv.txt:mtsprg	1, rX		std	rX, magic_page->sprg1
Documentation/virtual/kvm/ppc-pv.txt:mtsprg	2, rX		std	rX, magic_page->sprg2
Documentation/virtual/kvm/ppc-pv.txt:mtsprg	3, rX		std	rX, magic_page->sprg3
Documentation/virtual/kvm/ppc-pv.txt:mtsrr0	rX		std	rX, magic_page->srr0
Documentation/virtual/kvm/ppc-pv.txt:mtsrr1	rX		std	rX, magic_page->srr1
Documentation/virtual/kvm/ppc-pv.txt:mtdar	rX		std	rX, magic_page->dar
Documentation/virtual/kvm/ppc-pv.txt:mtdsisr	rX		stw	rX, magic_page->dsisr
Documentation/vm/.gitignore:page-types
Documentation/vm/cleancache.rst:Cleancache can be thought of as a page-granularity victim cache for clean
Documentation/vm/cleancache.rst:balancing for some RAM-like devices).  Evicted page-cache pages (and
Documentation/vm/cleancache.rst:"page-object-oriented" specification provides a nice way to read and
Documentation/vm/cleancache.rst:The one-page-at-a-time copy semantics simplifies the implementation
Documentation/vm/frontswap.rst:a synchronous concurrency-safe page-oriented "pseudo-RAM device" conforming
Documentation/vm/frontswap.rst:evicted page-cache pages) are a great use for this kind of slower-than-RAM-
Documentation/vm/highmem.rst:* Linux needs a page-frame structure for each page in the system and the
Documentation/vm/highmem.rst:* you can have 896M/sizeof(struct page) page-frames at most; with struct
Documentation/vm/highmem.rst:  page-frames in that memory...
Documentation/vm/hmm.rst:CPU page-table mirroring works and the purpose of HMM in this context. The
Documentation/vm/hugetlbfs_reserv.rst:The page->private field points to any subpool associated with the page.
Documentation/vm/hwpoison.rst:		page-types -p `pidof init`   --hwpoison  # shall do nothing
Documentation/vm/hwpoison.rst:		page-types -p `pidof usemem` --hwpoison  # poison its pages
Documentation/vm/hwpoison.rst:	flag bits are defined in include/linux/kernel-page-flags.h and
Documentation/vm/ksm.rst:list of :c:type:`struct rmap_item` and the ``page->mapping`` of the
Documentation/vm/ksm.rst:page with ``page->mapping`` pointing to that "dup".
Documentation/vm/page_migration.rst:     reuses page->mapping's lower bits to represent it.
Documentation/vm/page_migration.rst:	page->mapping = page->mapping | PAGE_MAPPING_MOVABLE;
Documentation/vm/page_migration.rst:     so driver shouldn't access page->mapping directly. Instead, driver should
Documentation/vm/page_migration.rst:     use page_mapping which mask off the low two bits of page->mapping under
Documentation/vm/page_migration.rst:     page->mapping field is unified with other variables in struct page.
Documentation/vm/page_migration.rst:     As well, if driver releases the page after isolation by VM, page->mapping
Documentation/vm/page_migration.rst:     LRU pages never can have PAGE_MAPPING_MOVABLE in page->mapping. It is also
Documentation/vm/page_migration.rst:     Unlike __PageMovable, PageMovable functions validates page->mapping and
Documentation/vm/page_migration.rst:     destroying of page->mapping.
Documentation/vm/split_page_table_lock.rst:allocation: slab uses page->slab_cache for its pages.
Documentation/vm/split_page_table_lock.rst:This field shares storage with page->ptl.
Documentation/vm/split_page_table_lock.rst:page->ptl
Documentation/vm/split_page_table_lock.rst:page->ptl is used to access split page table lock, where 'page' is struct
Documentation/vm/split_page_table_lock.rst:page of page containing the table. It shares storage with page->private
Documentation/vm/split_page_table_lock.rst: - if spinlock_t fits into long, we use page->ptr as spinlock, so we
Documentation/vm/split_page_table_lock.rst: - if size of spinlock_t is bigger then size of long, we use page->ptl as
Documentation/vm/split_page_table_lock.rst:Please, never access page->ptl directly -- use appropriate helper.
Documentation/vm/transhuge.rst:page (like for checking page->mapping or other bits that are relevant
Documentation/vm/transhuge.rst:split_huge_page uses migration entries to stabilize page->_refcount and
Documentation/vm/transhuge.rst:page->_mapcount of anonymous pages. File pages just got unmapped.
Documentation/x86/protection-keys.txt:Memory Protection Keys provides a mechanism for enforcing page-based
arch/alpha/boot/bootp.c:	/* The initrd must be page-aligned.  See below for the 
arch/alpha/include/asm/pgtable.h: * This hopefully works with any standard Alpha page-size, as defined
arch/alpha/include/asm/pgtable.h: * the page is accessed. They are cleared only by the page-out routines
arch/alpha/include/asm/pgtable.h: * BAD_PAGETABLE is used when we need a bogus page-table, while
arch/alpha/include/asm/pgtable.h:/* to find an entry in a page-table */
arch/alpha/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/alpha/include/asm/pgtable.h:/* to find an entry in a page-table-directory. */
arch/alpha/mm/fault.c:		/* Synchronize this task's top level page-table
arch/alpha/mm/init.c: * BAD_PAGETABLE is the accompanying page-table: it is initialized
arch/arc/include/asm/pgalloc.h: * With software-only page-tables, addr-split for traversal is tweakable and
arch/arc/mm/cache.c: * However for larger Caches (way-size > page-size) - i.e. in Aliasing config,
arch/arc/mm/cache.c:		clear_bit(PG_dc_clean, &page->flags);
arch/arc/mm/cache.c:		clear_bit(PG_dc_clean, &page->flags);
arch/arc/mm/cache.c:		unsigned long vaddr = page->index << PAGE_SHIFT;
arch/arc/mm/cache.c:	clear_bit(PG_dc_clean, &page->flags);
arch/arc/mm/fault.c:	 * Synchronize this task's top level page-table
arch/arc/mm/ioremap.c:	/* Mappings have to be page-aligned */
arch/arc/mm/tlb.c: *      = page-fault thrice as fast (75 usec to 28 usec)
arch/arc/mm/tlb.c:	 * Exec page : Independent of aliasing/page-color considerations,
arch/arc/mm/tlb.c:		int dirty = !test_and_set_bit(PG_dc_clean, &page->flags);
arch/arc/mm/tlbex.S:; This macro does the page-table lookup for the faulting address.
arch/arc/mm/tlbex.S:	; (1) x = addr >> PAGE_SHIFT 	-> masks page-off bits from @fault-addr
arch/arm/boot/dts/am335x-nano.dts:		gpmc,page-burst-access-ns = <10>;
arch/arm/boot/dts/am335x-nano.dts:		gpmc,page-burst-access-ns = <10>;
arch/arm/boot/dts/exynos5410-smdk5410.dts:		samsung,srom-page-mode;
arch/arm/boot/dts/imx28-cfa10036.dts:					solomon,page-offset = <0>;
arch/arm/boot/dts/lpc3250-phy3250.dts:		at25,page-size = <64>;
arch/arm/boot/dts/lpc4350-hitex-eval.dts:		mpmc,page-mode-read-delay = <70>;
arch/arm/boot/dts/lpc4350-hitex-eval.dts:		mpmc,page-mode-read-delay = <55>;
arch/arm/boot/dts/lpc4357-ea4357-devkit.dts:		mpmc,page-mode-read-delay = <70>;
arch/arm/boot/dts/omap-gpmc-smsc911x.dtsi:		gpmc,page-burst-access-ns = <20>;
arch/arm/boot/dts/omap-gpmc-smsc9221.dtsi:		gpmc,page-burst-access-ns = <0>;
arch/arm/boot/dts/omap-zoom-common.dtsi:		gpmc,page-burst-access-ns = <20>;
arch/arm/boot/dts/omap2420-h4.dts:		gpmc,page-burst-access-ns = <10>;
arch/arm/boot/dts/omap2420-n8x0-common.dtsi:		gpmc,page-burst-access-ns = <27>;
arch/arm/boot/dts/omap2430-sdp.dts:		gpmc,page-burst-access-ns = <24>;
arch/arm/boot/dts/omap3-cm-t3x.dtsi:		gpmc,page-burst-access-ns = <6>;
arch/arm/boot/dts/omap3-devkit8000-common.dtsi:		gpmc,page-burst-access-ns = <24>;
arch/arm/boot/dts/omap3-gta04a5one.dts:		gpmc,page-burst-access-ns = <15>;
arch/arm/boot/dts/omap3-igep.dtsi:		gpmc,page-burst-access-ns = <12>;
arch/arm/boot/dts/omap3-lilly-a83x.dtsi:		gpmc,page-burst-access-ns = <5>;
arch/arm/boot/dts/omap3-lilly-a83x.dtsi:		gpmc,page-burst-access-ns = <5>;
arch/arm/boot/dts/omap3-lilly-dbb056.dts:		gpmc,page-burst-access-ns = <5>;
arch/arm/boot/dts/omap3-n900.dts:		gpmc,page-burst-access-ns = <15>;
arch/arm/boot/dts/omap3-n900.dts:		gpmc,page-burst-access-ns = <0>;
arch/arm/boot/dts/omap3-n950-n9.dtsi:		gpmc,page-burst-access-ns = <15>;
arch/arm/boot/dts/omap3-overo-tobiduo-common.dtsi:		gpmc,page-burst-access-ns = <0>;
arch/arm/boot/dts/omap3-sb-t35.dtsi:		gpmc,page-burst-access-ns = <20>;
arch/arm/boot/dts/omap3430-sdp.dts:		gpmc,page-burst-access-ns = <6>;
arch/arm/boot/dts/omap4-duovero-parlor.dts:		gpmc,page-burst-access-ns = <0>;
arch/arm/boot/dts/socfpga_arria10_socdk_qspi.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_arria5_socdk.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_cyclone5_socdk.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_cyclone5_sockit.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_cyclone5_sodia.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_cyclone5_vining_fpga.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/socfpga_cyclone5_vining_fpga.dts:		cdns,page-size = <256>;
arch/arm/boot/dts/zynq-zc770-xm013.dts:		at25,page-size = <32>;
arch/arm/common/dmabounce.c:#include <linux/page-flags.h>
arch/arm/common/dmabounce.c:				     0 /* no page-crossing issues */);
arch/arm/include/asm/cacheflush.h: * cache page at virtual address page->virtual.
arch/arm/include/asm/hugetlb.h:	clear_bit(PG_dcache_clean, &page->flags);
arch/arm/include/asm/page-nommu.h: *  arch/arm/include/asm/page-nommu.h
arch/arm/include/asm/page.h:#include <asm/page-nommu.h>
arch/arm/include/asm/page.h: *	page-based copying and clearing for user space for the particular
arch/arm/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/arm/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/arm/include/asm/stage2_pgtable.h: * level table at VM creation. Since we have a 3 level page-table,
arch/arm/include/asm/word-at-a-time.h: * In the (very unlikely) case of the word being a page-crosser
arch/arm/include/asm/xen/page-coherent.h:#include <xen/arm/page-coherent.h>
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_COPY_V4WT)	+= copypage-v4wt.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_COPY_V4WB)	+= copypage-v4wb.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_COPY_FEROCEON)	+= copypage-feroceon.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_COPY_V6)	+= copypage-v6.o context.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_SA1100)	+= copypage-v4mc.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_XSCALE)	+= copypage-xscale.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_XSC3)		+= copypage-xsc3.o
arch/arm/mm/Makefile:obj-$(CONFIG_CPU_COPY_FA)	+= copypage-fa.o
arch/arm/mm/Makefile:CFLAGS_copypage-feroceon.o := -march=armv5te
arch/arm/mm/copypage-fa.c: *  linux/arch/arm/lib/copypage-fa.S
arch/arm/mm/copypage-fa.c: * Based on copypage-v4wb.S:
arch/arm/mm/copypage-feroceon.c: *  linux/arch/arm/mm/copypage-feroceon.S
arch/arm/mm/copypage-v4mc.c: *  linux/arch/arm/lib/copypage-armv4mc.S
arch/arm/mm/copypage-v4wb.c: *  linux/arch/arm/mm/copypage-v4wb.c
arch/arm/mm/copypage-v4wt.c: *  linux/arch/arm/mm/copypage-v4wt.S
arch/arm/mm/copypage-v6.c: *  linux/arch/arm/mm/copypage-v6.c
arch/arm/mm/copypage-xsc3.c: *  linux/arch/arm/mm/copypage-xsc3.S
arch/arm/mm/copypage-xscale.c: *  linux/arch/arm/lib/copypage-xscale.S
arch/arm/mm/dma-mapping.c:			set_bit(PG_dcache_clean, &page->flags);
arch/arm/mm/fault-armv.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/arm/mm/fault.c:#include <linux/page-flags.h>
arch/arm/mm/flush.c:	 * userspace colour, which is congruent with page->index.
arch/arm/mm/flush.c:				page->index << PAGE_SHIFT);
arch/arm/mm/flush.c:	pgoff = page->index;
arch/arm/mm/flush.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/arm/mm/flush.c:		if (test_bit(PG_dcache_clean, &page->flags))
arch/arm/mm/flush.c:			clear_bit(PG_dcache_clean, &page->flags);
arch/arm/mm/flush.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/arm/mm/flush.c:		set_bit(PG_dcache_clean, &page->flags);
arch/arm/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/arm/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/arm/vdso/Makefile:VDSO_LDFLAGS += -Wl,-z,max-page-size=4096 -Wl,-z,common-page-size=4096
arch/arm64/boot/dts/altera/socfpga_stratix10_socdk.dts:		cdns,page-size = <256>;
arch/arm64/boot/dts/broadcom/northstar2/ns2-svk.dts:		at25,page-size = <64>;
arch/arm64/boot/dts/exynos/exynos5433-tm2-common.dtsi:		homepage-key {
arch/arm64/include/asm/cacheflush.h: * cache page at virtual address page->virtual.
arch/arm64/include/asm/fixmap.h: * page-sized. Use set_fixmap(idx,phys) to associate
arch/arm64/include/asm/hugetlb.h:	clear_bit(PG_dcache_clean, &page->flags);
arch/arm64/include/asm/memory.h:#include <asm/page-def.h>
arch/arm64/include/asm/page.h:#include <asm/page-def.h>
arch/arm64/include/asm/pgtable-hwdef.h: * Number of page-table levels required to address 'va_bits' wide
arch/arm64/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/arm64/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/arm64/include/asm/tlbflush.h: *	DSB ISHST	// Ensure prior page-table updates have completed
arch/arm64/include/asm/tlbflush.h: *		operation only invalidates a single, last-level page-table
arch/arm64/include/asm/word-at-a-time.h: * In the (very unlikely) case of the word being a page-crosser
arch/arm64/include/asm/xen/page-coherent.h:#include <xen/arm/page-coherent.h>
arch/arm64/kernel/cpufeature.c:	 * We don't need to rewrite the page-tables if either we've done
arch/arm64/kernel/machine_kexec.c:#include <linux/page-flags.h>
arch/arm64/kernel/sys32.c: * requested offset because it is not page-aligned, we return -EINVAL.
arch/arm64/kvm/hyp/tlb.c:		 * vcpu state, we prevent the EL1 page-table walker to
arch/arm64/mm/fault.c:#include <linux/page-flags.h>
arch/arm64/mm/flush.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/arm64/mm/flush.c:	if (test_bit(PG_dcache_clean, &page->flags))
arch/arm64/mm/flush.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/arm64/mm/mmu.c:	 * We need dsb(ishst) here to ensure the page-table-walker sees
arch/arm64/mm/mmu.c:	 * Use page-level mappings here so that we can shrink the region
arch/csky/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/csky/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/csky/mm/fault.c:		 * Synchronize this task's top level page-table
arch/csky/mm/init.c:	 * Setup page-table and enable TLB-hardrefill
arch/hexagon/include/asm/pgtable.h: * pgd_offset - find an offset in a page-table-directory
arch/hexagon/include/asm/tlbflush.h: * "This is called in munmap when we have freed up some page-table pages.
arch/ia64/hp/common/sba_iommu.c:		set_bit(PG_arch_1, &page->flags);
arch/ia64/include/asm/cacheflush.h:#include <linux/page-flags.h>
arch/ia64/include/asm/kregs.h:#define IA64_DCR_DP_BIT		 9	/* defer page-not-present faults */
arch/ia64/include/asm/pgalloc.h: * This hopefully works with any (fixed) ia-64 page-size, as defined
arch/ia64/include/asm/pgalloc.h:#include <linux/page-flags.h>
arch/ia64/include/asm/pgtable.h: * This hopefully works with any (fixed) IA-64 page-size, as defined
arch/ia64/include/asm/pgtable.h: * page-out routines.
arch/ia64/include/asm/tlb.h: *	(2) Clear the relevant portions of the page-table
arch/ia64/include/asm/tlb.h: *	      for each page-table-entry PTE that needs to be removed do {
arch/ia64/include/asm/tlb.h:		/* now flush the virt. page-table area mapping the address range: */
arch/ia64/include/asm/uaccess.h:#include <linux/page-flags.h>
arch/ia64/kernel/ivt.S:	MOV_TO_ITIR(p8, r25, r24)		// change to default page-size for VHPT
arch/ia64/kernel/ivt.S:	 * update both the page-table and the TLB entry.  To efficiently access the PTE,
arch/ia64/kernel/setup.c:	 * Initialize the page-table base register to a global
arch/ia64/mm/init.c:	if (test_bit(PG_arch_1, &page->flags))
arch/ia64/mm/init.c:	set_bit(PG_arch_1, &page->flags);	/* mark page as clean */
arch/ia64/mm/init.c:	/* place the VMLPT at the end of each page-table mapped region: */
arch/ia64/mm/ioremap.c:		 * Mappings have to be page-aligned
arch/ia64/mm/tlb.c:	u64 mask;		/* mask of supported purge page-sizes */
arch/ia64/mm/tlb.c:	unsigned long max_bits;	/* log2 of largest supported purge page-size */
arch/ia64/mm/tlb.c:		       "defaulting to architected purge page-sizes.\n", status);
arch/m68k/ifpsp060/src/isp.S:#	_real_lock_page() - "callout" to lock op's page from page-outs	#
arch/m68k/include/asm/mac_via.h: * of the alternate video page for page-flipping animation. Since there
arch/m68k/include/asm/motorola_pgtable.h:/* to find an entry in a page-table-directory */
arch/m68k/include/asm/sun3_pgtable.h:/* Use these fake page-protections on PMDs. */
arch/m68k/mm/memory.c:	 * is not page-aligned. Page align the addresses to work
arch/microblaze/include/asm/pgtable.h: * entries per page directory level: our page-table tree is two-level, so
arch/microblaze/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/microblaze/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/microblaze/include/asm/processor.h:	void		*pgdir;		/* root of page-table tree */
arch/microblaze/include/asm/tlbflush.h: * This is called in munmap when we have freed up some page-table
arch/microblaze/include/asm/tlbflush.h: * about our page-table pages.  -- paulus
arch/mips/include/asm/cpu.h:#define MIPS_CPU_FTLB		MBIT_ULL(37)	/* CPU has Fixed-page-size TLB */
arch/mips/include/asm/pgtable-32.h:/* to find an entry in a kernel page-table-directory */
arch/mips/include/asm/pgtable-32.h:/* to find an entry in a page-table-directory */
arch/mips/include/asm/pgtable-64.h:/* to find an entry in a kernel page-table-directory */
arch/mips/include/asm/pgtable-64.h:/* to find an entry in a page-table-directory */
arch/mips/include/asm/vdso.h: * @data: Pointer to VDSO image data (page-aligned).
arch/mips/include/asm/vdso.h: * @size: Size of the VDSO image data (page-aligned).
arch/mips/kvm/commpage.c:	vcpu->arch.cop0 = &page->cop0;
arch/mips/mm/Makefile:				   gup.o init.o mmap.o page.o page-funcs.o \
arch/mips/mm/fault.c:		 * Synchronize this task's top level page-table
arch/mips/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/mips/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/mips/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/mips/mm/tlb-funcs.S: * Based on mm/page-funcs.c
arch/nds32/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/nds32/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/nds32/mm/cacheflush.c:	if ((test_and_clear_bit(PG_dcache_dirty, &page->flags)) ||
arch/nds32/mm/cacheflush.c:		set_bit(PG_dcache_dirty, &page->flags);
arch/nds32/mm/cacheflush.c:			vaddr = page->index << PAGE_SHIFT;
arch/nds32/mm/fault.c:		 * Synchronize this task's top level page-table
arch/nds32/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/nios2/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/nios2/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/nios2/mm/cacheflush.c:	pgoff = page->index;
arch/nios2/mm/cacheflush.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/nios2/mm/cacheflush.c:		set_bit(PG_dcache_clean, &page->flags);
arch/nios2/mm/cacheflush.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/nios2/mm/fault.c:		 * Synchronize this task's top level page-table
arch/nios2/mm/ioremap.c:	/* Mappings have to be page-aligned */
arch/openrisc/include/asm/cacheflush.h:	clear_bit(PG_dc_clean, &page->flags);
arch/openrisc/include/asm/pgtable.h:/* to find an entry in a page-table */
arch/openrisc/include/asm/pgtable.h:/* to set the page-dir */
arch/openrisc/include/asm/pgtable.h: * pte_pagenr refers to the page-number counted starting from the virtual
arch/openrisc/include/asm/pgtable.h:/* to find an entry in a page-table-directory. */
arch/openrisc/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/openrisc/kernel/vmlinux.lds.S:	/* Whatever comes after _e_kernel_ro had better be page-aligend, too */
arch/openrisc/mm/cache.c:	int dirty = !test_and_set_bit(PG_dc_clean, &page->flags);
arch/openrisc/mm/fault.c:		 * we get page-aligned addresses so we can only check
arch/openrisc/mm/fault.c:		 * Synchronize this task's top level page-table
arch/openrisc/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/openrisc/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/openrisc/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/parisc/include/asm/pgtable.h: * are page-aligned, we don't care about the PAGE_OFFSET bits, except
arch/parisc/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/parisc/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/parisc/include/uapi/asm/pdc.h:		tc_page : 1,	/* 0 = 2K page-size-machine, 1 = 4k page size */
arch/parisc/kernel/cache.c:	    test_bit(PG_dcache_dirty, &page->flags)) {
arch/parisc/kernel/cache.c:		clear_bit(PG_dcache_dirty, &page->flags);
arch/parisc/kernel/cache.c:		set_bit(PG_dcache_dirty, &page->flags);
arch/parisc/kernel/cache.c:	pgoff = page->index;
arch/parisc/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/parisc/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/parisc/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/powerpc/Kconfig:	  '-zmax-page-size' set to 256K (the default is 64K). Or, if using
arch/powerpc/Kconfig:	  page-based protections, but without requiring modification of the
arch/powerpc/boot/rs6000.h:#define	RS6K_AOUTHDR_ZMAGIC	0x010B	/* paged: text r/o, both page-aligned */
arch/powerpc/include/asm/book3s/32/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/powerpc/include/asm/book3s/32/pgtable.h:/* to find an entry in a page-table-directory */
arch/powerpc/include/asm/book3s/64/pgtable.h: * Find an entry in a page-table-directory.  We combine the address region
arch/powerpc/include/asm/book3s/64/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/powerpc/include/asm/kvm_ppc.h:	if (!test_bit(PG_arch_1, &page->flags)) {
arch/powerpc/include/asm/kvm_ppc.h:		set_bit(PG_arch_1, &page->flags);
arch/powerpc/include/asm/mmu.h: * If we store section details in page->flags we can't increase the MAX_PHYSMEM_BITS
arch/powerpc/include/asm/mmu.h: * if we increase SECTIONS_WIDTH we will not store node details in page->flags and
arch/powerpc/include/asm/mmu.h: * page_to_nid does a page->section->node lookup
arch/powerpc/include/asm/nohash/32/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/powerpc/include/asm/nohash/32/pgtable.h:/* to find an entry in a page-table-directory */
arch/powerpc/include/asm/nohash/64/pgtable.h: * Find an entry in a page-table-directory.  We combine the address region
arch/powerpc/include/asm/nohash/64/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/powerpc/include/asm/processor.h:	void		*pgdir;		/* root of page-table tree */
arch/powerpc/include/asm/spu_csa.h:	 * 'ls' must be page-aligned on all configurations.
arch/powerpc/kernel/dt_cpu_ftrs.c:	{"virtual-page-class-key-protection", feat_enable, 0},
arch/powerpc/kernel/head_32.S: * We put a few things here that have to be page-aligned.
arch/powerpc/kernel/head_32.S: * which is page-aligned.
arch/powerpc/kernel/head_40x.S:/* We put a few things here that have to be page-aligned. This stuff
arch/powerpc/kernel/head_40x.S: * goes at the beginning of the data segment, which is page-aligned.
arch/powerpc/kernel/head_44x.S: * We put a few things here that have to be page-aligned. This stuff
arch/powerpc/kernel/head_44x.S: * goes at the beginning of the data segment, which is page-aligned.
arch/powerpc/kernel/head_64.S: * We put a few things here that have to be page-aligned.
arch/powerpc/kernel/head_64.S: * This stuff goes at the beginning of the bss, which is page-aligned.
arch/powerpc/kernel/head_8xx.S: * We put a few things here that have to be page-aligned.
arch/powerpc/kernel/head_8xx.S: * which is page-aligned.
arch/powerpc/kernel/head_fsl_booke.S: * We put a few things here that have to be page-aligned. This stuff
arch/powerpc/kernel/head_fsl_booke.S: * goes at the beginning of the data segment, which is page-aligned.
arch/powerpc/kernel/prom.c:	/* Ensure that total memory size is page-aligned. */
arch/powerpc/kvm/book3s_hv.c:#include <linux/page-flags.h>
arch/powerpc/mm/Makefile:obj-$(CONFIG_PPC_BOOK3S_64)	+= hugetlbpage-hash64.o
arch/powerpc/mm/Makefile:obj-$(CONFIG_PPC_RADIX_MMU)	+= hugetlbpage-radix.o
arch/powerpc/mm/Makefile:obj-$(CONFIG_PPC_BOOK3E_MMU)	+= hugetlbpage-book3e.o
arch/powerpc/mm/Makefile:obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += hugepage-hash64.o
arch/powerpc/mm/Makefile:obj-$(CONFIG_PPC_SUBPAGE_PROT)	+= subpage-prot.o
arch/powerpc/mm/hash_low_32.S:	lwz	r5,PGDIR(r8)		/* virt page-table root */
arch/powerpc/mm/hash_utils_64.c:	prop = of_get_flat_dt_prop(node, "ibm,segment-page-sizes", &size);
arch/powerpc/mm/hash_utils_64.c:	if (!test_bit(PG_arch_1, &page->flags) && !PageReserved(page)) {
arch/powerpc/mm/hash_utils_64.c:			set_bit(PG_arch_1, &page->flags);
arch/powerpc/mm/mem.c:	if (test_bit(PG_arch_1, &page->flags))
arch/powerpc/mm/mem.c:		clear_bit(PG_arch_1, &page->flags);
arch/powerpc/mm/mmu_context_book3s64.c:	if (atomic_sub_and_test(PMD_FRAG_NR - count, &page->pt_frag_refcount)) {
arch/powerpc/mm/mmu_context_iommu.c:	list_add(&page->lru, &cma_migrate_pages);
arch/powerpc/mm/pgtable-book3s64.c:	atomic_set(&page->pt_frag_refcount, 1);
arch/powerpc/mm/pgtable-book3s64.c:		atomic_set(&page->pt_frag_refcount, PMD_FRAG_NR);
arch/powerpc/mm/pgtable-book3s64.c:	BUG_ON(atomic_read(&page->pt_frag_refcount) <= 0);
arch/powerpc/mm/pgtable-book3s64.c:	if (atomic_dec_and_test(&page->pt_frag_refcount)) {
arch/powerpc/mm/pgtable-frag.c:	if (atomic_sub_and_test(PTE_FRAG_NR - count, &page->pt_frag_refcount)) {
arch/powerpc/mm/pgtable-frag.c:	atomic_set(&page->pt_frag_refcount, 1);
arch/powerpc/mm/pgtable-frag.c:		atomic_set(&page->pt_frag_refcount, PTE_FRAG_NR);
arch/powerpc/mm/pgtable-frag.c:	BUG_ON(atomic_read(&page->pt_frag_refcount) <= 0);
arch/powerpc/mm/pgtable-frag.c:	if (atomic_dec_and_test(&page->pt_frag_refcount)) {
arch/powerpc/perf/hv-24x7.c:	hv_page_cache = kmem_cache_create("hv-page-4096", 4096, 4096, 0, NULL);
arch/powerpc/platforms/ps3/mm.c:	for (iopage--; 0 <= iopage; iopage--) {
arch/powerpc/sysdev/xive/native.c:	if (of_property_read_u32(np, "ibm,xive-provision-page-size",
arch/riscv/include/asm/cacheflush.h:	if (test_bit(PG_dcache_clean, &page->flags))
arch/riscv/include/asm/cacheflush.h:		clear_bit(PG_dcache_clean, &page->flags);
arch/riscv/mm/cacheflush.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/riscv/mm/fault.c:		 * Synchronize this task's top level page-table
arch/riscv/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/riscv/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/s390/include/asm/hugetlb.h:	clear_bit(PG_arch_1, &page->flags);
arch/s390/include/asm/pgtable.h:#include <linux/page-flags.h>
arch/s390/include/asm/pgtable.h: * A page-table entry has some bits we have to treat in a special way.
arch/s390/include/uapi/asm/sie.h:	{ 0x258, "DIAG (0x258) page-reference services" },	\
arch/s390/kernel/nmi.c:	mcck_backup = &sie_page->mcck_info;
arch/s390/kvm/kvm-s390.c:	vcpu->arch.sie_block = &sie_page->sie_block;
arch/s390/kvm/kvm-s390.c:	vcpu->arch.sie_block->itdba = (unsigned long) &sie_page->itdb;
arch/s390/kvm/kvm-s390.c:		mcck_info = &sie_page->mcck_info;
arch/s390/kvm/priv.c:#include <asm/page-states.h>
arch/s390/kvm/vsie.c:	atomic_or(PROG_REQUEST, &vsie_page->scb_s.prog20);
arch/s390/kvm/vsie.c:	if (vsie_page->scb_s.prog0c & PROG_IN_SIE)
arch/s390/kvm/vsie.c:		atomic_or(CPUSTAT_STOP_INT, &vsie_page->scb_s.cpuflags);
arch/s390/kvm/vsie.c:	while (vsie_page->scb_s.prog0c & PROG_IN_SIE)
arch/s390/kvm/vsie.c:	atomic_andnot(PROG_REQUEST, &vsie_page->scb_s.prog20);
arch/s390/kvm/vsie.c:	return !(atomic_read(&vsie_page->scb_s.prog20) & PROG_REQUEST);
arch/s390/kvm/vsie.c:	cpuflags = atomic_read(&vsie_page->scb_o->cpuflags);
arch/s390/kvm/vsie.c:	atomic_andnot(bits, &vsie_page->scb_s.cpuflags);
arch/s390/kvm/vsie.c:	atomic_or(cpuflags & bits, &vsie_page->scb_s.cpuflags);
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:		ret = setup_apcb(vcpu, &vsie_page->crycb, crycb_addr,
arch/s390/kvm/vsie.c:			    vsie_page->crycb.dea_wrapping_key_mask, 56))
arch/s390/kvm/vsie.c:	b1 = (unsigned long *) vsie_page->crycb.dea_wrapping_key_mask;
arch/s390/kvm/vsie.c:	scb_s->crycbd = ((__u32)(__u64) &vsie_page->crycb) | CRYCB_FORMAT2;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	rc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap, prefix);
arch/s390/kvm/vsie.c:		rc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap,
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:		unpin_guest_page(vcpu->kvm, vsie_page->sca_gpa, hpa);
arch/s390/kvm/vsie.c:		vsie_page->sca_gpa = 0;
arch/s390/kvm/vsie.c:		unpin_guest_page(vcpu->kvm, vsie_page->itdba_gpa, hpa);
arch/s390/kvm/vsie.c:		vsie_page->itdba_gpa = 0;
arch/s390/kvm/vsie.c:		unpin_guest_page(vcpu->kvm, vsie_page->gvrd_gpa, hpa);
arch/s390/kvm/vsie.c:		vsie_page->gvrd_gpa = 0;
arch/s390/kvm/vsie.c:		unpin_guest_page(vcpu->kvm, vsie_page->riccbd_gpa, hpa);
arch/s390/kvm/vsie.c:		vsie_page->riccbd_gpa = 0;
arch/s390/kvm/vsie.c:		unpin_guest_page(vcpu->kvm, vsie_page->sdnx_gpa, hpa);
arch/s390/kvm/vsie.c:		vsie_page->sdnx_gpa = 0;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:		vsie_page->sca_gpa = gpa;
arch/s390/kvm/vsie.c:		vsie_page->itdba_gpa = gpa;
arch/s390/kvm/vsie.c:		vsie_page->gvrd_gpa = gpa;
arch/s390/kvm/vsie.c:		vsie_page->riccbd_gpa = gpa;
arch/s390/kvm/vsie.c:		vsie_page->sdnx_gpa = gpa;
arch/s390/kvm/vsie.c:	hpa_t hpa = (hpa_t) vsie_page->scb_o;
arch/s390/kvm/vsie.c:	vsie_page->scb_o = NULL;
arch/s390/kvm/vsie.c: * Pin the scb at gpa provided by guest 2 at vsie_page->scb_o.
arch/s390/kvm/vsie.c:	vsie_page->scb_o = (struct kvm_s390_sie_block *) hpa;
arch/s390/kvm/vsie.c:	rc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap,
arch/s390/kvm/vsie.c:			vsie_page->fault_addr = current->thread.gmap_addr;
arch/s390/kvm/vsie.c:	if (vsie_page->fault_addr)
arch/s390/kvm/vsie.c:		kvm_s390_shadow_fault(vcpu, vsie_page->gmap,
arch/s390/kvm/vsie.c:				      vsie_page->fault_addr);
arch/s390/kvm/vsie.c:	vsie_page->fault_addr = 0;
arch/s390/kvm/vsie.c:	vsie_page->scb_s.icptcode = 0;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	__u32 fac = READ_ONCE(vsie_page->scb_o->fac) & 0x7ffffff8U;
arch/s390/kvm/vsie.c:		if (read_guest_real(vcpu, fac, &vsie_page->fac,
arch/s390/kvm/vsie.c:				    sizeof(vsie_page->fac)))
arch/s390/kvm/vsie.c:		scb_s->fac = (__u32)(__u64) &vsie_page->fac;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_o = vsie_page->scb_o;
arch/s390/kvm/vsie.c:		kvm_s390_reinject_machine_check(vcpu, &vsie_page->mcck_info);
arch/s390/kvm/vsie.c:	if (vsie_page->gmap)
arch/s390/kvm/vsie.c:		gmap_put(vsie_page->gmap);
arch/s390/kvm/vsie.c:	WRITE_ONCE(vsie_page->gmap, NULL);
arch/s390/kvm/vsie.c:	if (vsie_page->gmap && gmap_shadow_valid(vsie_page->gmap, asce, edat))
arch/s390/kvm/vsie.c:	WRITE_ONCE(vsie_page->gmap, gmap);
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:	WRITE_ONCE(vcpu->arch.vsie_block, &vsie_page->scb_s);
arch/s390/kvm/vsie.c:	struct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;
arch/s390/kvm/vsie.c:			gmap_enable(vsie_page->gmap);
arch/s390/kvm/vsie.c:		radix_tree_delete(&kvm->arch.vsie.addr_to_page, page->index >> 9);
arch/s390/kvm/vsie.c:	page->index = addr;
arch/s390/kvm/vsie.c:	memset(&vsie_page->scb_s, 0, sizeof(struct kvm_s390_sie_block));
arch/s390/kvm/vsie.c:	vsie_page->fault_addr = 0;
arch/s390/kvm/vsie.c:	vsie_page->scb_s.ihcpu = 0xffffU;
arch/s390/kvm/vsie.c:		radix_tree_delete(&kvm->arch.vsie.addr_to_page, page->index >> 9);
arch/s390/mm/Makefile:obj-y		+= page-states.o gup.o pageattr.o pgtable.o pgalloc.o
arch/s390/mm/gmap.c:	page->index = 0;
arch/s390/mm/gmap.c:	list_add(&page->lru, &gmap->crst_list);
arch/s390/mm/gmap.c:		list_add(&page->lru, &gmap->crst_list);
arch/s390/mm/gmap.c:		page->index = gaddr;
arch/s390/mm/gmap.c:	return page->index + offset;
arch/s390/mm/gmap.c:	list_del(&page->lru);
arch/s390/mm/gmap.c:		list_del(&page->lru);
arch/s390/mm/gmap.c:	list_del(&page->lru);
arch/s390/mm/gmap.c:		list_del(&page->lru);
arch/s390/mm/gmap.c:	list_del(&page->lru);
arch/s390/mm/gmap.c:		list_del(&page->lru);
arch/s390/mm/gmap.c:	list_del(&page->lru);
arch/s390/mm/gmap.c:		list_del(&page->lru);
arch/s390/mm/gmap.c:	page->index = r2t & _REGION_ENTRY_ORIGIN;
arch/s390/mm/gmap.c:		page->index |= GMAP_SHADOW_FAKE_TABLE;
arch/s390/mm/gmap.c:	list_add(&page->lru, &sg->crst_list);
arch/s390/mm/gmap.c:	page->index = r3t & _REGION_ENTRY_ORIGIN;
arch/s390/mm/gmap.c:		page->index |= GMAP_SHADOW_FAKE_TABLE;
arch/s390/mm/gmap.c:	list_add(&page->lru, &sg->crst_list);
arch/s390/mm/gmap.c:	page->index = sgt & _REGION_ENTRY_ORIGIN;
arch/s390/mm/gmap.c:		page->index |= GMAP_SHADOW_FAKE_TABLE;
arch/s390/mm/gmap.c:	list_add(&page->lru, &sg->crst_list);
arch/s390/mm/gmap.c:		*pgt = page->index & ~GMAP_SHADOW_FAKE_TABLE;
arch/s390/mm/gmap.c:		*fake = !!(page->index & GMAP_SHADOW_FAKE_TABLE);
arch/s390/mm/gmap.c:	page->index = pgt & _SEGMENT_ENTRY_ORIGIN;
arch/s390/mm/gmap.c:		page->index |= GMAP_SHADOW_FAKE_TABLE;
arch/s390/mm/gmap.c:	list_add(&page->lru, &sg->pt_list);
arch/s390/mm/gmap.c:	set_bit(PG_arch_1, &page->flags);
arch/s390/mm/hugetlbpage.c:	if (!test_and_set_bit(PG_arch_1, &page->flags))
arch/s390/mm/page-states.c:#include <asm/page-states.h>
arch/s390/mm/page-states.c:		set_bit(PG_arch_1, &page->flags);
arch/s390/mm/page-states.c:			if (__test_and_clear_bit(PG_arch_1, &page->flags))
arch/s390/mm/page-states.c:			if (!list_empty(&page->lru))
arch/s390/mm/pgalloc.c:			mask = atomic_read(&page->_refcount) >> 24;
arch/s390/mm/pgalloc.c:				atomic_xor_bits(&page->_refcount,
arch/s390/mm/pgalloc.c:				list_del(&page->lru);
arch/s390/mm/pgalloc.c:		atomic_xor_bits(&page->_refcount, 3 << 24);
arch/s390/mm/pgalloc.c:		atomic_xor_bits(&page->_refcount, 1 << 24);
arch/s390/mm/pgalloc.c:		list_add(&page->lru, &mm->context.pgtable_list);
arch/s390/mm/pgalloc.c:		mask = atomic_xor_bits(&page->_refcount, 1U << (bit + 24));
arch/s390/mm/pgalloc.c:			list_add(&page->lru, &mm->context.pgtable_list);
arch/s390/mm/pgalloc.c:			list_del(&page->lru);
arch/s390/mm/pgalloc.c:		atomic_xor_bits(&page->_refcount, 3U << 24);
arch/s390/mm/pgalloc.c:	mask = atomic_xor_bits(&page->_refcount, 0x11U << (bit + 24));
arch/s390/mm/pgalloc.c:		list_add_tail(&page->lru, &mm->context.pgtable_list);
arch/s390/mm/pgalloc.c:		list_del(&page->lru);
arch/s390/mm/pgalloc.c:		mask = atomic_xor_bits(&page->_refcount, mask << (4 + 24));
arch/s390/mm/pgalloc.c:			atomic_xor_bits(&page->_refcount, 3 << 24);
arch/s390/mm/pgalloc.c:	 * This isn't an RCU grace period and hence the page-tables cannot be
arch/s390/mm/pgalloc.c:	 * It is however sufficient for software page-table walkers that rely
arch/s390/mm/pgtable.c:#include <asm/page-states.h>
arch/sh/include/asm/hugetlb.h:	clear_bit(PG_dcache_clean, &page->flags);
arch/sh/include/asm/pgtable_32.h:/* to find an entry in a page-table-directory. */
arch/sh/include/asm/pgtable_32.h:/* to find an entry in a kernel page-table-directory */
arch/sh/mm/cache-sh4.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/sh/mm/cache-sh4.c:			test_bit(PG_dcache_clean, &page->flags) &&
arch/sh/mm/cache-sh5.c:	   Also, eaddr is page-aligned. */
arch/sh/mm/cache-sh5.c:	   access_process_vm->flush_cache_page->here, (e.g. when reading from
arch/sh/mm/cache-sh7705.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/sh/mm/cache-sh7705.c: * This is called when a page-cache page is about to be mapped into a
arch/sh/mm/cache.c:	    test_bit(PG_dcache_clean, &page->flags)) {
arch/sh/mm/cache.c:			clear_bit(PG_dcache_clean, &page->flags);
arch/sh/mm/cache.c:	    test_bit(PG_dcache_clean, &page->flags)) {
arch/sh/mm/cache.c:			clear_bit(PG_dcache_clean, &page->flags);
arch/sh/mm/cache.c:		int dirty = !test_and_set_bit(PG_dcache_clean, &page->flags);
arch/sh/mm/cache.c:		    test_bit(PG_dcache_clean, &page->flags)) {
arch/sh/mm/fault.c:	 * Synchronize this task's top level page-table
arch/sh/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/sh/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/sh/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/sh/mm/ioremap_fixed.c:	 * Mappings have to be page-aligned
arch/sh/mm/kmap.c:	BUG_ON(!test_bit(PG_dcache_clean, &page->flags));
arch/sparc/include/asm/pgtable_32.h:/* to find an entry in a page-table-directory */
arch/sparc/include/asm/pgtable_32.h:/* to find an entry in a kernel page-table-directory */
arch/sparc/include/asm/pgtable_64.h:/* to find an entry in a page-table-directory. */
arch/sparc/include/asm/pgtable_64.h:/* to find an entry in a kernel page-table-directory */
arch/sparc/kernel/head_32.S: * these page-table data structures.
arch/sparc/kernel/mdesc.c:	const u64 *pgsz_prop = mdesc_get_property(hp, mp, "mmu-page-size-list", NULL);
arch/sparc/mm/fault_32.c:		 * Synchronize this task's top level page-table
arch/sparc/mm/init_64.c:			     : "r" (mask), "r" (non_cpu_bits), "r" (&page->flags)
arch/sparc/mm/init_64.c:			     : "r" (cpu), "r" (mask), "r" (&page->flags),
arch/sparc/mm/init_64.c:		pg_flags = page->flags;
arch/sparc/mm/init_64.c:		int dirty = test_bit(PG_dcache_dirty, &page->flags);
arch/sparc/mm/init_64.c:	 * page->flags usage will work.
arch/sparc/mm/init_64.c:	 * cpu number starting at bit 32 in the page->flags.  Also,
arch/sparc/mm/ultra.S:				     %g5 == (page->mapping != NULL)  */
arch/sparc/vdso/Makefile:			-z max-page-size=8192
arch/unicore32/include/asm/cacheflush.h: * cache page at virtual address page->virtual.
arch/unicore32/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/unicore32/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/unicore32/mm/fault.c:#include <linux/page-flags.h>
arch/unicore32/mm/flush.c:		clear_bit(PG_dcache_clean, &page->flags);
arch/unicore32/mm/flush.c:		set_bit(PG_dcache_clean, &page->flags);
arch/unicore32/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/unicore32/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/unicore32/mm/mmu.c:	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
arch/x86/Kconfig:	  page-based protections, but without requiring modification of the
arch/x86/Makefile:# The 64-bit kernel must be aligned to 2MB.  Pass -z max-page-size=0x200000 to
arch/x86/Makefile:KBUILD_LDFLAGS += $(call ld-option, -z max-page-size=0x200000)
arch/x86/boot/compressed/pgtable_64.c:		/* Keep bios_start page-aligned. */
arch/x86/entry/vdso/Makefile:			-z max-page-size=4096
arch/x86/entry/vdso/Makefile:			   -z max-page-size=4096
arch/x86/events/intel/pt.c: * struct topa - page-sized ToPA table with metadata at the top
arch/x86/events/intel/pt.c: * topa_alloc() - allocate page-sized ToPA table
arch/x86/events/intel/pt.c: * topa_free() - free a page-sized ToPA table
arch/x86/include/asm/mmu_context.h:	 * Xen requires page-aligned LDTs with special permissions.  This is
arch/x86/include/asm/pgtable.h:/* to find an entry in a page-table-directory. */
arch/x86/include/asm/pgtable.h: * Page table pages are page-aligned.  The lower half of the top
arch/x86/include/asm/processor-flags.h: * (i.e. it's 32-byte aligned, not page-aligned) and CR3[4:0] is ignored.
arch/x86/include/asm/tlbflush.h: * and page-granular flushes are available only on i486 and up.
arch/x86/include/asm/word-at-a-time.h: * In the (very unlikely) case of the word being a page-crosser
arch/x86/include/asm/xen/interface_32.h: * page-aligned, and putting the 12 MSB of the address into the 12 LSB
arch/x86/kernel/head_32.S:	.align PAGE_SIZE		/* needs to be page-sized too */
arch/x86/kernel/idt.c:/* Must be page-aligned because the real IDT is used in a fixmap. */
arch/x86/kernel/ldt.c:	 * Xen is very picky: it requires a page-aligned LDT that has no
arch/x86/kernel/ldt.c:	/* Propagate LDT mapping to the user page-table */
arch/x86/kernel/smpboot.c:	/* start_ip had better be page-aligned! */
arch/x86/kernel/tboot.c:	/* Look for valid page-aligned address for shared page. */
arch/x86/kvm/mmu.c:	 * This can happen if a guest gets a page-fault on data access but the HW
arch/x86/kvm/svm.c:		 * If the user buffer is not page-aligned, calculate the offset
arch/x86/kvm/vmx/nested.c: * KVM wants to inject page-faults which it got to the guest. This function
arch/x86/kvm/vmx/nested.c:	 * Whether page-faults are trapped is determined by a combination of
arch/x86/kvm/vmx/nested.c:	/* only 4 levels page-walk length are valid */
arch/x86/kvm/x86.c:	 * Note: if the guest uses a non-page-table modifying instruction
arch/x86/mm/cpu_entry_area.c:	 *  will access incorrect information without generating a page-fault
arch/x86/mm/fault.c:	 * Synchronize this task's top level page-table
arch/x86/mm/fault.c:	 * 3. A fault caused by a page-level protection violation.
arch/x86/mm/init_64.c:		magic = (unsigned long)page->freelist;
arch/x86/mm/ioremap.c: * NOTE! We need to allow non-page-aligned mappings too: we will obviously
arch/x86/mm/ioremap.c: * have to convert them into an offset in a page-aligned mapping, but the
arch/x86/mm/ioremap.c:	 * Mappings have to be page-aligned
arch/x86/mm/ioremap.c: * systems this ends up setting page-attribute flags PCD=1, PWT=1. For PAT
arch/x86/mm/kmmio.c:			if (!faultpage->old_presence)
arch/x86/mm/kmmio.c:	if (ctx->fpage->count)
arch/x86/mm/kmmio.c: * With page-unaligned ioremaps, one or two armed pages may contain
arch/x86/mm/mmio-mod.c:	/* These are page-unaligned. */
arch/x86/mm/mpx.c:	 * this ensures that it remains page-aligned and does
arch/x86/mm/pageattr.c:	 * and the kernel identity mapping share the same page-table pages,
arch/x86/mm/pgtable.c:	 * NOTE! For PAE, any changes to the top page-directory-pointer-table
arch/x86/mm/pgtable.c:	list_add(&page->lru, &pgd_list);
arch/x86/mm/pgtable.c:	list_del(&page->lru);
arch/x86/mm/pgtable.c:	return page->pt_mm;
arch/x86/mm/pgtable.c: * We allocate separate PMDs for the kernel part of the user page-table
arch/x86/mm/pgtable.c: * user-space page-table.
arch/x86/mm/pti.c: * Define the page-table levels we clone for user-space on 32
arch/x86/mm/pti.c:			/* Walk the page-table down to the pte level */
arch/x86/mm/pti.c:			/* Allocate PTE in the user page-table */
arch/x86/mm/pti.c: * address space into the user page-tables, making PTI useless. So clone
arch/x86/mm/pti.c: * the page-table on the PMD level to prevent that.
arch/x86/mm/pti.c:	 * the last level for areas that are not huge-page-aligned.
arch/x86/mm/pti.c: * Finalize the kernel mappings in the userspace page-table. Some of the
arch/x86/mm/pti.c: * userspace page-table.
arch/x86/um/vdso/Makefile:       -Wl,-z,max-page-size=4096
arch/x86/xen/enlighten_pv.c:#include <linux/page-flags.h>
arch/x86/xen/enlighten_pv.c:	 * page-walk to get the underlying MFN for the
arch/x86/xen/mmu.c:	/* otherwise we have to do a (slower) full page-table walk */
arch/x86/xen/mmu_pv.c:		user_ptr = (pgd_t *)page->private;
arch/x86/xen/mmu_pv.c:		BUG_ON(page->private != 0);
arch/x86/xen/mmu_pv.c:		page->private = (unsigned long)user_pgd;
arch/xtensa/include/asm/pgtable.h:/* to find an entry in a kernel page-table-directory */
arch/xtensa/include/asm/pgtable.h:/* to find an entry in a page-table-directory */
arch/xtensa/kernel/entry.S:	/* Read ptevaddr and convert to top of page-table page.
arch/xtensa/mm/cache.c:	set_bit(PG_arch_1, &page->flags);
arch/xtensa/mm/cache.c:		if (!test_bit(PG_arch_1, &page->flags))
arch/xtensa/mm/cache.c:			set_bit(PG_arch_1, &page->flags);
arch/xtensa/mm/cache.c:		unsigned long temp = page->index << PAGE_SHIFT;
arch/xtensa/mm/cache.c:	if (!PageReserved(page) && test_bit(PG_arch_1, &page->flags)) {
arch/xtensa/mm/cache.c:		clear_bit(PG_arch_1, &page->flags);
arch/xtensa/mm/cache.c:	if (!PageReserved(page) && !test_bit(PG_arch_1, &page->flags)
arch/xtensa/mm/cache.c:		set_bit(PG_arch_1, &page->flags);
arch/xtensa/mm/fault.c:		/* Synchronize this task's top level page-table
arch/xtensa/mm/mmu.c:	 * of the page-size array.  For nonexistent PGSZID<w> fields,
block/bio.c:	if (!page->mem_cgroup)
block/bio.c:	css = cgroup_e_css(page->mem_cgroup->css.cgroup, &io_cgrp_subsys);
block/blk-mq.c:		list_del_init(&page->lru);
block/blk-mq.c:		__free_pages(page, page->private);
block/blk-mq.c:		page->private = this_order;
block/blk-mq.c:		list_add_tail(&page->lru, &tags->page_list);
drivers/acpi/nvs.c: *	The NVS region need not be page-aligned (both ends) and we arrange
drivers/acpi/nvs.c: *	things so that the data from page-aligned addresses in this region will
drivers/android/binder_alloc.c:		if (!page->page_ptr) {
drivers/android/binder_alloc.c:		if (page->page_ptr) {
drivers/android/binder_alloc.c:			on_lru = list_lru_del(&binder_alloc_lru, &page->lru);
drivers/android/binder_alloc.c:		page->page_ptr = alloc_page(GFP_KERNEL |
drivers/android/binder_alloc.c:		if (!page->page_ptr) {
drivers/android/binder_alloc.c:		page->alloc = alloc;
drivers/android/binder_alloc.c:		INIT_LIST_HEAD(&page->lru);
drivers/android/binder_alloc.c:					       &page->page_ptr);
drivers/android/binder_alloc.c:		ret = list_lru_add(&binder_alloc_lru, &page->lru);
drivers/android/binder_alloc.c:		__free_page(page->page_ptr);
drivers/android/binder_alloc.c:		page->page_ptr = NULL;
drivers/android/binder_alloc.c:		if (!page->page_ptr)
drivers/android/binder_alloc.c:		else if (list_empty(&page->lru))
drivers/android/binder_alloc.c:	alloc = page->alloc;
drivers/android/binder_alloc.c:	if (!page->page_ptr)
drivers/android/binder_alloc.c:	__free_page(page->page_ptr);
drivers/android/binder_alloc.c:	page->page_ptr = NULL;
drivers/atm/fore200e.c:	/* returned chunks are page-aligned */
drivers/block/brd.c:	BUG_ON(page && page->index != idx);
drivers/block/brd.c:	page->index = idx;
drivers/block/brd.c:		BUG_ON(page->index != idx);
drivers/block/drbd/drbd_bitmap.c:/* we store some "meta" info about our pages in page->private */
drivers/block/drbd/drbd_receiver.c: * page->private being our "next" pointer.
drivers/block/drbd/drbd_receiver.c: * Returns a page chain linked via page->private.
drivers/block/null_blk_main.c:	t_page->page = alloc_pages(gfp_flags, 0);
drivers/block/null_blk_main.c:	if (!t_page->page)
drivers/block/null_blk_main.c:	memset(t_page->bitmap, 0, sizeof(t_page->bitmap));
drivers/block/null_blk_main.c:	__set_bit(NULLB_PAGE_FREE, t_page->bitmap);
drivers/block/null_blk_main.c:	if (test_bit(NULLB_PAGE_LOCK, t_page->bitmap))
drivers/block/null_blk_main.c:	__free_page(t_page->page);
drivers/block/null_blk_main.c:	return find_first_bit(page->bitmap, size) == size;
drivers/block/null_blk_main.c:		__clear_bit(sector_bit, t_page->bitmap);
drivers/block/null_blk_main.c:		WARN_ON(!t_page || t_page->page->index != idx);
drivers/block/null_blk_main.c:			pos = t_pages[i]->page->index;
drivers/block/null_blk_main.c:	WARN_ON(t_page && t_page->page->index != idx);
drivers/block/null_blk_main.c:	if (t_page && (for_write || test_bit(sector_bit, t_page->bitmap)))
drivers/block/null_blk_main.c:	t_page->page->index = idx;
drivers/block/null_blk_main.c:	idx = c_page->page->index;
drivers/block/null_blk_main.c:	__clear_bit(NULLB_PAGE_LOCK, c_page->bitmap);
drivers/block/null_blk_main.c:	if (test_bit(NULLB_PAGE_FREE, c_page->bitmap)) {
drivers/block/null_blk_main.c:	src = kmap_atomic(c_page->page);
drivers/block/null_blk_main.c:	dst = kmap_atomic(t_page->page);
drivers/block/null_blk_main.c:		if (test_bit(i, c_page->bitmap)) {
drivers/block/null_blk_main.c:			__set_bit(i, t_page->bitmap);
drivers/block/null_blk_main.c:		nullb->cache_flush_pos = c_pages[i]->page->index;
drivers/block/null_blk_main.c:		dst = kmap_atomic(t_page->page);
drivers/block/null_blk_main.c:		__set_bit(sector & SECTOR_MASK, t_page->bitmap);
drivers/block/null_blk_main.c:		src = kmap_atomic(t_page->page);
drivers/block/umem.c:		card->Active, page->headcnt, page->cnt - 1);
drivers/block/umem.c:	desc = &page->desc[page->cnt-1];
drivers/block/umem.c:	desc = &page->desc[page->headcnt];
drivers/block/umem.c:	offset = ((char *)desc) - ((char *)page->desc);
drivers/block/umem.c:	writel(cpu_to_le32((page->page_dma+offset) & 0xffffffff),
drivers/block/umem.c:	writel(cpu_to_le32(((u64)page->page_dma)>>32),
drivers/block/umem.c:	page->cnt = 0;
drivers/block/umem.c:	page->headcnt = 0;
drivers/block/umem.c:	page->bio = NULL;
drivers/block/umem.c:	page->biotail = &page->bio;
drivers/block/umem.c:	while (page->headcnt < page->cnt) {
drivers/block/umem.c:		struct bio *bio = page->bio;
drivers/block/umem.c:		struct mm_dma_desc *desc = &page->desc[page->headcnt];
drivers/block/umem.c:		page->headcnt++;
drivers/block/umem.c:		vec = bio_iter_iovec(bio, page->iter);
drivers/block/umem.c:		bio_advance_iter(bio, &page->iter, vec.bv_len);
drivers/block/umem.c:		if (!page->iter.bi_size) {
drivers/block/umem.c:			page->bio = bio->bi_next;
drivers/block/umem.c:			if (page->bio)
drivers/block/umem.c:				page->iter = page->bio->bi_iter;
drivers/block/umem.c:		if (bio != page->bio) {
drivers/block/umem.c:	if (page->headcnt >= page->cnt) {
drivers/block/xen-blkback/xenbus.c:	err = xenbus_printf(XBT_NIL, dev->nodename, "max-ring-page-order", "%u",
drivers/block/xen-blkback/xenbus.c:		pr_warn("%s write out 'max-ring-page-order' failed\n", __func__);
drivers/block/xen-blkback/xenbus.c: * Each ring may have multi pages, depends on "ring-page-order".
drivers/block/xen-blkback/xenbus.c:	err = xenbus_scanf(XBT_NIL, dev->otherend, "ring-page-order", "%u",
drivers/block/xen-blkfront.c:		list_del(&indirect_page->lru);
drivers/block/xen-blkfront.c:			list_del(&indirect_page->lru);
drivers/block/xen-blkfront.c:					list_add(&indirect_page->lru, &rinfo->indirect_pages);
drivers/block/xen-blkfront.c:					      "max-ring-page-order", 0);
drivers/block/xen-blkfront.c:		err = xenbus_printf(xbt, dev->nodename, "ring-page-order", "%u",
drivers/block/xen-blkfront.c:			message = "writing ring-page-order";
drivers/block/xen-blkfront.c:			list_add(&indirect_page->lru, &rinfo->indirect_pages);
drivers/block/xen-blkfront.c:			list_del(&indirect_page->lru);
drivers/block/zram/zram_drv.h:	atomic64_t invalid_io;	/* non-page-aligned I/O requests */
drivers/char/agp/amd-k7-agp.c:#include <linux/page-flags.h>
drivers/char/agp/efficeon-agp.c:#include <linux/page-flags.h>
drivers/char/agp/i460-agp.c:			"I/O (GART) page-size %luKB doesn't match expected "
drivers/char/agp/nvidia-agp.c:#include <linux/page-flags.h>
drivers/crypto/qat/qat_common/qat_uclo.c:	page->encap_page = encap_image->page;
drivers/crypto/qat/qat_common/qat_uclo.c:	ae_slice->page->region = ae_slice->region;
drivers/crypto/qat/qat_common/qat_uclo.c:		patt_pos = page->beg_addr_p + page->micro_words_num;
drivers/crypto/qat/qat_common/qat_uclo.c:				  page->beg_addr_p, &fill_data[0]);
drivers/crypto/qat/qat_common/qat_uclo.c:				  &fill_data[page->beg_addr_p]);
drivers/crypto/qat/qat_common/qat_uclo.c:		     code_page->uc_var_tab_offset);
drivers/crypto/qat/qat_common/qat_uclo.c:		      code_page->imp_var_tab_offset);
drivers/crypto/qat/qat_common/qat_uclo.c:		       code_page->imp_expr_tab_offset);
drivers/crypto/qat/qat_common/qat_uclo.c:			code_page->neigh_reg_tab_offset);
drivers/crypto/qat/qat_common/qat_uclo.c:	page->def_page = code_page->def_page;
drivers/crypto/qat/qat_common/qat_uclo.c:	page->page_region = code_page->page_region;
drivers/crypto/qat/qat_common/qat_uclo.c:	page->beg_addr_v = code_page->beg_addr_v;
drivers/crypto/qat/qat_common/qat_uclo.c:	page->beg_addr_p = code_page->beg_addr_p;
drivers/crypto/qat/qat_common/qat_uclo.c:						code_page->code_area_offset);
drivers/crypto/qat/qat_common/qat_uclo.c:	page->micro_words_num = code_area->micro_words_num;
drivers/crypto/qat/qat_common/qat_uclo.c:	page->uwblock_num = uword_block_tab->entry_num;
drivers/crypto/qat/qat_common/qat_uclo.c:	page->uwblock = (struct icp_qat_uclo_encap_uwblock *)uwblock;
drivers/crypto/qat/qat_common/qat_uclo.c:		page->uwblock[i].micro_words =
drivers/crypto/qat/qat_common/qat_uclo.c:		image->uwords_num = image->page->beg_addr_p +
drivers/crypto/qat/qat_common/qat_uclo.c:					image->page->micro_words_num;
drivers/crypto/qat/qat_common/qat_uclo.c:	for (i = 0; i < encap_page->uwblock_num; i++) {
drivers/crypto/qat/qat_common/qat_uclo.c:		if (raddr >= encap_page->uwblock[i].start_addr &&
drivers/crypto/qat/qat_common/qat_uclo.c:		    raddr <= encap_page->uwblock[i].start_addr +
drivers/crypto/qat/qat_common/qat_uclo.c:		    encap_page->uwblock[i].words_num - 1) {
drivers/crypto/qat/qat_common/qat_uclo.c:			raddr -= encap_page->uwblock[i].start_addr;
drivers/crypto/qat/qat_common/qat_uclo.c:			       encap_page->uwblock[i].micro_words) + raddr),
drivers/crypto/qat/qat_common/qat_uclo.c:	uw_physical_addr = encap_page->beg_addr_p;
drivers/crypto/qat/qat_common/qat_uclo.c:	words_num = encap_page->micro_words_num;
drivers/crypto/qat/qat_common/qat_uclo.c:		if (!page->encap_page->def_page)
drivers/crypto/qat/qat_common/qat_uclo.c:		qat_uclo_wr_uimage_raw_page(handle, page->encap_page, ae);
drivers/dax/device.c:			if (page->mapping)
drivers/dax/device.c:			page->mapping = filp->f_mapping;
drivers/dax/device.c:			page->index = pgoff + i;
drivers/dma-buf/dma-buf.c: *   To support dma_buf objects residing in highmem cpu access is page-based
drivers/dma/nbpfaxi.c:	for (i = 0, ldesc = dpage->ldesc, hwdesc = dpage->hwdesc;
drivers/dma/nbpfaxi.c:	     i < ARRAY_SIZE(dpage->ldesc);
drivers/dma/nbpfaxi.c:	for (i = 0, desc = dpage->desc;
drivers/dma/nbpfaxi.c:	     i < ARRAY_SIZE(dpage->desc);
drivers/dma/nbpfaxi.c:	list_add(&dpage->node, &chan->desc_page);
drivers/dma/nbpfaxi.c:	return ARRAY_SIZE(dpage->desc);
drivers/dma/nbpfaxi.c:		list_del(&dpage->node);
drivers/dma/nbpfaxi.c:		for (i = 0, ldesc = dpage->ldesc;
drivers/dma/nbpfaxi.c:		     i < ARRAY_SIZE(dpage->ldesc);
drivers/dma/ppc4xx/adma.c:		 * we sort out cases where temporary page-sized buffers
drivers/dma/sh/rcar-dmac.c:		struct rcar_dmac_desc *desc = &page->descs[i];
drivers/dma/sh/rcar-dmac.c:	list_add_tail(&page->node, &chan->desc.pages);
drivers/dma/sh/rcar-dmac.c:		struct rcar_dmac_xfer_chunk *chunk = &page->chunks[i];
drivers/dma/sh/rcar-dmac.c:	list_add_tail(&page->node, &chan->desc.pages);
drivers/dma/sh/rcar-dmac.c:		list_del(&page->node);
drivers/firewire/ohci.c:	 * List of page-sized buffers for storing DMA descriptors.
drivers/firmware/efi/libstub/efi-stub-helper.c:	 * requesting a specific address.  We are doing page-based (or
drivers/firmware/efi/libstub/efi-stub-helper.c:	 * requesting a specific address.  We are doing page-based (or
drivers/fpga/dfl-afu-dma-region.c:	 * Check Inputs, only accept page-aligned user memory region with
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	return page->kernel_address;
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	page->kernel_address = backing_store;
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	page->need_to_free_pages = true;
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	ev->user_signal_address = &p->signal_page->user_address[ev->event_id];
drivers/gpu/drm/amd/amdkfd/kfd_events.c:		if (page->need_to_free_pages)
drivers/gpu/drm/amd/amdkfd/kfd_events.c:			free_pages((unsigned long)page->kernel_address,
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	page->kernel_address = kernel_address;
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	pfn = __pa(page->kernel_address);
drivers/gpu/drm/amd/amdkfd/kfd_events.c:	page->user_address = (uint64_t __user *)vma->vm_start;
drivers/gpu/drm/drm_bufs.c:	/* page-align _DRM_SHM maps. They are allocated here so there is no security
drivers/gpu/drm/drm_connector.c: *	vertical front porch duration will be extended until page-flip or
drivers/gpu/drm/drm_connector.c: *	refresh rate timings. The semantics for the page-flip event
drivers/gpu/drm/drm_damage_helper.c: * page-flip), irrespective of whether currently attached framebuffer is same as
drivers/gpu/drm/drm_fb_helper.c:		start = page->index << PAGE_SHIFT;
drivers/gpu/drm/drm_gem.c: * This reads the page-array of the shmem-backing storage of the given gem
drivers/gpu/drm/drm_gem.c: * whole object is covered by the page-array and pinned in memory.
drivers/gpu/drm/drm_gem.c:	/* We already BUG_ON() for non-page-aligned sizes in
drivers/gpu/drm/drm_gem.c:	/* We already BUG_ON() for non-page-aligned sizes in
drivers/gpu/drm/drm_memory.c:	 * page-table instead (that's probably faster anyhow...).
drivers/gpu/drm/drm_vma_manager.c: * This offset manager works on page-based addresses. That is, every argument
drivers/gpu/drm/drm_vma_manager.c: * must always be page-aligned (as usual).
drivers/gpu/drm/drm_vma_manager.c: * @page_offset: Offset of available memory area (page-based)
drivers/gpu/drm/drm_vma_manager.c: * @size: Size of available address space range (page-based)
drivers/gpu/drm/drm_vma_manager.c: * page-numbers, not bytes.
drivers/gpu/drm/drm_vma_manager.c: * @start: Start address for object (page-based)
drivers/gpu/drm/drm_vma_manager.c: * @pages: Size of object (page-based)
drivers/gpu/drm/etnaviv/etnaviv_drv.h:	 * the context's page-tables here.
drivers/gpu/drm/i915/gvt/gtt.c:	struct intel_vgpu_ppgtt_spt *spt = oos_page->spt;
drivers/gpu/drm/i915/gvt/gtt.c:	trace_oos_change(vgpu->id, "sync", oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:		ops->get_entry(oos_page->mem, &old, index, false, 0, vgpu);
drivers/gpu/drm/i915/gvt/gtt.c:		trace_oos_sync(vgpu->id, oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:		ops->set_entry(oos_page->mem, &new, index, false, 0, vgpu);
drivers/gpu/drm/i915/gvt/gtt.c:	struct intel_vgpu_ppgtt_spt *spt = oos_page->spt;
drivers/gpu/drm/i915/gvt/gtt.c:	trace_oos_change(vgpu->id, "detach", oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:	oos_page->spt = NULL;
drivers/gpu/drm/i915/gvt/gtt.c:	list_del_init(&oos_page->vm_list);
drivers/gpu/drm/i915/gvt/gtt.c:	list_move_tail(&oos_page->list, &gvt->gtt.oos_page_free_list_head);
drivers/gpu/drm/i915/gvt/gtt.c:			oos_page->mem, I915_GTT_PAGE_SIZE);
drivers/gpu/drm/i915/gvt/gtt.c:	oos_page->spt = spt;
drivers/gpu/drm/i915/gvt/gtt.c:	list_move_tail(&oos_page->list, &gvt->gtt.oos_page_use_list_head);
drivers/gpu/drm/i915/gvt/gtt.c:	trace_oos_change(spt->vgpu->id, "attach", oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:	trace_oos_change(spt->vgpu->id, "set page sync", oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:	list_del_init(&oos_page->vm_list);
drivers/gpu/drm/i915/gvt/gtt.c:		ret = ppgtt_set_guest_page_sync(oos_page->spt);
drivers/gpu/drm/i915/gvt/gtt.c:	trace_oos_change(spt->vgpu->id, "set page out of sync", oos_page->id,
drivers/gpu/drm/i915/gvt/gtt.c:	list_add_tail(&oos_page->vm_list, &spt->vgpu->gtt.oos_page_list_head);
drivers/gpu/drm/i915/gvt/gtt.c:		ret = ppgtt_set_guest_page_sync(oos_page->spt);
drivers/gpu/drm/i915/gvt/gtt.c:		ops->set_entry(spt->guest_page.oos_page->mem, &we, index,
drivers/gpu/drm/i915/gvt/gtt.c:		list_del(&oos_page->list);
drivers/gpu/drm/i915/gvt/gtt.c:		INIT_LIST_HEAD(&oos_page->list);
drivers/gpu/drm/i915/gvt/gtt.c:		INIT_LIST_HEAD(&oos_page->vm_list);
drivers/gpu/drm/i915/gvt/gtt.c:		oos_page->id = i;
drivers/gpu/drm/i915/gvt/gtt.c:		list_add_tail(&oos_page->list, &gtt->oos_page_free_list_head);
drivers/gpu/drm/i915/i915_drv.h: * you only need to pass in the minor offsets, page-aligned pointers are
drivers/gpu/drm/i915/i915_gem.c:		/* Note that the gtt paths might fail with non-page-backed user
drivers/gpu/drm/i915/i915_gem.c: *     we called the page-fault-of-doom where we would ping-pong between
drivers/gpu/drm/i915/i915_gem.c:	 * Calculate the supported page-sizes which fit into the given
drivers/gpu/drm/i915/i915_gem.c:	 * sg_page_sizes. This will give us the page-sizes which we may be able
drivers/gpu/drm/i915/i915_gem_execbuffer.c:	 * any non-page-aligned or non-canonical addresses.
drivers/gpu/drm/i915/i915_gem_gtt.c:	 * page-table operating in 64K mode must point to a properly aligned 64K
drivers/gpu/drm/i915/i915_gem_gtt.c:		 * filled whole page-table with 64K entries, or filled part of
drivers/gpu/drm/i915/i915_gem_gtt.c:	 * mmio, otherwise the page-walker will simply ignore the IPS bit. This
drivers/gpu/drm/i915/i915_gem_gtt.c:	 * GTT. On 48b / 4-level page-tables, the difference is very,
drivers/gpu/drm/i915/i915_gem_stolen.c:	/* KISS and expect everything to be page-aligned */
drivers/gpu/drm/i915/i915_vma.c:			 * We can't mix 64K and 4K PTEs in the same page-table
drivers/gpu/drm/i915/selftests/huge_pages.c:	 * largest to smallest page-size, while ensuring that we use *every*
drivers/gpu/drm/i915/selftests/huge_pages.c:	 * page-size as per the given page-mask.
drivers/gpu/drm/i915/selftests/huge_pages.c:					"%s timed out at offset %x with page-size %x\n",
drivers/gpu/drm/i915/selftests/huge_pages.c:	 * safely mark the whole page-table(2M block) as 64K, or we have to
drivers/gpu/drm/i915/selftests/huge_pages.c:		 * For our page mask we want to enumerate all the page-size
drivers/gpu/drm/i915/selftests/huge_pages.c:			/* Force the page-size for the gtt insertion */
drivers/gpu/drm/i915/selftests/huge_pages.c:			pr_info("Unable to allocate page-size %x, finishing test early\n",
drivers/gpu/drm/i915/selftests/huge_pages.c:		pr_err("residual page-size bits left\n");
drivers/gpu/drm/msm/disp/dpu1/dpu_rm.c:	/* Check if this is just a page-flip */
drivers/gpu/drm/msm/msm_gem_submit.c:	 * to do it page-by-page, w/ kmap() if not vmap()d..
drivers/gpu/drm/nouveau/nouveau_display.c:		/* Give up ownership of vblank for page-flipped crtc */
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	pgt->page = page ? page->shift : 0;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	const struct nvkm_vmm_desc *desc = page->desc;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	u64 bits = addr >> page->shift;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	it.cnt = size >> page->shift;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	         addr, size, page->shift, it.cnt);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	return addr << page->shift;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		      page->desc->func->invalid ?
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		      page->desc->func->invalid : page->desc->func->unmap);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	if ((page->type & NVKM_VMM_PAGE_SPARSE)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:					 NULL, page->desc->func->sparse);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	const struct nvkm_vmm_desc_func *func = page->desc->func;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	const struct nvkm_vmm_desc_func *func = page->desc->func;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	for (levels = 0, desc = page->desc; desc->bits; desc++, levels++)
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	bits += page->shift;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		if (!(map->page->type & NVKM_VMM_PAGE_VRAM)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:			VMM_DEBUG(vmm, "%d !VRAM", map->page->shift);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		if (!(map->page->type & NVKM_VMM_PAGE_HOST)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:			VMM_DEBUG(vmm, "%d !HOST", map->page->shift);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	if (!IS_ALIGNED(     vma->addr, 1ULL << map->page->shift) ||
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	    !IS_ALIGNED((u64)vma->size, 1ULL << map->page->shift) ||
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	    !IS_ALIGNED(   map->offset, 1ULL << map->page->shift) ||
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	    nvkm_memory_page(map->memory) < map->page->shift) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		    vma->addr, (u64)vma->size, map->offset, map->page->shift,
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:	for (map->page = vmm->func->page; map->page->shift; map->page++) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		VMM_DEBUG(vmm, "trying %d", map->page->shift);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		func = map->page->desc->func->mem;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		func = map->page->desc->func->sgl;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		func = map->page->desc->func->dma;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		for (page = vmm->func->page; page->shift; page++) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:			if (shift == page->shift)
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c:		if (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.h:		u64 _ptes = ((SIZE) - MAP->off) >> MAP->page->shift;           \
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.h:			MAP->off += PTEN << MAP->page->shift;                  \
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:	if (map->page->shift == PAGE_SHIFT) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:	const bool gm20x = page->desc->func->sparse != NULL;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:	map->next = (1 << page->shift) >> 8;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:		u32 comp = (page->shift == 16 && !gm20x) ? 16 : 17;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:		if (aper != 0 || !(page->type & NVKM_VMM_PAGE_COMP)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:			VMM_DEBUG(vmm, "comp %d %02x", aper, page->type);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgf100.c:			if (page->shift == 17 || !gm20x) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgp100.c:	if (map->page->shift == PAGE_SHIFT) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgp100.c:	map->next = (1ULL << page->shift) >> 4;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgp100.c:		if (aper != 0 || !(page->type & NVKM_VMM_PAGE_COMP)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgp100.c:			VMM_DEBUG(vmm, "comp %d %02x", aper, page->type);
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmgp100.c:			map->ctag |= ((1ULL << page->shift) >> 16) << 36;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmnv50.c:	if (map->page->shift == PAGE_SHIFT) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmnv50.c:	map->next = 1 << page->shift;
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmnv50.c:		if (aper != 0 || !(page->type & NVKM_VMM_PAGE_COMP)) {
drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmmnv50.c:			VMM_DEBUG(vmm, "comp %d %02x", aper, page->type);
drivers/gpu/drm/omapdrm/omap_gem.c: * can create a second page-aligned mapping of parts of the buffer
drivers/gpu/drm/radeon/r100.c:	/* set PCI GART page-table base address */
drivers/gpu/drm/ttm/ttm_bo_vm.c:			page->index = drm_vma_node_start(&bo->vma_node) +
drivers/gpu/drm/ttm/ttm_page_alloc.c: * - Use page->lru to keep a free list
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	dma_addr_t dma = d_page->dma;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	d_page->vaddr &= ~VADDR_FLAG_HUGE_POOL;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	dma_free_coherent(pool->dev, pool->size, (void *)d_page->vaddr, dma);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	vaddr = dma_alloc_attrs(pool->dev, pool->size, &d_page->dma,
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			d_page->p = vmalloc_to_page(vaddr);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			d_page->p = virt_to_page(vaddr);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		d_page->vaddr = (unsigned long)vaddr;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			d_page->vaddr |= VADDR_FLAG_HUGE_POOL;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	struct page *page = d_page->p;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:	list_del(&d_page->page_list);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		list_del(&d_page->page_list);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		if (d_page->p != p)
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		list_del(&d_page->page_list);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		ttm->pages[index] = d_page->p;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		ttm_dma->dma_address[index] = d_page->dma;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		list_move_tail(&d_page->page_list, &ttm_dma->pages_list);
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		d_page->vaddr |= VADDR_FLAG_UPDATED_COUNT;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		d_page->vaddr |= VADDR_FLAG_UPDATED_COUNT;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			if (!(d_page->vaddr & VADDR_FLAG_HUGE_POOL))
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			if (d_page->vaddr & VADDR_FLAG_UPDATED_COUNT) {
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:				ttm_mem_global_free_page(mem_glob, d_page->p,
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:				d_page->vaddr &= ~VADDR_FLAG_UPDATED_COUNT;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		ttm->pages[count] = d_page->p;
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:		if (d_page->vaddr & VADDR_FLAG_UPDATED_COUNT) {
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			ttm_mem_global_free_page(mem_glob, d_page->p,
drivers/gpu/drm/ttm/ttm_page_alloc_dma.c:			d_page->vaddr &= ~VADDR_FLAG_UPDATED_COUNT;
drivers/gpu/drm/vc4/vc4_crtc.c:	 * FIXME: we should move to generic async-page-flip when it's
drivers/gpu/drm/vc4/vc4_crtc.c:	 * FIXME: we should move to generic async-page-flip when it's
drivers/gpu/drm/vc4/vc4_crtc.c:	 * FIXME: we should move to generic async-page-flip when it's
drivers/gpu/drm/via/via_dmablit.c:		if (descriptor_this_page-- == 0) {
drivers/gpu/drm/via/via_dmablit.c:			cur_descriptor_page--;
drivers/gpu/drm/vmwgfx/device_include/svga3d_dx.h: *      how many page-outs have happened.)
drivers/gpu/drm/vmwgfx/vmwgfx_drv.c: * vmw_dma_masks - set required page- and dma masks
drivers/gpu/drm/vmwgfx/vmwgfx_drv.h: * @size:           Size of the table (page-aligned).
drivers/gpu/drm/vmwgfx/vmwgfx_fb.c:		start = page->index << PAGE_SHIFT;
drivers/gpu/drm/vmwgfx/vmwgfx_mob.c: * @size:           Size of the table (page-aligned).
drivers/gpu/drm/vmwgfx/vmwgfx_validation.c:		list_add_tail(&page->lru, &ctx->page_list);
drivers/gpu/drm/vmwgfx/vmwgfx_validation.c: * @cpu_blit: Validate in a page-mappable location.
drivers/gpu/drm/xen/xen_drm_front_evtchnl.c:	prod = page->in_prod;
drivers/gpu/drm/xen/xen_drm_front_evtchnl.c:	if (prod == page->in_cons)
drivers/gpu/drm/xen/xen_drm_front_evtchnl.c:	for (cons = page->in_cons; cons != prod; cons++) {
drivers/gpu/drm/xen/xen_drm_front_evtchnl.c:	page->in_cons = cons;
drivers/hv/channel.c:			(unsigned long *)&monitorpage->trigger_group
drivers/hv/channel.c: * @size: page-size multiple
drivers/hv/vmbus_drv.c:	return monitor_page->trigger_group[monitor_group].pending;
drivers/hv/vmbus_drv.c:	return monitor_page->latency[monitor_group][monitor_offset];
drivers/hv/vmbus_drv.c:	return monitor_page->parameter[monitor_group][monitor_offset].connectionid.u.id;
drivers/hwtracing/intel_th/msu.c:		page->mapping = NULL;
drivers/hwtracing/intel_th/msu.c:		page->mapping = NULL;
drivers/hwtracing/intel_th/msu.c:		if (page->mapping)
drivers/hwtracing/intel_th/msu.c:			page->mapping = NULL;
drivers/hwtracing/intel_th/msu.c:	vmf->page->mapping = vmf->vma->vm_file->f_mapping;
drivers/hwtracing/intel_th/msu.c:	vmf->page->index = vmf->pgoff;
drivers/infiniband/hw/bnxt_re/qplib_rcfw.c:	/* Supply (log-base-2-of-host-page-size - base-page-shift)
drivers/infiniband/hw/bnxt_re/roce_hsi.h:	/* This value is (log-base-2-of-DBR-page-size - 12).
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->qp_start = rdev->lldi.vr->qp.start;
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->qp_size = rdev->lldi.vr->qp.size;
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->cq_start = rdev->lldi.vr->cq.start;
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->cq_size = rdev->lldi.vr->cq.size;
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->write_cmpl_supported = rdev->lldi.write_cmpl_support;
drivers/infiniband/hw/cxgb4/device.c:	rdev->status_page->db_off = 0;
drivers/infiniband/hw/cxgb4/device.c:		ctx->dev->rdev.status_page->db_off = 1;
drivers/infiniband/hw/cxgb4/device.c:				ctx->dev->rdev.status_page->db_off = 0;
drivers/infiniband/hw/cxgb4/qp.c: * then this is a user mapping so compute the page-aligned physical address
drivers/infiniband/hw/cxgb4/qp.c:	if (!rhp->rdev.status_page->db_off) {
drivers/infiniband/hw/cxgb4/qp.c:	if (!qhp->rhp->rdev.status_page->db_off) {
drivers/infiniband/hw/hfi1/init.c:	 * size to the max MTU (page-aligned).
drivers/infiniband/hw/hns/hns_roce_db.c:		if (page->user_virt == (virt & PAGE_MASK))
drivers/infiniband/hw/hns/hns_roce_db.c:	refcount_set(&page->refcount, 1);
drivers/infiniband/hw/hns/hns_roce_db.c:	page->user_virt = (virt & PAGE_MASK);
drivers/infiniband/hw/hns/hns_roce_db.c:	page->umem = ib_umem_get(&context->ibucontext, virt & PAGE_MASK,
drivers/infiniband/hw/hns/hns_roce_db.c:	if (IS_ERR(page->umem)) {
drivers/infiniband/hw/hns/hns_roce_db.c:		ret = PTR_ERR(page->umem);
drivers/infiniband/hw/hns/hns_roce_db.c:	list_add(&page->list, &context->page_list);
drivers/infiniband/hw/hns/hns_roce_db.c:	db->dma = sg_dma_address(page->umem->sg_head.sgl) +
drivers/infiniband/hw/hns/hns_roce_db.c:	page->umem->sg_head.sgl->offset = virt & ~PAGE_MASK;
drivers/infiniband/hw/hns/hns_roce_db.c:	db->virt_addr = sg_virt(page->umem->sg_head.sgl);
drivers/infiniband/hw/hns/hns_roce_db.c:	refcount_inc(&page->refcount);
drivers/infiniband/hw/hns/hns_roce_db.c:	refcount_dec(&db->u.user_page->refcount);
drivers/infiniband/hw/hns/hns_roce_db.c:	if (refcount_dec_if_one(&db->u.user_page->refcount)) {
drivers/infiniband/hw/hns/hns_roce_db.c:		list_del(&db->u.user_page->list);
drivers/infiniband/hw/hns/hns_roce_db.c:		ib_umem_release(db->u.user_page->umem);
drivers/infiniband/hw/i40iw/i40iw_hmc.c:		page_desc = page->pa | 0x1;
drivers/infiniband/hw/mlx4/doorbell.c:		if (page->user_virt == (virt & PAGE_MASK))
drivers/infiniband/hw/mlx4/doorbell.c:	page->user_virt = (virt & PAGE_MASK);
drivers/infiniband/hw/mlx4/doorbell.c:	page->refcnt    = 0;
drivers/infiniband/hw/mlx4/doorbell.c:	page->umem      = ib_umem_get(&context->ibucontext, virt & PAGE_MASK,
drivers/infiniband/hw/mlx4/doorbell.c:	if (IS_ERR(page->umem)) {
drivers/infiniband/hw/mlx4/doorbell.c:		err = PTR_ERR(page->umem);
drivers/infiniband/hw/mlx4/doorbell.c:	list_add(&page->list, &context->db_page_list);
drivers/infiniband/hw/mlx4/doorbell.c:	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
drivers/infiniband/hw/mlx4/doorbell.c:	++page->refcnt;
drivers/infiniband/hw/mlx4/doorbell.c:	if (!--db->u.user_page->refcnt) {
drivers/infiniband/hw/mlx4/doorbell.c:		list_del(&db->u.user_page->list);
drivers/infiniband/hw/mlx4/doorbell.c:		ib_umem_release(db->u.user_page->umem);
drivers/infiniband/hw/mlx5/doorbell.c:		if (page->user_virt == (virt & PAGE_MASK))
drivers/infiniband/hw/mlx5/doorbell.c:	page->user_virt = (virt & PAGE_MASK);
drivers/infiniband/hw/mlx5/doorbell.c:	page->refcnt    = 0;
drivers/infiniband/hw/mlx5/doorbell.c:	page->umem      = ib_umem_get(&context->ibucontext, virt & PAGE_MASK,
drivers/infiniband/hw/mlx5/doorbell.c:	if (IS_ERR(page->umem)) {
drivers/infiniband/hw/mlx5/doorbell.c:		err = PTR_ERR(page->umem);
drivers/infiniband/hw/mlx5/doorbell.c:	list_add(&page->list, &context->db_page_list);
drivers/infiniband/hw/mlx5/doorbell.c:	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
drivers/infiniband/hw/mlx5/doorbell.c:	++page->refcnt;
drivers/infiniband/hw/mlx5/doorbell.c:	if (!--db->u.user_page->refcnt) {
drivers/infiniband/hw/mlx5/doorbell.c:		list_del(&db->u.user_page->list);
drivers/infiniband/hw/mlx5/doorbell.c:		ib_umem_release(db->u.user_page->umem);
drivers/infiniband/hw/mlx5/mr.c:		/* Wait for all running page-fault handlers to finish. */
drivers/infiniband/hw/mlx5/odp.c: * Handle a single data segment in a page-fault WQE or RDMA region.
drivers/infiniband/hw/mthca/mthca_memfree.c:	page->db_rec = dma_alloc_coherent(&dev->pdev->dev,
drivers/infiniband/hw/mthca/mthca_memfree.c:					  MTHCA_ICM_PAGE_SIZE, &page->mapping,
drivers/infiniband/hw/mthca/mthca_memfree.c:	if (!page->db_rec) {
drivers/infiniband/hw/mthca/mthca_memfree.c:	ret = mthca_MAP_ICM_page(dev, page->mapping,
drivers/infiniband/hw/mthca/mthca_memfree.c:				  page->db_rec, page->mapping);
drivers/infiniband/hw/mthca/mthca_memfree.c:	bitmap_zero(page->used, MTHCA_DB_REC_PER_PAGE);
drivers/infiniband/hw/mthca/mthca_memfree.c:	j = find_first_zero_bit(page->used, MTHCA_DB_REC_PER_PAGE);
drivers/infiniband/hw/mthca/mthca_memfree.c:	set_bit(j, page->used);
drivers/infiniband/hw/mthca/mthca_memfree.c:	page->db_rec[j] = cpu_to_be64((qn << 8) | (type << 5));
drivers/infiniband/hw/mthca/mthca_memfree.c:	*db = (__be32 *) &page->db_rec[j];
drivers/infiniband/hw/mthca/mthca_memfree.c:	page->db_rec[j] = 0;
drivers/infiniband/hw/mthca/mthca_memfree.c:	clear_bit(j, page->used);
drivers/infiniband/hw/mthca/mthca_memfree.c:	if (bitmap_empty(page->used, MTHCA_DB_REC_PER_PAGE) &&
drivers/infiniband/hw/mthca/mthca_memfree.c:				  page->db_rec, page->mapping);
drivers/infiniband/hw/mthca/mthca_memfree.c:		page->db_rec = NULL;
drivers/infiniband/ulp/isert/ib_isert.c:	 * FIXME: Non page-aligned unsolicited_data out
drivers/input/misc/xen-kbdfront.c:	prod = page->in_prod;
drivers/input/misc/xen-kbdfront.c:	if (prod == page->in_cons)
drivers/input/misc/xen-kbdfront.c:	for (cons = page->in_cons; cons != prod; cons++)
drivers/input/misc/xen-kbdfront.c:	page->in_cons = cons;
drivers/iommu/Kconfig:	  a series of page-table consistency checks during boot.
drivers/iommu/Kconfig:	  a series of page-table consistency checks during boot.
drivers/iommu/amd_iommu_types.h: * Takes a page-table level and returns the default page-size for this level
drivers/iommu/dma-iommu.c:		if (msi_page->phys == msi_addr)
drivers/iommu/dma-iommu.c:	INIT_LIST_HEAD(&msi_page->list);
drivers/iommu/dma-iommu.c:	msi_page->phys = msi_addr;
drivers/iommu/dma-iommu.c:	msi_page->iova = iova;
drivers/iommu/dma-iommu.c:	list_add(&msi_page->list, &cookie->msi_page_list);
drivers/iommu/dma-iommu.c:		msg->address_hi = upper_32_bits(msi_page->iova);
drivers/iommu/dma-iommu.c:		msg->address_lo += lower_32_bits(msi_page->iova);
drivers/iommu/intel-iommu.c:   know the hardware page-walk will no longer touch them.
drivers/iommu/intel-iommu.c:		pgd_page->freelist = freelist;
drivers/iommu/omap-iopgtable.h:/* to find an entry in a page-table-directory */
drivers/md/md-bitmap.c:			page->index = index;
drivers/md/md-bitmap.c:		if (page->index == store->file_pages-1) {
drivers/md/md-bitmap.c:			if (rdev->sb_start + offset + (page->index
drivers/md/md-bitmap.c:			    + (long)(page->index * (PAGE_SIZE/512))
drivers/md/md-bitmap.c:			    + page->index*(PAGE_SIZE/512) + size/512
drivers/md/md-bitmap.c:			       + page->index * (PAGE_SIZE/512),
drivers/md/md-bitmap.c:	page->index = index;
drivers/md/md-bitmap.c:	bitmap->storage.sb_page->index = 0;
drivers/md/md-bitmap.c:		store->sb_page->index = offset;
drivers/md/md-bitmap.c:	pr_debug("set file bit %lu page %lu\n", bit, page->index);
drivers/md/md-bitmap.c:	set_page_attr(bitmap, page->index - node_offset, BITMAP_PAGE_DIRTY);
drivers/md/md-bitmap.c:	if (!test_page_attr(bitmap, page->index - node_offset, BITMAP_PAGE_NEEDWRITE)) {
drivers/md/md-bitmap.c:		set_page_attr(bitmap, page->index - node_offset, BITMAP_PAGE_PENDING);
drivers/md/raid10.c:	 * We repeat the read in smaller page-sized sections.
drivers/media/common/saa7146/saa7146_hlp.c:	   most likely wrong, this version here only works for page-aligned
drivers/media/common/videobuf2/videobuf2-dma-sg.c:				while (last_page--)
drivers/media/common/videobuf2/videobuf2-dma-sg.c:	/* add offset in case userptr is not page-aligned */
drivers/media/pci/b2c2/flexcop-pci.c: * filtering is turned off, we use the page-change-IRQ */
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->uaddr = first & PAGE_MASK;
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->offset = first & ~PAGE_MASK;
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->tail = 1 + ((first+size-1) & ~PAGE_MASK);
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->first = (first & PAGE_MASK) >> PAGE_SHIFT;
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->last = ((first+size-1) & PAGE_MASK) >> PAGE_SHIFT;
drivers/media/pci/ivtv/ivtv-udma.c:	dma_page->page_count = dma_page->last - dma_page->first + 1;
drivers/media/pci/ivtv/ivtv-udma.c:	if (dma_page->page_count == 1) dma_page->tail -= dma_page->offset;
drivers/media/pci/ivtv/ivtv-udma.c:	offset = dma_page->offset;
drivers/media/pci/ivtv/ivtv-udma.c:	for (i = 0; i < dma_page->page_count; i++) {
drivers/media/pci/ivtv/ivtv-udma.c:		unsigned int len = (i == dma_page->page_count - 1) ?
drivers/media/pci/ivtv/ivtv-udma.c:			dma_page->tail : PAGE_SIZE - offset;
drivers/media/pci/pt1/pt1.c:	if (!page->upackets[PT1_NR_UPACKETS - 1])
drivers/media/pci/pt1/pt1.c:		upacket = le32_to_cpu(page->upackets[i]);
drivers/media/pci/pt1/pt1.c:	page->upackets[PT1_NR_UPACKETS - 1] = 0;
drivers/media/pci/pt1/pt1.c:	page->upackets[PT1_NR_UPACKETS - 1] = 0;
drivers/media/pci/pt1/pt1.c:		page->buf_pfns[i] = cpu_to_le32(buf_pfn);
drivers/media/pci/pt1/pt1.c:		tables[i - 1].page->next_pfn = cpu_to_le32(pfn);
drivers/media/pci/pt1/pt1.c:	tables[pt1_nr_tables - 1].page->next_pfn = cpu_to_le32(first_pfn);
drivers/media/pci/pt1/pt1.c:			pt1->tables[i].bufs[j].page->upackets[PT1_NR_UPACKETS-1]
drivers/media/pci/saa7134/saa7134-core.c:MODULE_PARM_DESC(saa7134_userptr, "enable page-aligned userptr support");
drivers/media/pci/saa7134/saa7134-core.c:	size += PAGE_SIZE; /* for non-page-aligned buffers */
drivers/media/pci/saa7134/saa7134-vbi.c:		pr_err("The buffer is not page-aligned\n");
drivers/media/pci/saa7134/saa7134-video.c:		pr_err("The buffer is not page-aligned\n");
drivers/media/v4l2-core/videobuf-dma-sg.c: * Return a scatterlist for some page-aligned vmalloc()'ed memory
drivers/memory/atmel-ebi.c:	ret = of_property_read_u32(np, "atmel,smc-page-mode", &tmp);
drivers/memory/omap-gpmc.c:	GPMC_GET_TICKS(GPMC_CS_CONFIG5, 24, 27, "page-burst-access-ns");
drivers/memory/omap-gpmc.c:	of_property_read_u32(np, "gpmc,page-burst-access-ns",
drivers/memory/pl172.c:	if (of_property_read_bool(np, "mpmc,async-page-mode"))
drivers/memory/pl172.c:	ret = pl172_timing_prop(adev, np, "mpmc,page-mode-read-delay",
drivers/memory/samsung/exynos-srom.c:	if (of_property_read_bool(np, "samsung,srom-page-mode"))
drivers/memstick/core/ms_block.c:		if (be16_to_cpu(page->header.block_id) != MS_BLOCK_BOOT_ID) {
drivers/misc/eeprom/at25.c:	    device_property_read_u32(dev, "at25,page-size", &val) == 0) {
drivers/misc/genwqe/card_utils.c:#include <linux/page-flags.h>
drivers/misc/sgi-xp/xpc_partition.c:	xpc_rsvd_page->ts_jiffies = 0;
drivers/misc/sgi-xp/xpc_uv.c:	msg_hdr->rp_ts_jiffies = xpc_rsvd_page->ts_jiffies;
drivers/misc/sgi-xp/xpc_uv.c:		msg.heartbeat_gpa = xpc_rsvd_page->sn.uv.heartbeat_gpa;
drivers/misc/sgi-xp/xpc_uv.c:		    xpc_rsvd_page->sn.uv.activate_gru_mq_desc_gpa;
drivers/misc/sram-exec.c: * writeable and executable at the same time. This region must be page-aligned
drivers/misc/vmw_balloon.c:			list_add(&page->lru, &ctl->pages);
drivers/misc/vmw_balloon.c:		list_move(&page->lru, &ctl->refused_pages);
drivers/misc/vmw_balloon.c:		list_del(&page->lru);
drivers/misc/vmw_balloon.c:		list_move(&page->lru, pages);
drivers/mmc/host/usdhi6rol0.c:	 * Setting .max_seg_size to 1 page would simplify our page-mapping code,
drivers/mtd/devices/st_spi_fsm.c:		/* We are now page-aligned */
drivers/mtd/mtdswap.c:		page--;
drivers/mtd/mtdswap.c:		page--;
drivers/mtd/nand/raw/brcmnand/brcmnand.c: *     roundup(log2(size / page-size) / 8)
drivers/mtd/nand/raw/nandsim.c:	if (mypage->byte == NULL) {
drivers/mtd/nand/raw/nandsim.c:		if (mypage->byte != NULL) {
drivers/mtd/nand/raw/nandsim.c:			kmem_cache_free(ns->nand_pages_slab, mypage->byte);
drivers/mtd/nand/raw/nandsim.c:			mypage->byte = NULL;
drivers/mtd/nand/raw/nandsim.c:	if (mypage->byte == NULL) {
drivers/mtd/nand/raw/nandsim.c:		mypage->byte = kmem_cache_alloc(ns->nand_pages_slab, GFP_NOFS);
drivers/mtd/nand/raw/nandsim.c:		if (mypage->byte == NULL) {
drivers/mtd/nand/raw/nandsim.c:		memset(mypage->byte, 0xFF, ns->geom.pgszoob);
drivers/net/can/janz-ican3.c:	/* make sure we page-align the next queue */
drivers/net/can/peak_canfd/peak_pciefd_main.c:	if (page->offset + msg_size > page->size) {
drivers/net/can/peak_canfd/peak_pciefd_main.c:		lk = page->vbase + page->offset;
drivers/net/can/peak_canfd/peak_pciefd_main.c:		lk->laddr_lo = cpu_to_le32(page->lbase);
drivers/net/can/peak_canfd/peak_pciefd_main.c:		lk->laddr_hi = cpu_to_le32(page->lbase >> 32);
drivers/net/can/peak_canfd/peak_pciefd_main.c:		page->offset = 0;
drivers/net/can/peak_canfd/peak_pciefd_main.c:	*room_left = priv->tx_pages_free * page->size;
drivers/net/can/peak_canfd/peak_pciefd_main.c:	msg = page->vbase + page->offset;
drivers/net/can/peak_canfd/peak_pciefd_main.c:	*room_left += page->size - (page->offset + msg_size);
drivers/net/can/peak_canfd/peak_pciefd_main.c:	page->offset += le16_to_cpu(msg->size);
drivers/net/ethernet/8390/axnet_cs.c:	outb_p(ei_local->stop_page-1, e8390_base + EN0_BOUNDARY);	/* 3c503 says 0x3f,NS0x26*/
drivers/net/ethernet/8390/lib8390.c:	int num_rx_pages = ei_local->stop_page-ei_local->rx_start_page;
drivers/net/ethernet/8390/lib8390.c:			ei_outb(ei_local->current_page-1, e8390_base+EN0_BOUNDARY);
drivers/net/ethernet/8390/lib8390.c:	ei_outb_p(ei_local->stop_page-1, e8390_base + EN0_BOUNDARY);	/* 3c503 says 0x3f,NS0x26*/
drivers/net/ethernet/amd/pcnet32.c:	/* pci_alloc_consistent returns page-aligned memory, so we do not have to check the alignment */
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:	write_offset = *(rx_page->write_offset_addr);
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:	if (likely(rx_page->read_offset < write_offset)) {
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:			prrs = (struct atl1e_recv_ret_status *) (rx_page->addr +
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:						 rx_page->read_offset);
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:			rx_page->read_offset +=
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:			if (rx_page->read_offset >= rx_ring->page_size) {
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:				rx_page->read_offset =
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:					*(rx_page->write_offset_addr) = 0;
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:			write_offset = *(rx_page->write_offset_addr);
drivers/net/ethernet/atheros/atl1e/atl1e_main.c:		} while (rx_page->read_offset < write_offset);
drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c:		/* clear page-end entries */
drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c:				  i, j, rx_sge[1], rx_sge[0], sw_page->page);
drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c:	 * 2. Since CDU page-size is not a single 4KB page (which is the case
drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c:	 * allocation of sub-page-size in the last entry.
drivers/net/ethernet/broadcom/sb1250-mac.c:		 * descriptor table was page-aligned and contiguous in
drivers/net/ethernet/broadcom/sb1250-mac.c:		 * descriptor table was page-aligned and contiguous in
drivers/net/ethernet/brocade/bna/bnad.c:/* Default is page-based allocation. Multi-buffer support - TBD */
drivers/net/ethernet/cavium/thunder/nicvf_queues.c:		prefetch(&page->_refcount);
drivers/net/ethernet/cortina/gemini.c:		if (gpage->mapping == mapping)
drivers/net/ethernet/cortina/gemini.c:		put_page(gpage->page);
drivers/net/ethernet/cortina/gemini.c:	if (gpage->page) {
drivers/net/ethernet/cortina/gemini.c:		put_page(gpage->page);
drivers/net/ethernet/cortina/gemini.c:	gpage->mapping = mapping;
drivers/net/ethernet/cortina/gemini.c:	gpage->page = page;
drivers/net/ethernet/cortina/gemini.c:		page = gpage->page;
drivers/net/ethernet/cortina/gemini.c:		put_page(gpage->page);
drivers/net/ethernet/cortina/gemini.c:		while (page_ref_count(gpage->page) > 0)
drivers/net/ethernet/cortina/gemini.c:			put_page(gpage->page);
drivers/net/ethernet/cortina/gemini.c:		page = gpage->page;
drivers/net/ethernet/emulex/benet/be.h:	/* set to page-addr for last frag of the page & frag-addr otherwise */
drivers/net/ethernet/freescale/dpaa/dpaa_eth.c: * half-page-aligned buffers, so we reserve some more space for start-of-buffer
drivers/net/ethernet/ibm/ehea/ehea_qmr.h:	return &current_page->entries[q_offset & (EHEA_PAGESIZE - 1)];
drivers/net/ethernet/intel/e1000e/netdev.c:		if (ps_page->page) {
drivers/net/ethernet/intel/e1000e/netdev.c:				       16, 1, page_address(ps_page->page),
drivers/net/ethernet/intel/e1000e/netdev.c:			if (!ps_page->page) {
drivers/net/ethernet/intel/e1000e/netdev.c:				ps_page->page = alloc_page(gfp);
drivers/net/ethernet/intel/e1000e/netdev.c:				if (!ps_page->page) {
drivers/net/ethernet/intel/e1000e/netdev.c:				ps_page->dma = dma_map_page(&pdev->dev,
drivers/net/ethernet/intel/e1000e/netdev.c:							    ps_page->page,
drivers/net/ethernet/intel/e1000e/netdev.c:						      ps_page->dma)) {
drivers/net/ethernet/intel/e1000e/netdev.c:			    cpu_to_le64(ps_page->dma);
drivers/net/ethernet/intel/e1000e/netdev.c:							ps_page->dma,
drivers/net/ethernet/intel/e1000e/netdev.c:				vaddr = kmap_atomic(ps_page->page);
drivers/net/ethernet/intel/e1000e/netdev.c:							   ps_page->dma,
drivers/net/ethernet/intel/e1000e/netdev.c:			dma_unmap_page(&pdev->dev, ps_page->dma, PAGE_SIZE,
drivers/net/ethernet/intel/e1000e/netdev.c:			ps_page->dma = 0;
drivers/net/ethernet/intel/e1000e/netdev.c:			skb_fill_page_desc(skb, j, ps_page->page, 0, length);
drivers/net/ethernet/intel/e1000e/netdev.c:			ps_page->page = NULL;
drivers/net/ethernet/intel/e1000e/netdev.c:			if (!ps_page->page)
drivers/net/ethernet/intel/e1000e/netdev.c:			dma_unmap_page(&pdev->dev, ps_page->dma, PAGE_SIZE,
drivers/net/ethernet/intel/e1000e/netdev.c:			ps_page->dma = 0;
drivers/net/ethernet/intel/e1000e/netdev.c:			put_page(ps_page->page);
drivers/net/ethernet/intel/e1000e/netdev.c:			ps_page->page = NULL;
drivers/net/ethernet/intel/i40e/i40e_hmc.c:		page_desc = page->pa | 0x1;
drivers/net/ethernet/mellanox/mlx4/en_rx.c:/* When the rx ring is running in page-per-packet mode, a released frame can go
drivers/net/ethernet/mellanox/mlx4/mlx4_en.h:/* Minimum ring size for our page-allocation scheme to work */
drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c:		/* When XDP enabled then page-refcnt==1 here */
drivers/net/ethernet/mellanox/mlx5/core/en_rx.c:		/* a DROP, save the page-reuse checks */
drivers/net/ethernet/qlogic/qla3xxx.h:  * page-specific registers after that.
drivers/net/ethernet/sfc/falcon/io.h:/* Calculate offset to page-mapped register */
drivers/net/ethernet/sfc/falcon/io.h:/* Write a page-mapped 32-bit CSR (EVQ_RPTR, EVQ_TMR (EF10), or the
drivers/net/ethernet/sfc/falcon/io.h:/* Write TIMER_COMMAND.  This is a page-mapped 32-bit CSR, but a bug
drivers/net/ethernet/sfc/falcon/rx.c: * ef4_init_rx_buffers - create EF4_RX_BATCH page-based RX buffers
drivers/net/ethernet/sfc/io.h:/* Calculate offset to page-mapped register */
drivers/net/ethernet/sfc/io.h:/* Write a page-mapped 32-bit CSR (EVQ_RPTR, EVQ_TMR (EF10), or the
drivers/net/ethernet/sfc/io.h:/* Write TIMER_COMMAND.  This is a page-mapped 32-bit CSR, but a bug
drivers/net/ethernet/sfc/mcdi.c:	/* consuming code assumes buffer is page-sized */
drivers/net/ethernet/sfc/mcdi.c:	char *buf = mcdi->logging_buffer; /* page-sized */
drivers/net/ethernet/sfc/mcdi.c:	char *buf = mcdi->logging_buffer; /* page-sized */
drivers/net/ethernet/sfc/mcdi_pcol.h: * statistics are dmad to that (page-aligned location). Locks required: None.
drivers/net/ethernet/sfc/mcdi_pcol.h: * DMA_ADDR != 0, then the statistics are dmad to that (page-aligned location).
drivers/net/ethernet/sfc/rx.c: * efx_init_rx_buffers - create EFX_RX_BATCH page-based RX buffers
drivers/net/ethernet/sfc/vfdi.h: * The address must be page-aligned.  After receiving such a valid
drivers/net/ethernet/sfc/vfdi.h: *	This address must be page-aligned and the PF may write up to a
drivers/net/ethernet/sfc/vfdi.h: *	must be page-aligned.
drivers/net/ethernet/sun/cassini.c: *  page-based RX descriptor engine with separate completion rings
drivers/net/ethernet/sun/cassini.c:	pci_unmap_page(cp->pdev, page->dma_addr, cp->page_size,
drivers/net/ethernet/sun/cassini.c:	__free_pages(page->buffer, cp->page_order);
drivers/net/ethernet/sun/cassini.c:	INIT_LIST_HEAD(&page->list);
drivers/net/ethernet/sun/cassini.c:	page->buffer = alloc_pages(flags, cp->page_order);
drivers/net/ethernet/sun/cassini.c:	if (!page->buffer)
drivers/net/ethernet/sun/cassini.c:	page->dma_addr = pci_map_page(cp->pdev, page->buffer, 0,
drivers/net/ethernet/sun/cassini.c:		if (page_count(page->buffer) > 1)
drivers/net/ethernet/sun/cassini.c:	if (page_count(page->buffer) == 1)
drivers/net/ethernet/sun/cassini.c:		list_add(&page->list, &cp->rx_inuse_list);
drivers/net/ethernet/sun/cassini.c:		rxd[i].buffer = cpu_to_le64(page->dma_addr);
drivers/net/ethernet/sun/cassini.c:		pci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:		addr = cas_page_map(page->buffer);
drivers/net/ethernet/sun/cassini.c:		pci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:		pci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:			addr = cas_page_map(page->buffer);
drivers/net/ethernet/sun/cassini.c:			pci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:		__skb_frag_set_page(frag, page->buffer);
drivers/net/ethernet/sun/cassini.c:			pci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr,
drivers/net/ethernet/sun/cassini.c:			pci_dma_sync_single_for_device(cp->pdev, page->dma_addr,
drivers/net/ethernet/sun/cassini.c:			__skb_frag_set_page(frag, page->buffer);
drivers/net/ethernet/sun/cassini.c:			addr = cas_page_map(page->buffer);
drivers/net/ethernet/sun/cassini.c:		pci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:		addr = cas_page_map(page->buffer);
drivers/net/ethernet/sun/cassini.c:		pci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,
drivers/net/ethernet/sun/cassini.c:			pci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr,
drivers/net/ethernet/sun/cassini.c:			addr = cas_page_map(page->buffer);
drivers/net/ethernet/sun/cassini.c:			pci_dma_sync_single_for_device(cp->pdev, page->dma_addr,
drivers/net/ethernet/sun/cassini.h:/* descriptor ring for free buffers contains page-sized buffers. the index
drivers/net/ethernet/sun/niu.c:	page->index = base;
drivers/net/ethernet/sun/niu.c:	page->mapping = (struct address_space *) rp->rxhash[h];
drivers/net/ethernet/sun/niu.c:		if ((page->index + PAGE_SIZE) - rcr_size == addr) {
drivers/net/ethernet/sun/niu.c:			*link = (struct page *) page->mapping;
drivers/net/ethernet/sun/niu.c:			np->ops->unmap_page(np->device, page->index,
drivers/net/ethernet/sun/niu.c:			page->index = 0;
drivers/net/ethernet/sun/niu.c:			page->mapping = NULL;
drivers/net/ethernet/sun/niu.c:		if ((page->index + rp->rbr_block_size) - rcr_size == addr) {
drivers/net/ethernet/sun/niu.c:			*link = (struct page *) page->mapping;
drivers/net/ethernet/sun/niu.c:			np->ops->unmap_page(np->device, page->index,
drivers/net/ethernet/sun/niu.c:			page->index = 0;
drivers/net/ethernet/sun/niu.c:			page->mapping = NULL;
drivers/net/ethernet/sun/niu.c:			struct page *next = (struct page *) page->mapping;
drivers/net/ethernet/sun/niu.c:			u64 base = page->index;
drivers/net/ethernet/sun/niu.c:			page->index = 0;
drivers/net/ethernet/sun/niu.c:			page->mapping = NULL;
drivers/net/ethernet/sun/niu.c:						(struct page *) page->mapping;
drivers/net/ethernet/sun/niu.c:					u64 base = page->index;
drivers/net/tun.c:	if (tpage->page)
drivers/net/tun.c:		__page_frag_cache_drain(tpage->page, tpage->count);
drivers/net/tun.c:			if (tpage->page == page) {
drivers/net/tun.c:				++tpage->count;
drivers/net/tun.c:				tpage->page = page;
drivers/net/tun.c:				tpage->count = 1;
drivers/net/virtio_net.c:		page = (struct page *)page->private;
drivers/net/wireless/intel/iwlegacy/3945.c:		il->alloc_rxb_page--;
drivers/net/wireless/intel/iwlegacy/4965-mac.c:		il->alloc_rxb_page--;
drivers/net/wireless/intel/iwlegacy/common.h:	il->alloc_rxb_page--;
drivers/net/wireless/intel/iwlegacy/common.h:	il->alloc_rxb_page--;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:	get_page(hdr_page->page);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:	start_hdr = hdr_page->pos;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:	*page_ptr = hdr_page->page;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:	memcpy(hdr_page->pos, skb->data + hdr_len, iv_len);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:	hdr_page->pos += iv_len;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		u8 *subf_hdrs_start = hdr_page->pos;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		memset(hdr_page->pos, 0, amsdu_pad);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		hdr_page->pos += amsdu_pad;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		ether_addr_copy(hdr_page->pos, ieee80211_get_DA(hdr));
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		hdr_page->pos += ETH_ALEN;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		ether_addr_copy(hdr_page->pos, ieee80211_get_SA(hdr));
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		hdr_page->pos += ETH_ALEN;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		*((__be16 *)hdr_page->pos) = cpu_to_be16(length);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		hdr_page->pos += sizeof(length);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		tso_build_hdr(skb, hdr_page->pos, &tso, data_left, !total_len);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		hdr_page->pos += snap_ip_tcp_hdrlen;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		tb_len = hdr_page->pos - start_hdr;
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		le16_add_cpu(&tx_cmd->len, hdr_page->pos - subf_hdrs_start);
drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c:		start_hdr = hdr_page->pos;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:	get_page(hdr_page->page);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:	start_hdr = hdr_page->pos;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:	*page_ptr = hdr_page->page;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:	memcpy(hdr_page->pos, skb->data + hdr_len, iv_len);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:	hdr_page->pos += iv_len;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		u8 *iph, *subf_hdrs_start = hdr_page->pos;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		memset(hdr_page->pos, 0, amsdu_pad);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_page->pos += amsdu_pad;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		ether_addr_copy(hdr_page->pos, ieee80211_get_DA(hdr));
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_page->pos += ETH_ALEN;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		ether_addr_copy(hdr_page->pos, ieee80211_get_SA(hdr));
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_page->pos += ETH_ALEN;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		*((__be16 *)hdr_page->pos) = cpu_to_be16(length);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_page->pos += sizeof(length);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		tso_build_hdr(skb, hdr_page->pos, &tso, data_left, !total_len);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		iph = hdr_page->pos + 8;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_page->pos += snap_ip_tcp_hdrlen;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		hdr_tb_len = hdr_page->pos - start_hdr;
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		le16_add_cpu(&tx_cmd->len, hdr_page->pos - subf_hdrs_start);
drivers/net/wireless/intel/iwlwifi/pcie/tx.c:		start_hdr = hdr_page->pos;
drivers/nvdimm/pmem.c:	wake_up_var(&page->_refcount);
drivers/nvdimm/pmem.h:#include <linux/page-flags.h>
drivers/nvmem/rave-sp-eeprom.c: * @page:	Data to write or buffer to store result (via page->data)
drivers/nvmem/rave-sp-eeprom.c:	const unsigned int data_size = is_write ? sizeof(page->data) : 0;
drivers/nvmem/rave-sp-eeprom.c:		is_write ? sizeof(*page) - sizeof(page->data) : sizeof(*page);
drivers/nvmem/rave-sp-eeprom.c:	u8 cmd[RAVE_SP_EEPROM_HEADER_MAX + sizeof(page->data)];
drivers/nvmem/rave-sp-eeprom.c:	memcpy(&cmd[offset], page->data, data_size);
drivers/nvmem/rave-sp-eeprom.c:	if (page->type != type)
drivers/nvmem/rave-sp-eeprom.c:	if (!page->success)
drivers/pci/pci.c: * Later on, the kernel will assign page-aligned memory resource back
drivers/s390/block/dcssblk.c:		/* Request is not page-aligned. */
drivers/s390/block/xpram.c:		/* Request is not page-aligned. */
drivers/s390/block/xpram.c:		/* Request size is no page-aligned. */
drivers/s390/char/vmur.c: * is used to read spool data page-wise.
drivers/s390/scsi/zfcp_fc.c:		if ((port->d_id & range) == (ntoh24(page->rscn_fid) & range))
drivers/s390/scsi/zfcp_fc.c:		afmt = page->rscn_page_flags & ELS_RSCN_ADDR_FMT_MASK;
drivers/scsi/hisi_sas/hisi_sas_v1_hw.c:		struct hisi_sas_sge *entry = &sge_page->sge[i];
drivers/scsi/hisi_sas/hisi_sas_v2_hw.c:		struct hisi_sas_sge *entry = &sge_page->sge[i];
drivers/scsi/hisi_sas/hisi_sas_v3_hw.c:		struct hisi_sas_sge *entry = &sge_page->sge[i];
drivers/scsi/hpsa.c:	 * least 4-byte aligned (most likely, it's page-aligned).
drivers/scsi/ipr.c:	entry_length = mode_page->entry_length;
drivers/scsi/ipr.c:	bus = mode_page->bus;
drivers/scsi/ipr.c:	for (i = 0; i < mode_page->num_entries; i++) {
drivers/scsi/ipr.c:	entry_length = mode_page->entry_length;
drivers/scsi/ipr.c:	for (i = 0, bus = mode_page->bus;
drivers/scsi/ipr.c:	     i < mode_page->num_entries;
drivers/scsi/ipr.c:		mode_page->flags |= IPR_ENABLE_DUAL_IOA_AF;
drivers/scsi/lpfc/lpfc_hw4.h:/* word0 of page-1 struct shares the same SHIFT/MASK/WORD defines as above */
drivers/scsi/lpfc/lpfc_sli.c:			rpi_page->start_rpi = phba->sli4_hba.rpi_ids[lrpi];
drivers/scsi/lpfc/lpfc_sli.c:	       rpi_page->start_rpi);
drivers/scsi/lpfc/lpfc_sli.c:	       hdr_tmpl, rpi_page->page_count);
drivers/scsi/lpfc/lpfc_sli.c:	hdr_tmpl->rpi_paddr_lo = putPaddrLow(rpi_page->dmabuf->phys);
drivers/scsi/lpfc/lpfc_sli.c:	hdr_tmpl->rpi_paddr_hi = putPaddrHigh(rpi_page->dmabuf->phys);
drivers/scsi/lpfc/lpfc_sli.c:		phba->sli4_hba.next_rpi = rpi_page->next_rpi;
drivers/scsi/mpt3sas/mpt3sas_config.c:		for (i = 0; i < config_page->NumElements; i++) {
drivers/scsi/mpt3sas/mpt3sas_config.c:			element_type = le16_to_cpu(config_page->
drivers/scsi/mpt3sas/mpt3sas_config.c:				    le16_to_cpu(config_page->ConfigElement[i].
drivers/scsi/mpt3sas/mpt3sas_config.c:					    le16_to_cpu(config_page->
drivers/scsi/mpt3sas/mpt3sas_config.c:		config_num = config_page->ConfigNum;
drivers/scsi/qedi/qedi_fw.c:		 * check if end addr is page-aligned.
drivers/scsi/qedi/qedi_fw.c:		 * check if start addr is page-aligned.
drivers/scsi/qedi/qedi_fw.c:		 * check if start and end addr is page-aligned
drivers/scsi/scsi_sysfs.c:				vpd_page->data, vpd_page->len);		\
drivers/scsi/smartpqi/smartpqi_init.c:	/* +1 to cover when the buffer is not page-aligned. */
drivers/staging/android/ion/ion_page_pool.c:		list_add_tail(&page->lru, &pool->high_items);
drivers/staging/android/ion/ion_page_pool.c:		list_add_tail(&page->lru, &pool->low_items);
drivers/staging/android/ion/ion_page_pool.c:	list_del(&page->lru);
drivers/staging/android/ion/ion_system_heap.c:		list_add_tail(&page->lru, &pages);
drivers/staging/android/ion/ion_system_heap.c:		list_del(&page->lru);
drivers/staging/android/uapi/ashmem.h:	__u32 offset;	/* offset into region, in bytes, page-aligned */
drivers/staging/android/uapi/ashmem.h:	__u32 len;	/* length forward from offset, in bytes, page-aligned */
drivers/staging/erofs/Kconfig:	  read-only file system with modern designs (eg. page-sized
drivers/staging/erofs/TODO:   (currently erofs only works as expected with the page-sized
drivers/staging/erofs/data.c:		if (unlikely(page->mapping != mapping)) {
drivers/staging/erofs/data.c:	erofs_off_t current_block = (erofs_off_t)page->index;
drivers/staging/erofs/data.c:	bio = erofs_read_raw_page(NULL, page->mapping,
drivers/staging/erofs/data.c:		prefetchw(&page->flags);
drivers/staging/erofs/data.c:		list_del(&page->lru);
drivers/staging/erofs/data.c:		if (!add_to_page_cache_lru(page, mapping, page->index, gfp)) {
drivers/staging/erofs/data.c:				       __func__, page->index,
drivers/staging/erofs/erofs_fs.h:		 * eg. for 4k page-sized cluster, maximum 4K*64k = 256M)
drivers/staging/erofs/include/trace/events/erofs.h:		__entry->dev	= page->mapping->host->i_sb->s_dev;
drivers/staging/erofs/include/trace/events/erofs.h:		__entry->nid	= EROFS_V(page->mapping->host)->nid;
drivers/staging/erofs/include/trace/events/erofs.h:		__entry->dir	= S_ISDIR(page->mapping->host->i_mode);
drivers/staging/erofs/include/trace/events/erofs.h:		__entry->index	= page->index;
drivers/staging/erofs/include/trace/events/erofs.h:		__entry->start	= page->index;
drivers/staging/erofs/super.c:	struct address_space *const mapping = page->mapping;
drivers/staging/erofs/unzip_vle.c:		if (!page || page->mapping != mapping)
drivers/staging/erofs/unzip_vle.c:	page->mapping = Z_EROFS_MAPPING_STAGING;
drivers/staging/erofs/unzip_vle.c:	index = page->index - map->m_la / PAGE_SIZE;
drivers/staging/erofs/unzip_vle.c:		DBG_BUGON(!page->mapping);
drivers/staging/erofs/unzip_vle.c:			struct inode *const inode = page->mapping->host;
drivers/staging/erofs/unzip_vle.c:		 * however, page->mapping never be NULL if working properly.
drivers/staging/erofs/unzip_vle.c:		cachemngd = (page->mapping == mc);
drivers/staging/erofs/unzip_vle.c:		DBG_BUGON(!page->mapping);
drivers/staging/erofs/unzip_vle.c:		DBG_BUGON(!page->mapping);
drivers/staging/erofs/unzip_vle.c:			if (page->mapping == MNGD_MAPPING(sbi)) {
drivers/staging/erofs/unzip_vle.c:		if (page->mapping == MNGD_MAPPING(sbi))
drivers/staging/erofs/unzip_vle.c:		DBG_BUGON(!page->mapping);
drivers/staging/erofs/unzip_vle.c:	mapping = READ_ONCE(page->mapping);
drivers/staging/erofs/unzip_vle.c:	if (page->mapping == mc) {
drivers/staging/erofs/unzip_vle.c:	DBG_BUGON(page->mapping);
drivers/staging/erofs/unzip_vle.c:		list_add(&page->lru, pagepool);
drivers/staging/erofs/unzip_vle.c:		page->mapping = Z_EROFS_MAPPING_STAGING;
drivers/staging/erofs/unzip_vle.c:	struct inode *const inode = page->mapping->host;
drivers/staging/erofs/unzip_vle.c:	f.headoffset = (erofs_off_t)page->index << PAGE_SHIFT;
drivers/staging/erofs/unzip_vle.c:		prefetchw(&page->flags);
drivers/staging/erofs/unzip_vle.c:		list_del(&page->lru);
drivers/staging/erofs/unzip_vle.c:		if (add_to_page_cache_lru(page, mapping, page->index, gfp)) {
drivers/staging/erofs/unzip_vle.c:			list_add(&page->lru, &pagepool);
drivers/staging/erofs/unzip_vle.c:				__func__, page->index, vi->nid);
drivers/staging/erofs/unzip_vle.c:	if (mpage->index != mblk) {
drivers/staging/erofs/unzip_vle.c:	if (!mpage || mpage->index != mblk) {
drivers/staging/erofs/unzip_vle.h:		list_add(&page->lru, page_pool);
drivers/staging/erofs/utils.c:		list_del(&page->lru);
drivers/staging/fbtft/fbtft-core.c:		index = page->index << PAGE_SHIFT;
drivers/staging/fbtft/fbtft-core.c:			"page->index=%lu y_low=%d y_high=%d\n",
drivers/staging/fbtft/fbtft-core.c:			page->index, y_low, y_high);
drivers/staging/gasket/gasket_core.c:			"Base address not page-aligned: 0x%lx\n",
drivers/staging/gasket/gasket_page_table.c:	 * not be page-aligned. Offset is the index into the containing page of
drivers/staging/gasket/gasket_page_table.c:	 * and page-aligned addresses.
drivers/staging/rtl8192u/r819xU_phyreg.h: * page-8
drivers/staging/rtl8192u/r819xU_phyreg.h:/* page-a */
drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c:		 * The firmware expects blocks after the first to be page-
drivers/tee/optee/call.c: * @dst: page-aligned buffer where list of pages will be stored
drivers/tee/optee/shm_pool.c: * optee_shm_pool_alloc_pages() - create page-based allocator pool
drivers/usb/gadget/udc/net2280.c:		0 /* or page-crossing issues */);
drivers/usb/host/ohci-mem.c:		0 /* no page-crossing issues */);
drivers/usb/host/ohci-mem.c:		0 /* no page-crossing issues */);
drivers/usb/mon/mon_bin.c: * page-sized chunks for the time being.
drivers/usb/storage/sddr55.c:		// convert page to block and page-within-block
drivers/video/fbdev/core/fb_defio.c:		page->mapping = vmf->vma->vm_file->f_mapping;
drivers/video/fbdev/core/fb_defio.c:	BUG_ON(!page->mapping);
drivers/video/fbdev/core/fb_defio.c:	page->index = vmf->pgoff;
drivers/video/fbdev/core/fb_defio.c:		else if (cur->index > page->index)
drivers/video/fbdev/core/fb_defio.c:	list_add_tail(&page->lru, &cur->lru);
drivers/video/fbdev/core/fb_defio.c:		page->mapping = NULL;
drivers/video/fbdev/fsl-diu-fb.c:	 * always page-aligned.  We need the memory to be 32-byte aligned,
drivers/video/fbdev/ssd1307fb.c:	if (of_property_read_u32(node, "solomon,page-offset", &par->page_offset))
drivers/video/fbdev/xen-fbfront.c:	prod = info->page->out_prod;
drivers/video/fbdev/xen-fbfront.c:	info->page->out_prod = prod + 1;
drivers/video/fbdev/xen-fbfront.c:	prod = info->page->out_prod;
drivers/video/fbdev/xen-fbfront.c:	cons = info->page->out_cons;
drivers/video/fbdev/xen-fbfront.c:		beg = page->index << PAGE_SHIFT;
drivers/video/fbdev/xen-fbfront.c:	xenfb_refresh(info, 0, 0, info->page->width, info->page->height);
drivers/video/fbdev/xen-fbfront.c:		    var->bits_per_pixel == xenfb_info->page->depth) {
drivers/video/fbdev/xen-fbfront.c:	required_mem_len = var->xres * var->yres * xenfb_info->page->depth / 8;
drivers/video/fbdev/xen-fbfront.c:	if (var->bits_per_pixel == xenfb_info->page->depth &&
drivers/video/fbdev/xen-fbfront.c:	if (page->in_cons != page->in_prod) {
drivers/video/fbdev/xen-fbfront.c:		info->page->in_cons = info->page->in_prod;
drivers/video/fbdev/xen-fbfront.c:		info->page->pd[i] = vmalloc_to_gfn(&info->gfns[i * epd]);
drivers/video/fbdev/xen-fbfront.c:	info->page->width = fb_info->var.xres;
drivers/video/fbdev/xen-fbfront.c:	info->page->height = fb_info->var.yres;
drivers/video/fbdev/xen-fbfront.c:	info->page->depth = fb_info->var.bits_per_pixel;
drivers/video/fbdev/xen-fbfront.c:	info->page->line_length = fb_info->fix.line_length;
drivers/video/fbdev/xen-fbfront.c:	info->page->mem_length = fb_info->fix.smem_len;
drivers/video/fbdev/xen-fbfront.c:	info->page->in_cons = info->page->in_prod = 0;
drivers/video/fbdev/xen-fbfront.c:	info->page->out_cons = info->page->out_prod = 0;
drivers/video/fbdev/xen-fbfront.c:	ret = xenbus_printf(xbt, dev->nodename, "page-ref", "%lu",
drivers/virt/fsl_hypervisor.c:	 * page-aligned memory.  Since the user buffer is probably not
drivers/virt/fsl_hypervisor.c:	 * page-aligned, we need to handle the discrepancy.
drivers/virt/vboxguest/vboxguest_core.c:		vbg_err("vboxguest: Error host too old (does not support page-lists)\n");
drivers/virtio/virtio_balloon.c:		list_del(&page->lru);
drivers/virtio/virtio_balloon.c:		list_add(&page->lru, &pages);
drivers/xen/balloon.c:		list_add_tail(&page->lru, &ballooned_pages);
drivers/xen/balloon.c:		list_add(&page->lru, &ballooned_pages);
drivers/xen/balloon.c:	list_del(&page->lru);
drivers/xen/balloon.c:	struct list_head *next = page->lru.next;
drivers/xen/balloon.c:		list_add(&page->lru, &pages);
drivers/xen/balloon.c:		list_del(&page->lru);
drivers/xen/privcmd.c:			list_add_tail(&page->lru, pagelist);
drivers/xen/pvcalls-back.c:	err = xenbus_printf(xbt, dev->nodename, "max-page-order", "%u",
drivers/xen/pvcalls-back.c:		pr_warn("%s write out 'max-page-order' failed\n", __func__);
drivers/xen/pvcalls-front.c:					      "max-page-order", 0);
drivers/xen/pvcalls-front.c:	pr_info("%s max-page-order is %u\n", __func__, max_page_order);
drivers/xen/swiotlb-xen.c:#include <asm/xen/page-coherent.h>
fs/9p/cache.c:	struct inode *inode = page->mapping->host;
fs/9p/cache.c:	struct inode *inode = page->mapping->host;
fs/9p/vfs_addr.c:	struct inode *inode = page->mapping->host;
fs/9p/vfs_addr.c:	struct inode *inode = page->mapping->host;
fs/9p/vfs_addr.c:	if (page->index == size >> PAGE_SHIFT)
fs/9p/vfs_addr.c:			mapping_set_error(page->mapping, retval);
fs/9p/vfs_addr.c:	struct inode *inode = page->mapping->host;
fs/9p/vfs_addr.c:	struct inode *inode = page->mapping->host;
fs/9p/vfs_file.c:	if (page->mapping != inode->i_mapping)
fs/affs/file.c:	struct inode *inode = page->mapping->host;
fs/affs/file.c:		 page->index, to);
fs/affs/file.c:	tmp = page->index << PAGE_SHIFT;
fs/affs/file.c:	struct inode *inode = page->mapping->host;
fs/affs/file.c:	pr_debug("%s(%lu, %ld)\n", __func__, inode->i_ino, page->index);
fs/affs/file.c:	if (((page->index + 1) << PAGE_SHIFT) > inode->i_size) {
fs/affs/file.c:	tmp = (page->index << PAGE_SHIFT) + from;
fs/affs/file.c:	tmp = (page->index << PAGE_SHIFT) + from;
fs/affs/symlink.c:	struct inode *inode = page->mapping->host;
fs/afs/dir.c:	struct afs_vnode *dvnode = AFS_FS_I(page->mapping->host);
fs/afs/dir.c:	_enter("{{%llx:%llu}[%lu]}", dvnode->fid.vid, dvnode->fid.vnode, page->index);
fs/afs/dir.c:	struct afs_vnode *dvnode = AFS_FS_I(page->mapping->host);
fs/afs/dir.c:	_enter("{%lu},%u,%u", page->index, offset, length);
fs/afs/dir_edit.c:	meta = &meta_page->blocks[0];
fs/afs/dir_edit.c:		block = &dir_page->blocks[b % AFS_DIR_BLOCKS_PER_PAGE];
fs/afs/dir_edit.c:	meta = &meta_page->blocks[0];
fs/afs/dir_edit.c:		block = &dir_page->blocks[b % AFS_DIR_BLOCKS_PER_PAGE];
fs/afs/file.c:	struct inode *inode = page->mapping->host;
fs/afs/file.c:	_enter("{%x},{%lu},{%lu}", key_serial(key), inode->i_ino, page->index);
fs/afs/file.c:		req->pos = (loff_t)page->index << PAGE_SHIFT;
fs/afs/file.c:		struct inode *inode = page->mapping->host;
fs/afs/file.c:		if (page->index != index)
fs/afs/file.c:		list_del(&page->lru);
fs/afs/file.c:		index = page->index;
fs/afs/file.c:	struct afs_vnode *vnode = AFS_FS_I(page->mapping->host);
fs/afs/file.c:	_enter("{%lu},%u,%u", page->index, offset, length);
fs/afs/file.c:			struct afs_vnode *vnode = AFS_FS_I(page->mapping->host);
fs/afs/file.c:					     page->index, priv);
fs/afs/file.c:	struct afs_vnode *vnode = AFS_FS_I(page->mapping->host);
fs/afs/file.c:	       vnode->fid.vid, vnode->fid.vnode, page->index, page->flags,
fs/afs/file.c:				     page->index, priv);
fs/afs/write.c:	 * page->private.
fs/afs/write.c:	BUILD_BUG_ON(PAGE_SIZE > 32768 && sizeof(page->private) < 8);
fs/afs/write.c:					     page->index, priv);
fs/afs/write.c:			     page->index, priv);
fs/afs/write.c:	       vnode->fid.vid, vnode->fid.vnode, page->index);
fs/afs/write.c:			if (page->index >= first)
fs/afs/write.c:				first = page->index + 1;
fs/afs/write.c:			if (page->index >= first)
fs/afs/write.c:				first = page->index + 1;
fs/afs/write.c:	_enter(",%lx", primary_page->index);
fs/afs/write.c:	start = primary_page->index;
fs/afs/write.c:			     primary_page->index, priv);
fs/afs/write.c:				     primary_page->index, priv);
fs/afs/write.c:			if (page->index > final_page)
fs/afs/write.c:					     page->index, priv);
fs/afs/write.c:	first = primary_page->index;
fs/afs/write.c:	_enter("{%lx},", page->index);
fs/afs/write.c:	ret = afs_write_back_from_locked_page(page->mapping, wbc, page,
fs/afs/write.c:		_debug("wback %lx", page->index);
fs/afs/write.c:		 * (changing page->mapping to NULL), or even swizzled
fs/afs/write.c:		if (page->mapping != mapping || !PageDirty(page)) {
fs/afs/write.c:	       vnode->fid.vid, vnode->fid.vnode, vmf->page->index);
fs/afs/write.c:	/* We mustn't change page->private until writeback is complete as that
fs/afs/write.c:			     vmf->page->index, priv);
fs/afs/write.c:	struct address_space *mapping = page->mapping;
fs/afs/write.c:	_enter("{%lx}", page->index);
fs/afs/write.c:				     page->index, priv);
fs/afs/write.c:		ret = afs_store_data(mapping, page->index, page->index, t, f);
fs/afs/write.c:			     page->index, priv);
fs/aio.c:		pr_debug("pid(%d) [%d] page->count=%d\n", current->pid, i,
fs/befs/linuxvfs.c:	struct inode *inode = page->mapping->host;
fs/block_dev.c:	struct super_block *super = BDEV_I(page->mapping->host)->bdev.bd_super;
fs/btrfs/compression.c:		page->mapping = NULL;
fs/btrfs/compression.c:		page->mapping = NULL;
fs/btrfs/compression.c:		page->mapping = inode->i_mapping;
fs/btrfs/compression.c:		page->mapping = NULL;
fs/btrfs/compression.c:		if (page->index == end_index) {
fs/btrfs/compression.c:		page->mapping = inode->i_mapping;
fs/btrfs/compression.c:		page->index = em_start >> PAGE_SHIFT;
fs/btrfs/compression.c:		page->mapping = NULL;
fs/btrfs/disk-io.c:	eb = (struct extent_buffer *)page->private;
fs/btrfs/disk-io.c:	struct btrfs_root *root = BTRFS_I(page->mapping->host)->root;
fs/btrfs/disk-io.c:	if (!page->private)
fs/btrfs/disk-io.c:	eb = (struct extent_buffer *)page->private;
fs/btrfs/disk-io.c:		root = BTRFS_I(bvec->bv_page->mapping->host)->root;
fs/btrfs/disk-io.c:	tree = &BTRFS_I(page->mapping->host)->io_tree;
fs/btrfs/disk-io.c:	tree = &BTRFS_I(page->mapping->host)->io_tree;
fs/btrfs/disk-io.c:		btrfs_warn(BTRFS_I(page->mapping->host)->root->fs_info,
fs/btrfs/disk-io.c:	eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:#include <linux/page-flags.h>
fs/btrfs/extent_io.c:	if (index == locked_page->index && end_index == index)
fs/btrfs/extent_io.c:	if (index == locked_page->index && index == end_index)
fs/btrfs/extent_io.c:	struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:		mapping_set_error(page->mapping, ret);
fs/btrfs/extent_io.c:		struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:		struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:			eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:			if (page->index == end_index && off)
fs/btrfs/extent_io.c:	bio->bi_write_hint = page->mapping->host->i_write_hint;
fs/btrfs/extent_io.c:		WARN_ON(page->private != (unsigned long)eb);
fs/btrfs/extent_io.c:	struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:	if (page->index == last_byte >> PAGE_SHIFT) {
fs/btrfs/extent_io.c:	struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:			       page->index, cur, end);
fs/btrfs/extent_io.c:	struct inode *inode = page->mapping->host;
fs/btrfs/extent_io.c:	if (page->index > end_index ||
fs/btrfs/extent_io.c:	   (page->index == end_index && !pg_offset)) {
fs/btrfs/extent_io.c:		page->mapping->a_ops->invalidatepage(page, 0, PAGE_SIZE);
fs/btrfs/extent_io.c:	if (page->index == end_index) {
fs/btrfs/extent_io.c:	struct extent_buffer *eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:		eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:			eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:			done_index = page->index;
fs/btrfs/extent_io.c:			 * invalidated (changing page->mapping to NULL),
fs/btrfs/extent_io.c:			if (unlikely(page->mapping != mapping)) {
fs/btrfs/extent_io.c:				done_index = page->index + 1;
fs/btrfs/extent_io.c:		.tree = &BTRFS_I(page->mapping->host)->io_tree,
fs/btrfs/extent_io.c:			prefetchw(&page->flags);
fs/btrfs/extent_io.c:			list_del(&page->lru);
fs/btrfs/extent_io.c:			if (add_to_page_cache_lru(page, mapping, page->index,
fs/btrfs/extent_io.c:	size_t blocksize = page->mapping->host->i_sb->s_blocksize;
fs/btrfs/extent_io.c:	struct btrfs_inode *btrfs_inode = BTRFS_I(page->mapping->host);
fs/btrfs/extent_io.c:	    page->mapping->host->i_size > SZ_16M) {
fs/btrfs/extent_io.c:			spin_lock(&page->mapping->private_lock);
fs/btrfs/extent_io.c:		    page->private == (unsigned long)eb) {
fs/btrfs/extent_io.c:			spin_unlock(&page->mapping->private_lock);
fs/btrfs/extent_io.c:			 * overwrite page->private.
fs/btrfs/extent_io.c:		xa_lock_irq(&page->mapping->i_pages);
fs/btrfs/extent_io.c:			__xa_clear_mark(&page->mapping->i_pages,
fs/btrfs/extent_io.c:		xa_unlock_irq(&page->mapping->i_pages);
fs/btrfs/extent_io.c:	spin_lock(&page->mapping->private_lock);
fs/btrfs/extent_io.c:		spin_unlock(&page->mapping->private_lock);
fs/btrfs/extent_io.c:	eb = (struct extent_buffer *)page->private;
fs/btrfs/extent_io.c:		spin_unlock(&page->mapping->private_lock);
fs/btrfs/extent_io.c:	spin_unlock(&page->mapping->private_lock);
fs/btrfs/extent_io.h: * page->private values.  Every page that is controlled by the extent
fs/btrfs/extent_io.h: * map has page->private set to one.
fs/btrfs/file.c:		if (page->mapping != inode->i_mapping) {
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:	if (!page->mapping || !PageDirty(page) || !PageChecked(page)) {
fs/btrfs/inode.c:	inode = page->mapping->host;
fs/btrfs/inode.c:		mapping_set_error(page->mapping, ret);
fs/btrfs/inode.c:		mapping_set_error(page->mapping, ret);
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:		if (page->mapping != mapping) {
fs/btrfs/inode.c:	tree = &BTRFS_I(page->mapping->host)->io_tree;
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:	struct inode *inode = page->mapping->host;
fs/btrfs/inode.c:	if ((page->mapping != inode->i_mapping) ||
fs/btrfs/inode.c:	if (page->index == ((size - 1) >> PAGE_SHIFT)) {
fs/btrfs/ioctl.c:			if (page->mapping != inode->i_mapping) {
fs/btrfs/ioctl.c:		if (page->mapping != inode->i_mapping) {
fs/btrfs/locking.c:#include <linux/page-flags.h>
fs/btrfs/scrub.c:	return page->recover &&
fs/btrfs/scrub.c:	       (page->recover->bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK);
fs/btrfs/scrub.c:			 * handles this case (page->io_error), by
fs/btrfs/scrub.c:			page->sblock = sblock;
fs/btrfs/scrub.c:			page->flags = flags;
fs/btrfs/scrub.c:			page->generation = generation;
fs/btrfs/scrub.c:			page->logical = logical;
fs/btrfs/scrub.c:			page->have_csum = have_csum;
fs/btrfs/scrub.c:				memcpy(page->csum,
fs/btrfs/scrub.c:			page->physical = bbio->stripes[stripe_index].physical +
fs/btrfs/scrub.c:			page->dev = bbio->stripes[stripe_index].dev;
fs/btrfs/scrub.c:			page->physical_for_dev_replace =
fs/btrfs/scrub.c:			page->mirror_num = mirror_index + 1;
fs/btrfs/scrub.c:			page->page = alloc_page(GFP_NOFS);
fs/btrfs/scrub.c:			if (!page->page)
fs/btrfs/scrub.c:			page->recover = recover;
fs/btrfs/scrub.c:	bio->bi_iter.bi_sector = page->logical >> 9;
fs/btrfs/scrub.c:	mirror_num = page->sblock->pagev[0]->mirror_num;
fs/btrfs/scrub.c:	ret = raid56_parity_recover(fs_info, bio, page->recover->bbio,
fs/btrfs/scrub.c:				    page->recover->map_length,
fs/btrfs/scrub.c:	ASSERT(first_page->dev);
fs/btrfs/scrub.c:	if (!first_page->dev->bdev)
fs/btrfs/scrub.c:	bio_set_dev(bio, first_page->dev->bdev);
fs/btrfs/scrub.c:		WARN_ON(!page->page);
fs/btrfs/scrub.c:		bio_add_page(bio, page->page, PAGE_SIZE, 0);
fs/btrfs/scrub.c:		if (page->dev->bdev == NULL) {
fs/btrfs/scrub.c:			page->io_error = 1;
fs/btrfs/scrub.c:		WARN_ON(!page->page);
fs/btrfs/scrub.c:		bio_set_dev(bio, page->dev->bdev);
fs/btrfs/scrub.c:		bio_add_page(bio, page->page, PAGE_SIZE, 0);
fs/btrfs/scrub.c:		bio->bi_iter.bi_sector = page->physical >> 9;
fs/btrfs/scrub.c:			page->io_error = 1;
fs/btrfs/scrub.c:	struct btrfs_fs_devices *fs_devices = spage->dev->fs_devices;
fs/btrfs/scrub.c:	BUG_ON(spage->page == NULL);
fs/btrfs/scrub.c:	if (spage->io_error) {
fs/btrfs/scrub.c:		void *mapped_buffer = kmap_atomic(spage->page);
fs/btrfs/scrub.c:		flush_dcache_page(spage->page);
fs/btrfs/scrub.c:		sbio->physical = spage->physical_for_dev_replace;
fs/btrfs/scrub.c:		sbio->logical = spage->logical;
fs/btrfs/scrub.c:		   spage->physical_for_dev_replace ||
fs/btrfs/scrub.c:		   spage->logical) {
fs/btrfs/scrub.c:	ret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);
fs/btrfs/scrub.c:			spage->io_error = 1;
fs/btrfs/scrub.c:	atomic_inc(&spage->refs);
fs/btrfs/scrub.c:	if (atomic_dec_and_test(&spage->refs)) {
fs/btrfs/scrub.c:		if (spage->page)
fs/btrfs/scrub.c:			__free_page(spage->page);
fs/btrfs/scrub.c:	struct scrub_block *sblock = spage->sblock;
fs/btrfs/scrub.c:		sbio->physical = spage->physical;
fs/btrfs/scrub.c:		sbio->logical = spage->logical;
fs/btrfs/scrub.c:		sbio->dev = spage->dev;
fs/btrfs/scrub.c:		   spage->physical ||
fs/btrfs/scrub.c:		   spage->logical ||
fs/btrfs/scrub.c:		   sbio->dev != spage->dev) {
fs/btrfs/scrub.c:	ret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);
fs/btrfs/scrub.c:		raid56_add_scrub_pages(rbio, spage->page, spage->logical);
fs/btrfs/scrub.c:		spage->sblock = sblock;
fs/btrfs/scrub.c:		spage->dev = dev;
fs/btrfs/scrub.c:		spage->flags = flags;
fs/btrfs/scrub.c:		spage->generation = gen;
fs/btrfs/scrub.c:		spage->logical = logical;
fs/btrfs/scrub.c:		spage->physical = physical;
fs/btrfs/scrub.c:		spage->physical_for_dev_replace = physical_for_dev_replace;
fs/btrfs/scrub.c:		spage->mirror_num = mirror_num;
fs/btrfs/scrub.c:			spage->have_csum = 1;
fs/btrfs/scrub.c:			memcpy(spage->csum, csum, sctx->csum_size);
fs/btrfs/scrub.c:			spage->have_csum = 0;
fs/btrfs/scrub.c:		spage->page = alloc_page(GFP_KERNEL);
fs/btrfs/scrub.c:		if (!spage->page)
fs/btrfs/scrub.c:			spage->io_error = 1;
fs/btrfs/scrub.c:			spage->sblock->no_io_error_seen = 0;
fs/btrfs/scrub.c:		struct scrub_block *sblock = spage->sblock;
fs/btrfs/scrub.c:		list_add_tail(&spage->list, &sparity->spages);
fs/btrfs/scrub.c:		spage->sblock = sblock;
fs/btrfs/scrub.c:		spage->dev = dev;
fs/btrfs/scrub.c:		spage->flags = flags;
fs/btrfs/scrub.c:		spage->generation = gen;
fs/btrfs/scrub.c:		spage->logical = logical;
fs/btrfs/scrub.c:		spage->physical = physical;
fs/btrfs/scrub.c:		spage->mirror_num = mirror_num;
fs/btrfs/scrub.c:			spage->have_csum = 1;
fs/btrfs/scrub.c:			memcpy(spage->csum, csum, sctx->csum_size);
fs/btrfs/scrub.c:			spage->have_csum = 0;
fs/btrfs/scrub.c:		spage->page = alloc_page(GFP_KERNEL);
fs/btrfs/scrub.c:		if (!spage->page)
fs/btrfs/tests/extent-io-tests.c:	/* Do it over again with an extent buffer which isn't page-aligned. */
fs/buffer.c:	struct address_space *buffer_mapping = bh->b_page->mapping;
fs/buffer.c:	if (page->mapping) {	/* Race with truncate? */
fs/buffer.c:	 * Lock out page->mem_cgroup migration to keep PageDirty
fs/buffer.c: * Create the page-cache page that contains the requested block.
fs/buffer.c: * mark_buffer_dirty() is atomic.  It takes bh->b_page->mapping->private_lock,
fs/buffer.c:	if (bh->b_page && bh->b_page->mapping)
fs/buffer.c:		mapping_set_error(bh->b_page->mapping, -EIO);
fs/buffer.c:		struct address_space *buffer_mapping = bh->b_page->mapping;
fs/buffer.c:	spin_lock(&page->mapping->private_lock);
fs/buffer.c:	spin_unlock(&page->mapping->private_lock);
fs/buffer.c:	block = (sector_t)page->index << (PAGE_SHIFT - bbits);
fs/buffer.c:	mapping_set_error(page->mapping, err);
fs/buffer.c:	struct inode *inode = page->mapping->host;
fs/buffer.c:	block = (sector_t)page->index << (PAGE_SHIFT - bbits);
fs/buffer.c:	struct inode *inode = page->mapping->host;
fs/buffer.c:	iblock = (sector_t)page->index << (PAGE_SHIFT - bbits);
fs/buffer.c:	struct inode *inode = page->mapping->host;
fs/buffer.c:	if ((page->mapping != inode->i_mapping) ||
fs/buffer.c:	if (((page->index + 1) << PAGE_SHIFT) > size)
fs/buffer.c:	spin_lock(&page->mapping->private_lock);
fs/buffer.c:	spin_unlock(&page->mapping->private_lock);
fs/buffer.c:	block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
fs/buffer.c:	struct inode *inode = page->mapping->host;
fs/buffer.c:	struct inode * const inode = page->mapping->host;
fs/buffer.c:	if (page->index < end_index)
fs/buffer.c:	if (page->index >= end_index+1 || !offset) {
fs/buffer.c:		if (page->mapping->a_ops->invalidatepage)
fs/buffer.c:			page->mapping->a_ops->invalidatepage(page, offset);
fs/buffer.c:	struct inode * const inode = page->mapping->host;
fs/buffer.c:	if (page->index < end_index)
fs/buffer.c:	if (page->index >= end_index+1 || !offset) {
fs/buffer.c:	struct address_space * const mapping = page->mapping;
fs/cachefiles/rdwr.c:	       monitor->netfs_page->index, mode, sync,
fs/cachefiles/rdwr.c:	if (key->flags != &page->flags ||
fs/cachefiles/rdwr.c:	_debug("--- monitor %p %lx ---", page, page->flags);
fs/cachefiles/rdwr.c:	       backpage->index, backpage->flags);
fs/cachefiles/rdwr.c:	if (backpage->mapping != bmapping) {
fs/cachefiles/rdwr.c:	backpage2 = find_get_page(bmapping, backpage->index);
fs/cachefiles/rdwr.c:		_debug("jumpstart %p {%lx}", backpage, backpage->flags);
fs/cachefiles/rdwr.c:		_debug("- copy {%lu}", monitor->back_page->index);
fs/cachefiles/rdwr.c:				(unsigned long) monitor->back_page->flags);
fs/cachefiles/rdwr.c:	       netpage, netpage->index, page_count(netpage));
fs/cachefiles/rdwr.c:		backpage = find_get_page(bmapping, netpage->index);
fs/cachefiles/rdwr.c:					    netpage->index, cachefiles_gfp);
fs/cachefiles/rdwr.c:		_debug("jumpstart %p {%lx}", backpage, backpage->flags);
fs/cachefiles/rdwr.c:	_debug("read %p {%lx}", backpage, backpage->flags);
fs/cachefiles/rdwr.c:	_enter("{%p},{%lx},,,", object, page->index);
fs/cachefiles/rdwr.c:	block0 = page->index;
fs/cachefiles/rdwr.c:		list_del(&netpage->lru);
fs/cachefiles/rdwr.c:		       netpage, netpage->index, page_count(netpage));
fs/cachefiles/rdwr.c:			backpage = find_get_page(bmapping, netpage->index);
fs/cachefiles/rdwr.c:						    netpage->index,
fs/cachefiles/rdwr.c:					    netpage->index, cachefiles_gfp);
fs/cachefiles/rdwr.c:			_debug("2unlock %p {%lx}", backpage, backpage->flags);
fs/cachefiles/rdwr.c:		_debug("- not ready %p{%lx}", backpage, backpage->flags);
fs/cachefiles/rdwr.c:			_debug("error %lx", backpage->flags);
fs/cachefiles/rdwr.c:		_debug("uptodate %lx", backpage->flags);
fs/cachefiles/rdwr.c:					    netpage->index, cachefiles_gfp);
fs/cachefiles/rdwr.c:		list_del(&netpage->lru);
fs/cachefiles/rdwr.c:		block0 = page->index;
fs/cachefiles/rdwr.c:			list_move(&page->lru, &backpages);
fs/cachefiles/rdwr.c:	_enter("%p,{%lx},", object, page->index);
fs/cachefiles/rdwr.c:	_enter("%p,%p{%lx},,,", object, page, page->index);
fs/cachefiles/rdwr.c:	pos = (loff_t)page->index << PAGE_SHIFT;
fs/cachefiles/rdwr.c:	_enter("%p,{%lu}", object, page->index);
fs/ceph/addr.c: * The page->private field is used to reference a struct
fs/ceph/addr.c:		return (void *)page->private;
fs/ceph/addr.c:	struct address_space *mapping = page->mapping;
fs/ceph/addr.c:		     mapping->host, page, page->index);
fs/ceph/addr.c:	     mapping->host, page, page->index,
fs/ceph/addr.c:	 * Reference snap context in page->private.  Also set
fs/ceph/addr.c:	page->private = (unsigned long)snapc;
fs/ceph/addr.c:	WARN_ON(!page->mapping);
fs/ceph/addr.c:	inode = page->mapping->host;
fs/ceph/addr.c:		     inode, page, page->index, offset, length);
fs/ceph/addr.c:	     inode, page, page->index);
fs/ceph/addr.c:	page->private = 0;
fs/ceph/addr.c:	dout("%p releasepage %p idx %lu (%sdirty)\n", page->mapping->host,
fs/ceph/addr.c:	     page, page->index, PageDirty(page) ? "" : "not ");
fs/ceph/addr.c:	     inode, filp, page, page->index);
fs/ceph/addr.c:		     page->index);
fs/ceph/addr.c:				list_del(&page->lru);
fs/ceph/addr.c:	next_index = page->index;
fs/ceph/addr.c:		if (page->index != next_index)
fs/ceph/addr.c:		list_del(&page->lru);
fs/ceph/addr.c:		     page->index);
fs/ceph/addr.c:		if (add_to_page_cache_lru(page, &inode->i_data, page->index,
fs/ceph/addr.c:	dout("writepage %p idx %lu\n", page, page->index);
fs/ceph/addr.c:	inode = page->mapping->host;
fs/ceph/addr.c:		page->mapping->a_ops->invalidatepage(page, 0, PAGE_SIZE);
fs/ceph/addr.c:	     inode, page, page->index, page_off, len, snapc, snapc->seq);
fs/ceph/addr.c:	page->private = 0;
fs/ceph/addr.c:	struct inode *inode = page->mapping->host;
fs/ceph/addr.c:			page->private = 0;
fs/ceph/addr.c:			dout("? %p idx %lu\n", page, page->index);
fs/ceph/addr.c:			    unlikely(page->mapping != mapping)) {
fs/ceph/addr.c:			if (strip_unit_end && (page->index > strip_unit_end)) {
fs/ceph/addr.c:				strip_unit_end = page->index +
fs/ceph/addr.c:			} else if (page->index !=
fs/ceph/addr.c:			     inode, page, page->index);
fs/ceph/addr.c:		if ((off > size) || (page->mapping != inode->i_mapping)) {
fs/ceph/cache.h:	struct inode* inode = page->mapping->host;
fs/cifs/cifs_unicode.c:	if (!strcmp(codepage->charset, "utf8")) {
fs/cifs/cifs_unicode.c:		charlen = codepage->char2uni(from, len, &wchar_to);
fs/cifs/cifs_unicode.c:		charlen = codepage->char2uni(from, len, &wchar_to);
fs/cifs/dir.c:		charlen = codepage->char2uni(&q->name[i], q->len - i, &c);
fs/cifs/dir.c:		l1 = codepage->char2uni(&str[i], len - i, &c1);
fs/cifs/dir.c:		l2 = codepage->char2uni(&name->name[i], name->len - i, &c2);
fs/cifs/file.c:	struct address_space *mapping = page->mapping;
fs/cifs/file.c:	loff_t offset = (loff_t)page->index << PAGE_SHIFT;
fs/cifs/file.c:	inode = page->mapping->host;
fs/cifs/file.c:		 * (changing page->mapping to NULL), or even swizzled
fs/cifs/file.c:		if (unlikely(page->mapping != mapping)) {
fs/cifs/file.c:		if (!wbc->range_cyclic && page->index > end) {
fs/cifs/file.c:		if (*next && (page->index != *next)) {
fs/cifs/file.c:		*next = page->index + 1;
fs/cifs/file.c:		mapping_set_error(page->mapping, rc);
fs/cifs/file.c:		} else if (page->index > eof_index) {
fs/cifs/file.c:				      page->index, gfp);
fs/cifs/file.c:	*offset = (loff_t)page->index << PAGE_SHIFT;
fs/cifs/file.c:	list_move_tail(&page->lru, tmplist);
fs/cifs/file.c:	expected_index = page->index + 1;
fs/cifs/file.c:		if (page->index != expected_index)
fs/cifs/file.c:		if (add_to_page_cache_locked(page, mapping, page->index, gfp)) {
fs/cifs/file.c:		list_move_tail(&page->lru, tmplist);
fs/cifs/file.c:				list_del(&page->lru);
fs/cifs/file.c:			list_del(&page->lru);
fs/cifs/file.c:	loff_t offset = (loff_t)page->index << PAGE_SHIFT;
fs/cifs/file.c:	struct cifsInodeInfo *cifsi = CIFS_I(page->mapping->host);
fs/cifs/file.c:	cifs_fscache_invalidate_page(page, page->mapping->host);
fs/cifs/fscache.c:		struct inode *inode = page->mapping->host;
fs/coda/symlink.c:	struct inode *inode = page->mapping->host;
fs/cramfs/inode.c:	struct inode *inode = page->mapping->host;
fs/cramfs/inode.c:	if (page->index < maxblock) {
fs/cramfs/inode.c:		u32 blkptr_offset = OFFSET(inode) + page->index * 4;
fs/cramfs/inode.c:				if (page->index == maxblock - 1)
fs/cramfs/inode.c:			if (page->index)
fs/crypto/bio.c:		int ret = fscrypt_decrypt_page(page->mapping->host, page,
fs/crypto/bio.c:				PAGE_SIZE, 0, page->index);
fs/dax.c:		WARN_ON_ONCE(page->mapping);
fs/dax.c:		page->mapping = mapping;
fs/dax.c:		page->index = index + i++;
fs/dax.c:		WARN_ON_ONCE(page->mapping && page->mapping != mapping);
fs/dax.c:		page->mapping = NULL;
fs/dax.c:		page->index = 0;
fs/dax.c:	/* Ensure page->mapping isn't freed while we look at it */
fs/dax.c:		struct address_space *mapping = READ_ONCE(page->mapping);
fs/dax.c:		if (mapping != page->mapping) {
fs/dax.c:		xas_set(&xas, page->index);
fs/dax.c:	struct address_space *mapping = page->mapping;
fs/dax.c:	XA_STATE(xas, &mapping->i_pages, page->index);
fs/dax.c: * page->count == 1. A filesystem uses this interface to determine if
fs/direct-io.c: * happily perform page-sized but 512-byte aligned IOs.  It is important that
fs/ecryptfs/crypto.c:	       ((loff_t)page->index << PAGE_SHIFT);
fs/ecryptfs/crypto.c:	pgoff_t page_index = op == ENCRYPT ? src_page->index : dst_page->index;
fs/ecryptfs/crypto.c:	ecryptfs_inode = page->mapping->host;
fs/ecryptfs/crypto.c:	ecryptfs_inode = page->mapping->host;
fs/ecryptfs/mmap.c:#include <linux/page-flags.h>
fs/ecryptfs/mmap.c:				"page (upper index [0x%.16lx])\n", page->index);
fs/ecryptfs/mmap.c:		loff_t view_extent_num = ((((loff_t)page->index)
fs/ecryptfs/mmap.c:					page_virt, page->mapping->host);
fs/ecryptfs/mmap.c:				crypt_stat->extent_size, page->mapping->host);
fs/ecryptfs/mmap.c:		&ecryptfs_inode_to_private(page->mapping->host)->crypt_stat;
fs/ecryptfs/mmap.c:		rc = ecryptfs_read_lower_page_segment(page, page->index, 0,
fs/ecryptfs/mmap.c:						      page->mapping->host);
fs/ecryptfs/mmap.c:				page, page->index, 0, PAGE_SIZE,
fs/ecryptfs/mmap.c:				page->mapping->host);
fs/ecryptfs/mmap.c:			page->index);
fs/ecryptfs/mmap.c:	struct inode *inode = page->mapping->host;
fs/ecryptfs/mmap.c:	if ((i_size_read(inode) / PAGE_SIZE) != page->index)
fs/ecryptfs/mmap.c:			    >= i_size_read(page->mapping->host)) {
fs/ecryptfs/mmap.c:					       __func__, page->index, rc);
fs/ecryptfs/mmap.c:		if (prev_page_end_size > i_size_read(page->mapping->host)) {
fs/ecryptfs/read_write.c: * eCryptfs inode page cache. This is done on a page-by-page, and then
fs/efs/symlink.c:	struct inode * inode = page->mapping->host;
fs/exofs/dir.c:	struct address_space *mapping = page->mapping;
fs/exofs/dir.c:	struct inode *dir = page->mapping->host;
fs/exofs/dir.c:	if ((dir->i_size >> PAGE_SHIFT) == page->index) {
fs/exofs/dir.c:		dir->i_ino, error, (page->index<<PAGE_SHIFT)+offs,
fs/exofs/dir.c:		dir->i_ino, (page->index<<PAGE_SHIFT)+offs,
fs/exofs/dir.c:	err = exofs_write_begin(NULL, page->mapping, pos, len, 0, &page, NULL);
fs/exofs/dir.c:	err = exofs_write_begin(NULL, page->mapping, pos, rec_len, 0,
fs/exofs/dir.c:	struct address_space *mapping = page->mapping;
fs/exofs/dir.c:	err = exofs_write_begin(NULL, page->mapping, pos, to - from, 0,
fs/exofs/dir.c:	err = exofs_write_begin(NULL, page->mapping, 0, chunk_size, 0,
fs/exofs/inode.c:		mapping_set_error(page->mapping, ret);
fs/exofs/inode.c:		struct inode *inode = page->mapping->host;
fs/exofs/inode.c:			  inode->i_ino, page->index,
fs/exofs/inode.c:			  page->index);
fs/exofs/inode.c:	if (page->index < end_index)
fs/exofs/inode.c:	else if (page->index == end_index)
fs/exofs/inode.c:			     pcol->read_4_write, page->index, end_index);
fs/exofs/inode.c:		pcol->pg_first = page->index;
fs/exofs/inode.c:		   page->index)) {
fs/exofs/inode.c:		     inode->i_ino, page->index, len);
fs/exofs/inode.c:	_pcol_init(&pcol, 1, page->mapping->host);
fs/exofs/inode.c:		struct inode *inode = page->mapping->host;
fs/exofs/inode.c:			     inode->i_ino, page->index, page_stat);
fs/exofs/inode.c:	    (pcol->that_locked_page->index != index)) {
fs/exofs/inode.c:			     pcol->that_locked_page->index);
fs/exofs/inode.c:		EXOFS_DBGMSG2("index=0x%lx\n", page->index);
fs/exofs/inode.c:		     ZERO_PAGE(0) == page ? -1 : page->index);
fs/exofs/inode.c:	if (page->index < end_index)
fs/exofs/inode.c:		if (page->index > end_index || !len) {
fs/exofs/inode.c:				     inode->i_ino, page->index);
fs/exofs/inode.c:		pcol->pg_first = page->index;
fs/exofs/inode.c:		   page->index)) {
fs/exofs/inode.c:			     inode->i_ino, page->index);
fs/exofs/inode.c:		     inode->i_ino, page->index, len);
fs/exofs/inode.c:		     inode->i_ino, page->index, ret);
fs/exofs/inode.c:	mapping_set_error(page->mapping, -EIO);
fs/exofs/inode.c:	_pcol_init(&pcol, 1, page->mapping->host);
fs/exofs/inode.c:		if (page->index > end_index) {
fs/exofs/inode.c:	EXOFS_DBGMSG("page 0x%lx\n", page->index);
fs/exofs/inode.c:		     page->index, offset, length);
fs/exofs/ore_raid.c:		   _LLU(si.obj_offset), pg_len, page->index, si.dev);
fs/ext2/dir.c:	struct address_space *mapping = page->mapping;
fs/ext2/dir.c:	struct inode *dir = page->mapping->host;
fs/ext2/dir.c:	if ((dir->i_size >> PAGE_SHIFT) == page->index) {
fs/ext2/dir.c:			dir->i_ino, error, (page->index<<PAGE_SHIFT)+offs,
fs/ext2/dir.c:			dir->i_ino, (page->index<<PAGE_SHIFT)+offs,
fs/ext2/dir.c:	struct inode *inode = page->mapping->host;
fs/ext4/Makefile:		mmp.o move_extent.o namei.o page-io.o readpage.o resize.o \
fs/ext4/ext4.h:/* page-io.c */
fs/ext4/extents.c: * stuff such as page-cache locking consistency, bh mapping consistency or
fs/ext4/inline.c:	BUG_ON(page->index);
fs/ext4/inline.c:	if (!page->index)
fs/ext4/inode.c:	struct inode *inode = page->mapping->host;
fs/ext4/inode.c:	block = (sector_t)page->index << (PAGE_SHIFT - bbits);
fs/ext4/inode.c:		err = fscrypt_decrypt_page(page->mapping->host, page,
fs/ext4/inode.c:				PAGE_SIZE, 0, page->index);
fs/ext4/inode.c:	if (page->mapping != mapping) {
fs/ext4/inode.c:	struct inode *inode = page->mapping->host;
fs/ext4/inode.c:			lblk = page->index <<
fs/ext4/inode.c:		lblk = page->index << (PAGE_SHIFT - inode->i_blkbits);
fs/ext4/inode.c:	struct address_space *mapping = page->mapping;
fs/ext4/inode.c:		BUG_ON(page->index != 0);
fs/ext4/inode.c:	if (page->mapping != mapping) {
fs/ext4/inode.c:	struct inode *inode = page->mapping->host;
fs/ext4/inode.c:	if (page->index == size >> PAGE_SHIFT)
fs/ext4/inode.c:	BUG_ON(page->index != mpd->first_page);
fs/ext4/inode.c:	if (page->index == size >> PAGE_SHIFT)
fs/ext4/inode.c:			if (mpd->map.m_len > 0 && mpd->next_page != page->index)
fs/ext4/inode.c:			    unlikely(page->mapping != mapping)) {
fs/ext4/inode.c:				mpd->first_page = page->index;
fs/ext4/inode.c:			mpd->next_page = page->index + 1;
fs/ext4/inode.c:			lblk = ((ext4_lblk_t)page->index) <<
fs/ext4/inode.c:	if (page->mapping != mapping) {
fs/ext4/inode.c:	struct inode *inode = page->mapping->host;
fs/ext4/inode.c:	 * mm/page-writeback.c, marks pages clean in preparation for
fs/ext4/inode.c:	struct inode *inode = page->mapping->host;
fs/ext4/inode.c:		return ext4_mpage_readpages(page->mapping, NULL, page, 1,
fs/ext4/inode.c:	journal_t *journal = EXT4_JOURNAL(page->mapping->host);
fs/ext4/inode.c:	journal_t *journal = EXT4_JOURNAL(page->mapping->host);
fs/ext4/inode.c:			WARN_ON_ONCE(fscrypt_decrypt_page(page->mapping->host,
fs/ext4/inode.c:						page, PAGE_SIZE, 0, page->index));
fs/ext4/inode.c:		error = ___wait_var_event(&page->_refcount,
fs/ext4/inode.c:				atomic_read(&page->_refcount) == 1,
fs/ext4/inode.c:	if (page->mapping != mapping || page_offset(page) > size) {
fs/ext4/inode.c:	if (page->index == size >> PAGE_SHIFT)
fs/ext4/mballoc.c:	mb_debug(1, "init page %lu\n", page->index);
fs/ext4/mballoc.c:	inode = page->mapping->host;
fs/ext4/mballoc.c:	first_group = page->index * blocks_per_page / 2;
fs/ext4/mballoc.c:	first_block = page->index * blocks_per_page;
fs/ext4/mballoc.c:				group, page->index, i * blocksize);
fs/ext4/mballoc.c:				group, page->index, i * blocksize);
fs/ext4/mballoc.c:	BUG_ON(page->mapping != inode->i_mapping);
fs/ext4/mballoc.c:	BUG_ON(page->mapping != inode->i_mapping);
fs/ext4/mballoc.c:			BUG_ON(page->mapping != inode->i_mapping);
fs/ext4/mballoc.c:			BUG_ON(page->mapping != inode->i_mapping);
fs/ext4/move_extent.c:	struct inode *inode = page->mapping->host;
fs/ext4/move_extent.c:	block = (sector_t)page->index << (PAGE_SHIFT - inode->i_blkbits);
fs/ext4/move_extent.c:		if (cur_len > blocks_per_page- offset_in_page)
fs/ext4/page-io.c: * linux/fs/ext4/page-io.c
fs/ext4/page-io.c:		if (!page->mapping) {
fs/ext4/page-io.c:			mapping_set_error(page->mapping, -EIO);
fs/ext4/page-io.c:	struct inode *inode = page->mapping->host;
fs/ext4/page-io.c:						page->index, gfp_flags);
fs/ext4/readpage.c:		prefetchw(&page->flags);
fs/ext4/readpage.c:			list_del(&page->lru);
fs/ext4/readpage.c:			if (add_to_page_cache_lru(page, mapping, page->index,
fs/ext4/readpage.c:		block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
fs/f2fs/checkpoint.c:	if (unlikely(page->mapping != mapping)) {
fs/f2fs/checkpoint.c:	if (wbc->for_reclaim && page->index < GET_SUM_BLOCK(sbi, 0))
fs/f2fs/checkpoint.c:				prev = page->index - 1;
fs/f2fs/checkpoint.c:			if (nr_to_write != LONG_MAX && page->index != prev + 1) {
fs/f2fs/checkpoint.c:			if (unlikely(page->mapping != mapping)) {
fs/f2fs/checkpoint.c:			prev = page->index;
fs/f2fs/data.c:	struct address_space *mapping = page->mapping;
fs/f2fs/data.c:	struct address_space *mapping = page->mapping;
fs/f2fs/data.c:			mapping_set_error(page->mapping, -EIO);
fs/f2fs/data.c:		f2fs_bug_on(sbi, page->mapping == NODE_MAPPING(sbi) &&
fs/f2fs/data.c:					page->index != nid_of_node(page));
fs/f2fs/data.c:		if (bvec->bv_page->mapping)
fs/f2fs/data.c:	if (unlikely(page->mapping != mapping)) {
fs/f2fs/data.c:			prefetchw(&page->flags);
fs/f2fs/data.c:			list_del(&page->lru);
fs/f2fs/data.c:						  page->index,
fs/f2fs/data.c:		block_in_file = (sector_t)page->index;
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:		ret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);
fs/f2fs/data.c:	struct inode *inode = fio->page->mapping->host;
fs/f2fs/data.c:			PAGE_SIZE, 0, fio->page->index, gfp_flags);
fs/f2fs/data.c:	struct inode *inode = fio->page->mapping->host;
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:			f2fs_lookup_extent_cache(inode, page->index, &ei)) {
fs/f2fs/data.c:		fio->old_blkaddr = ei.blk + page->index - ei.fofs;
fs/f2fs/data.c:	/* Deadlock due to between page->lock and f2fs_lock_op */
fs/f2fs/data.c:	err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
fs/f2fs/data.c:	if (page->index == 0)
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:	loff_t psize = (page->index + 1) << PAGE_SHIFT;
fs/f2fs/data.c:		mapping_set_error(page->mapping, -EIO);
fs/f2fs/data.c:	if (page->index < end_index)
fs/f2fs/data.c:	if ((page->index >= end_index + 1) || !offset)
fs/f2fs/data.c:	if (f2fs_is_volatile_file(inode) && (!page->index ||
fs/f2fs/data.c: * This function was copied from write_cche_pages from mm/page-writeback.c.
fs/f2fs/data.c:			done_index = page->index;
fs/f2fs/data.c:			if (unlikely(page->mapping != mapping)) {
fs/f2fs/data.c:				done_index = page->index + 1;
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:	pgoff_t index = page->index;
fs/f2fs/data.c:		if (page->mapping != mapping) {
fs/f2fs/data.c:		if (unlikely(page->mapping != mapping)) {
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:	struct inode *inode = page->mapping->host;
fs/f2fs/data.c:	struct address_space *mapping = page->mapping;
fs/f2fs/dir.c:		!f2fs_truncate_hole(dir, page->index, page->index + 1)) {
fs/f2fs/f2fs.h:#include <linux/page-flags.h>
fs/f2fs/f2fs.h:	return F2FS_M_SB(page->mapping);
fs/f2fs/file.c:	if (unlikely(page->mapping != inode->i_mapping ||
fs/f2fs/file.c:	err = f2fs_get_block(&dn, page->index);
fs/f2fs/file.c:	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
fs/f2fs/file.c:	pgofs = page->index;
fs/f2fs/gc.c:	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi))) {
fs/f2fs/gc.c:	if (page->index == 0)
fs/f2fs/inline.c:	struct inode *inode = page->mapping->host;
fs/f2fs/inline.c:	f2fs_bug_on(F2FS_P_SB(page), page->index);
fs/f2fs/inline.c:	if (page->index)
fs/f2fs/inline.c:	f2fs_bug_on(F2FS_I_SB(inode), page->index);
fs/f2fs/node.c:	return NODE_MAPPING(sbi) == page->mapping &&
fs/f2fs/node.c:	index = dn->node_page->index;
fs/f2fs/node.c:			BUG_ON(page->mapping != NODE_MAPPING(sbi));
fs/f2fs/node.c:	err = f2fs_get_node_info(sbi, page->index, &ni);
fs/f2fs/node.c:	if (unlikely(page->mapping != NODE_MAPPING(sbi))) {
fs/f2fs/node.c:			if (unlikely(page->mapping != NODE_MAPPING(sbi))) {
fs/f2fs/node.c:	f2fs_bug_on(sbi, page->index != nid);
fs/f2fs/node.c:			if (unlikely(page->mapping != NODE_MAPPING(sbi))) {
fs/f2fs/node.c:					ino, last_page->index);
fs/f2fs/node.c:			if (unlikely(page->mapping != NODE_MAPPING(sbi))) {
fs/f2fs/segment.c:			err = f2fs_get_dnode_of_data(&dn, page->index,
fs/f2fs/segment.c:		if (page->mapping == inode->i_mapping) {
fs/f2fs/segment.c:		struct inode *inode = fio->page->mapping->host;
fs/f2fs/segment.c:		struct inode *inode = fio->page->mapping->host;
fs/f2fs/segment.c:		.old_blkaddr = page->index,
fs/f2fs/segment.c:		.new_blkaddr = page->index,
fs/f2fs/segment.c:	if (unlikely(page->index >= MAIN_BLKADDR(sbi)))
fs/f2fs/segment.c:	stat_inc_meta_count(sbi, page->index);
fs/f2fs/super.c:		if (unlikely(page->mapping != mapping)) {
fs/f2fs/trace.c:	struct inode *inode = page->mapping->host;
fs/f2fs/trace.c:	inode = fio->page->mapping->host;
fs/file.c:	 * the fdarray into comfortable page-tuned chunks: starting at 1024B
fs/fs-writeback.c: * page->mapping->host, so the page-dirtying time is recorded in the internal
fs/fscache/page.c:	val = radix_tree_lookup(&cookie->stores, page->index);
fs/fscache/page.c:	val = radix_tree_lookup(&cookie->stores, page->index);
fs/fscache/page.c:	if (radix_tree_tag_get(&cookie->stores, page->index,
fs/fscache/page.c:	if (radix_tree_tag_get(&cookie->stores, page->index,
fs/fscache/page.c:	xpage = radix_tree_delete(&cookie->stores, page->index);
fs/fscache/page.c:			page, page->index);
fs/fscache/page.c:		radix_tree_tag_clear(&cookie->stores, page->index,
fs/fscache/page.c:		if (!radix_tree_tag_get(&cookie->stores, page->index,
fs/fscache/page.c:			xpage = radix_tree_delete(&cookie->stores, page->index);
fs/fscache/page.c:			val = radix_tree_lookup(&cookie->stores, page->index);
fs/fscache/page.c:	op = fscache_alloc_retrieval(cookie, page->mapping,
fs/fscache/page.c:	op = fscache_alloc_retrieval(cookie, page->mapping, NULL, NULL);
fs/fscache/page.c:	_debug("gang %d [%lx]", n, page->index);
fs/fscache/page.c:	radix_tree_tag_set(&cookie->stores, page->index,
fs/fscache/page.c:	radix_tree_tag_clear(&cookie->stores, page->index,
fs/fscache/page.c:	if (page->index >= op->store_limit)
fs/fscache/page.c:			radix_tree_delete(&cookie->stores, page->index);
fs/fscache/page.c:	_enter("%p,%x,", cookie, (u32) page->flags);
fs/fscache/page.c:	ret = radix_tree_insert(&cookie->stores, page->index, page);
fs/fscache/page.c:	radix_tree_tag_set(&cookie->stores, page->index,
fs/fscache/page.c:	radix_tree_delete(&cookie->stores, page->index);
fs/fscache/page.c:	_debug("- mark %p{%lx}", page, page->index);
fs/fscache/page.c:				cookie->def->name, page->index);
fs/fuse/dev.c: * anything that could cause a page-fault.  If the request was already
fs/fuse/dev.c:	    page->mapping != NULL ||
fs/fuse/dev.c:	    (page->flags & PAGE_FLAGS_CHECK_AT_PREP &
fs/fuse/dev.c:		printk(KERN_WARNING "  page=%p index=%li flags=%08lx, count=%i, mapcount=%i, mapping=%p\n", page, page->index, page->flags, page_count(page), page_mapcount(page), page->mapping);
fs/fuse/dir.c:	int err = fuse_readlink_page(page->mapping->host, page);
fs/fuse/file.c:	struct inode *inode = page->mapping->host;
fs/fuse/file.c:	 * page-cache page, so make sure we read a properly synced
fs/fuse/file.c:	fuse_wait_on_page_writeback(inode, page->index);
fs/fuse/file.c:	struct inode *inode = page->mapping->host;
fs/fuse/file.c:	fuse_wait_on_page_writeback(inode, page->index);
fs/fuse/file.c:	     req->pages[req->num_pages - 1]->index + 1 != page->index)) {
fs/fuse/file.c:	struct address_space *mapping = page->mapping;
fs/fuse/file.c:	mapping_set_error(page->mapping, error);
fs/fuse/file.c:	if (fuse_page_is_writeback(page->mapping->host, page->index)) {
fs/fuse/file.c:		if (curr_index <= page->index &&
fs/fuse/file.c:		    page->index < curr_index + old_req->num_pages) {
fs/fuse/file.c:		    curr_index == page->index) {
fs/fuse/file.c:		struct backing_dev_info *bdi = inode_to_bdi(page->mapping->host);
fs/fuse/file.c:	is_writeback = fuse_page_is_writeback(inode, page->index);
fs/fuse/file.c:	     data->orig_pages[req->num_pages - 1]->index + 1 != page->index)) {
fs/fuse/file.c:	fuse_wait_on_page_writeback(mapping->host, page->index);
fs/fuse/file.c:	struct inode *inode = page->mapping->host;
fs/fuse/file.c:		struct inode *inode = page->mapping->host;
fs/fuse/file.c:			fuse_wait_on_page_writeback(inode, page->index);
fs/fuse/file.c:	if (page->mapping != inode->i_mapping) {
fs/fuse/file.c:	fuse_wait_on_page_writeback(inode, page->index);
fs/fuse/fuse_i.h:	/** page-descriptor vector */
fs/fuse/fuse_i.h:	/** inline page-descriptor vector */
fs/gfs2/aops.c:	struct inode *inode = page->mapping->host;
fs/gfs2/aops.c:	if (page->index > end_index || (page->index == end_index && !offset)) {
fs/gfs2/aops.c:		page->mapping->a_ops->invalidatepage(page, 0, PAGE_SIZE);
fs/gfs2/aops.c:	struct inode * const inode = page->mapping->host;
fs/gfs2/aops.c:	if (page->index == end_index && offset)
fs/gfs2/aops.c:	struct inode *inode = page->mapping->host;
fs/gfs2/aops.c:	struct inode *inode = page->mapping->host;
fs/gfs2/aops.c:		*done_index = page->index;
fs/gfs2/aops.c:		if (unlikely(page->mapping != mapping)) {
fs/gfs2/aops.c:				*done_index = page->index + 1;
fs/gfs2/aops.c:	if (unlikely(page->index)) {
fs/gfs2/aops.c:	struct gfs2_inode *ip = GFS2_I(page->mapping->host);
fs/gfs2/aops.c:	struct gfs2_sbd *sdp = GFS2_SB(page->mapping->host);
fs/gfs2/aops.c:	if (i_blocksize(page->mapping->host) == PAGE_SIZE &&
fs/gfs2/aops.c:	struct address_space *mapping = page->mapping;
fs/gfs2/aops.c:	if (page->mapping == mapping && !PageUptodate(page))
fs/gfs2/aops.c:	struct gfs2_sbd *sdp = GFS2_SB(page->mapping->host);
fs/gfs2/aops.c:	struct address_space *mapping = page->mapping;
fs/gfs2/bmap.c:	if (!page || page->index) {
fs/gfs2/file.c:	struct inode *inode = page->mapping->host;
fs/gfs2/file.c:	u64 lblock = page->index << (PAGE_SHIFT - inode->i_blkbits);
fs/gfs2/file.c:	u64 pos = page->index << PAGE_SHIFT;
fs/gfs2/file.c:		if (!PageUptodate(page) || page->mapping != inode->i_mapping) {
fs/gfs2/file.c:	if (size == 0 || (page->index > last_index))
fs/gfs2/file.c:	if (!PageUptodate(page) || page->mapping != inode->i_mapping)
fs/gfs2/glops.c:	       bh->b_page->mapping, bh->b_page->flags);
fs/gfs2/log.c:		mapping = bh->b_page->mapping;
fs/gfs2/meta_io.c:	struct address_space *mapping = bh->b_page->mapping;
fs/hfs/inode.c:	struct inode *inode = page->mapping->host;
fs/hfs/inode.c:		nidx = page->index >> (tree->node_size_shift - PAGE_SHIFT);
fs/hfs/inode.c:		nidx = page->index << (PAGE_SHIFT - tree->node_size_shift);
fs/hfsplus/bnode.c:				src_page--;
fs/hfsplus/bnode.c:				dst_page--;
fs/hfsplus/bnode.c:					dst_page--;
fs/hfsplus/bnode.c:					src_page--;
fs/hfsplus/inode.c:	struct inode *inode = page->mapping->host;
fs/hfsplus/inode.c:		nidx = page->index >>
fs/hfsplus/inode.c:		nidx = page->index <<
fs/hostfs/hostfs_kern.c:	struct address_space *mapping = page->mapping;
fs/hostfs/hostfs_kern.c:	if (page->index >= end_index)
fs/hpfs/namei.c:	struct inode *i = page->mapping->host;
fs/hugetlbfs/inode.c: * hugetlbpage-backed filesystem.  Based on ramfs.
fs/hugetlbfs/inode.c:			index = page->index;
fs/hugetlbfs/inode.c:	pgoff_t index = page->index;
fs/iomap.c:	struct inode *inode = page->mapping->host;
fs/iomap.c:	BUG_ON(page->index);
fs/iomap.c:		gfp_t gfp = mapping_gfp_constraint(page->mapping, GFP_KERNEL);
fs/iomap.c:	struct inode *inode = page->mapping->host;
fs/iomap.c:		list_del(&page->lru);
fs/iomap.c:		if (!add_to_page_cache_lru(page, inode->i_mapping, page->index,
fs/iomap.c:	struct inode *inode = page->mapping->host;
fs/iomap.c:	 * Lock out page->mem_cgroup migration to keep PageDirty
fs/iomap.c:	if ((page->mapping != inode->i_mapping) ||
fs/iomap.c:	if (((page->index + 1) << PAGE_SHIFT) > size)
fs/iomap.c:	if (unlikely(page->mapping != inode->i_mapping))
fs/iomap.c: * page numbers of the swap device to the physical page-aligned extents.
fs/iomap.c: * swap only cares about contiguous page-aligned physical extents and makes no
fs/isofs/compress.c:	pgoff_t index = page->index, end_index;
fs/isofs/rock.c:	struct inode *inode = page->mapping->host;
fs/jbd2/commit.c:	if (page->mapping)
fs/jbd2/journal.c: * page-sized chunks of data, but sometimes it will be in
fs/jbd2/journal.c: * sub-page-size chunks.  (For example, 16k pages on Power systems
fs/jbd2/journal.c:			(bh->b_page && bh->b_page->mapping));
fs/jffs2/scan.c:		   contains data, and the end of the data isn't page-aligned,
fs/jfs/jfs_metapage.c:		l2mp_blocks = L2PSIZE - page->mapping->host->i_blkbits;
fs/jfs/jfs_metapage.c:	int l2mp_blocks = L2PSIZE - page->mapping->host->i_blkbits;
fs/jfs/jfs_metapage.c:	struct inode *inode = page->mapping->host;
fs/jfs/jfs_metapage.c:	page_start = (sector_t)page->index <<
fs/jfs/jfs_metapage.c:	struct inode *inode = page->mapping->host;
fs/jfs/jfs_metapage.c:	page_start = (sector_t)page->index <<
fs/jfs/jfs_txnmgr.c: * log age of page-frames in memory for which caller has
fs/jfs/jfs_unicode.c:			    codepage->uni2char(le16_to_cpu(from[i]),
fs/jfs/jfs_unicode.c:			charlen = codepage->char2uni(from, len, &to[i]);
fs/jfs/jfs_unicode.c:					codepage->charset, *from);
fs/libfs.c:	struct inode *inode = page->mapping->host;
fs/minix/dir.c:	struct address_space *mapping = page->mapping;
fs/minix/dir.c:	struct inode *inode = page->mapping->host;
fs/minix/dir.c:	struct inode *dir = page->mapping->host;
fs/minix/dir.c:		struct address_space *mapping = page->mapping;
fs/mpage.c:	struct inode *inode = page->mapping->host;
fs/mpage.c:	struct inode *inode = page->mapping->host;
fs/mpage.c:		gfp = readahead_gfp_mask(page->mapping);
fs/mpage.c:		gfp = mapping_gfp_constraint(page->mapping, GFP_KERNEL);
fs/mpage.c:	block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
fs/mpage.c:		prefetchw(&page->flags);
fs/mpage.c:		list_del(&page->lru);
fs/mpage.c:					page->index,
fs/mpage.c:	struct address_space *mapping = page->mapping;
fs/mpage.c:	struct inode *inode = page->mapping->host;
fs/mpage.c:	block_in_file = (sector_t)page->index << (PAGE_SHIFT - blkbits);
fs/mpage.c:	if (page->index >= end_index) {
fs/mpage.c:		if (page->index > end_index || !offset)
fs/nfs/blocklayout/blocklayout.c:	/* Code assumes extents are page-aligned */
fs/nfs/dir.c:	if (invalidate_inode_pages2_range(inode->i_mapping, page->index + 1, -1) < 0) {
fs/nfs/dir.c:	if (!desc->page->mapping)
fs/nfs/file.c:	nfs_fscache_invalidate_page(page, page->mapping->host);
fs/nfs/fscache.c:		struct fscache_cookie *cookie = nfs_i_fscache(page->mapping->host);
fs/nfs/fscache.c:			 cookie, page, NFS_I(page->mapping->host));
fs/nfs/fscache.c:		nfs_inc_fscache_stats(page->mapping->host,
fs/nfs/fscache.c:	nfs_inc_fscache_stats(page->mapping->host,
fs/nfs/fscache.c:		error = nfs_readpage_async(context, page->mapping->host, page);
fs/nfs/fscache.c:		 nfs_i_fscache(inode), page, page->index, page->flags, inode);
fs/nfs/fscache.c:		 nfs_i_fscache(inode), page, page->index, page->flags, sync);
fs/nfs/fscache.c:		 page, page->index, page->flags, ret);
fs/nilfs2/btnode.c:	mapping = page->mapping;
fs/nilfs2/btnode.c:		/* BUG_ON(oldkey != obh->b_page->index); */
fs/nilfs2/btnode.c:		if (unlikely(oldkey != opage->index))
fs/nilfs2/btnode.c:		 * Note: page->index will not change to newkey until
fs/nilfs2/btnode.c:		if (unlikely(oldkey != opage->index))
fs/nilfs2/btnode.c:		opage->index = obh->b_blocknr = newkey;
fs/nilfs2/btree.c:	inode = bh->b_page->mapping->host;
fs/nilfs2/dir.c:	struct inode *dir = page->mapping->host;
fs/nilfs2/dir.c:	if ((dir->i_size >> PAGE_SHIFT) == page->index) {
fs/nilfs2/dir.c:		    dir->i_ino, error, (page->index << PAGE_SHIFT) + offs,
fs/nilfs2/dir.c:		    dir->i_ino, (page->index << PAGE_SHIFT) + offs,
fs/nilfs2/dir.c:	struct address_space *mapping = page->mapping;
fs/nilfs2/dir.c:	nilfs_commit_chunk(page, page->mapping, from, to);
fs/nilfs2/dir.c:	struct address_space *mapping = page->mapping;
fs/nilfs2/file.c:	if (page->mapping != inode->i_mapping ||
fs/nilfs2/gcinode.c:		struct inode *inode = bh->b_page->mapping->host;
fs/nilfs2/inode.c:	struct inode *inode = page->mapping->host;
fs/nilfs2/inode.c:	struct inode *inode = page->mapping->host;
fs/nilfs2/mdt.c:	struct inode *inode = page->mapping->host;
fs/nilfs2/mdt.c:	page = grab_cache_page(&shadow->frozen_data, bh->b_page->index);
fs/nilfs2/mdt.c:	page = find_lock_page(&shadow->frozen_data, bh->b_page->index);
fs/nilfs2/page.c:#include <linux/page-flags.h>
fs/nilfs2/page.c:	m = page->mapping;
fs/nilfs2/page.c:	       (unsigned long long)page->index, page->flags, m, ino);
fs/nilfs2/page.c:		dpage = grab_cache_page(dmap, page->index);
fs/nilfs2/page.c:		pgoff_t offset = page->index;
fs/nilfs2/page.c:				page->mapping = NULL;
fs/nilfs2/page.c:				page->mapping = dmap;
fs/nilfs2/page.c:	struct inode *inode = page->mapping->host;
fs/nilfs2/page.c:	struct address_space *mapping = page->mapping;
fs/nilfs2/page.c:		if (test_bit(PG_dirty, &page->flags)) {
fs/nilfs2/page.c:	index = page->index + 1;
fs/nilfs2/segment.c:			inode = bh->b_page->mapping->host;
fs/ntfs/aops.c:	vi = page->mapping->host;
fs/ntfs/aops.c:		file_ofs = ((s64)page->index << PAGE_SHIFT) +
fs/ntfs/aops.c: * Fill the page @page of the address space belonging to the @page->host inode.
fs/ntfs/aops.c:	vi = page->mapping->host;
fs/ntfs/aops.c:	iblock = (s64)page->index << (PAGE_SHIFT - blocksize_bits);
fs/ntfs/aops.c:	vi = page->mapping->host;
fs/ntfs/aops.c:	if (unlikely(page->index >= (i_size + PAGE_SIZE - 1) >>
fs/ntfs/aops.c:	if (unlikely(page->index > 0)) {
fs/ntfs/aops.c:	vi = page->mapping->host;
fs/ntfs/aops.c:			"0x%lx.", ni->mft_no, ni->type, page->index);
fs/ntfs/aops.c:	block = (s64)page->index << (PAGE_SHIFT - blocksize_bits);
fs/ntfs/aops.c:	struct inode *vi = page->mapping->host;
fs/ntfs/aops.c:			"0x%lx.", vi->i_ino, ni->type, page->index);
fs/ntfs/aops.c:	rec_block = block = (sector_t)page->index <<
fs/ntfs/aops.c:			mft_no = (((s64)page->index << PAGE_SHIFT) + ofs)
fs/ntfs/aops.c:					ni->type, page->index, ofs);
fs/ntfs/aops.c:					page->index, bh_offset(tbh));
fs/ntfs/aops.c:			mft_no = (((s64)page->index << PAGE_SHIFT) + ofs)
fs/ntfs/aops.c:				"record 0x%lx.", page->index <<
fs/ntfs/aops.c:	struct inode *vi = page->mapping->host;
fs/ntfs/aops.c:	if (unlikely(page->index >= (i_size + PAGE_SIZE - 1) >>
fs/ntfs/aops.c:		if (page->index >= (i_size >> PAGE_SHIFT)) {
fs/ntfs/aops.c:	if (unlikely(page->index > 0)) {
fs/ntfs/aops.c:		ntfs_error(vi->i_sb, "BUG()! page->index (0x%lx) > 0.  "
fs/ntfs/aops.c:				"Aborting write.", page->index);
fs/ntfs/aops.c:	struct address_space *mapping = page->mapping;
fs/ntfs/compress.c:	if (((s64)page->index << PAGE_SHIFT) >= initialized_size) {
fs/ntfs/compress.c:	if ((page->index >= (initialized_size >> PAGE_SHIFT)) &&
fs/ntfs/compress.c:	struct address_space *mapping = page->mapping;
fs/ntfs/compress.c:	unsigned long offset, index = page->index;
fs/ntfs/compress.c:	ntfs_debug("Entering, page->index = 0x%lx, cb_size = 0x%x, nr_pages = "
fs/ntfs/compress.c:			cb_max_page--;
fs/ntfs/compress.c:			cb_max_page--;
fs/ntfs/compress.c:					"0x%lx.", ni->mft_no, page->index);
fs/ntfs/file.c:	bh_pos = (s64)page->index << PAGE_SHIFT;
fs/ntfs/file.c:			bh_pos = ((s64)page->index << PAGE_SHIFT) +
fs/ntfs/file.c:					((s64)page->index << PAGE_SHIFT) +
fs/ntfs/file.c:		bh_pos = (s64)page->index << PAGE_SHIFT;
fs/ntfs/file.c:	vi = page->mapping->host;
fs/ntfs/file.c:			vi->i_ino, ni->type, page->index, nr_pages,
fs/ntfs/logfile.c:		if (!page || page->index != idx) {
fs/ocfs2/alloc.c:		if ((end >> PAGE_SHIFT) == page->index)
fs/ocfs2/alloc.c:		start = (page->index + 1) << PAGE_SHIFT;
fs/ocfs2/aops.c:	struct inode *inode = page->mapping->host;
fs/ocfs2/aops.c:	loff_t start = (loff_t)page->index << PAGE_SHIFT;
fs/ocfs2/aops.c:			     (page ? page->index : 0));
fs/ocfs2/aops.c:	 * block_read_full_page->get_block freaks out if it is asked to read
fs/ocfs2/aops.c:		(unsigned long long)OCFS2_I(page->mapping->host)->ip_blkno,
fs/ocfs2/aops.c:		page->index);
fs/ocfs2/aops.c:			if (mmap_page->mapping != mapping) {
fs/ocfs2/aops.c:				WARN_ON(mmap_page->mapping);
fs/ocfs2/mmap.c:	if ((page->mapping != inode->i_mapping) ||
fs/ocfs2/mmap.c:	if (page->index == last_index)
fs/ocfs2/symlink.c:	struct inode *inode = page->mapping->host;
fs/orangefs/inode.c:	struct inode *inode = page->mapping->host;
fs/orangefs/inode.c:	if (page->index < max_block) {
fs/orangefs/inode.c:		loff_t blockptr_offset = (((loff_t) page->index) << blockbits);
fs/orangefs/inode.c:		list_del(&page->lru);
fs/orangefs/inode.c:				       page->index,
fs/proc/page.c:#include <linux/kernel-page-flags.h>
fs/proc/page.c:	k = page->flags;
fs/proc/page.c:	 * Note that page->_mapcount is overloaded in SLOB/SLUB/SLQB, so the
fs/proc/page.c:	 * Caveats on high order pages: page->_refcount will only be set
fs/pstore/ram_core.c:			/* We must vunmap() at page-granularity. */
fs/reiserfs/file.c:				    (new || page->index >= i_size_index)) {
fs/reiserfs/inode.c:	if (!hole_page || index != hole_page->index) {
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	if (page->index >= end_index) {
fs/reiserfs/inode.c:		if (page->index >= end_index + 1 || !last_offset) {
fs/reiserfs/inode.c:	block = page->index << (PAGE_SHIFT - s->s_blocksize_bits);
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	loff_t pos = ((loff_t) page->index << PAGE_SHIFT) + to;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/inode.c:	struct inode *inode = page->mapping->host;
fs/reiserfs/journal.c: * If page->mapping was null, we failed to truncate this page for
fs/reiserfs/journal.c: * lost pages before doing the final put_bh.  If page->mapping was
fs/reiserfs/journal.c:	if (!page->mapping && trylock_page(page)) {
fs/reiserfs/journal.c:		if (!page->mapping)
fs/reiserfs/journal.c:		if (buffer_dirty(bh) && unlikely(bh->b_page->mapping == NULL)) {
fs/reiserfs/tail_conversion.c:		struct inode *inode = bh->b_page->mapping->host;
fs/romfs/super.c: *	Oct 1999	2.3.24		page->owner hack obsoleted
fs/romfs/super.c: *	Nov 1999	2.3.27		2.3.25+ page->offset => index change
fs/romfs/super.c:	struct inode *inode = page->mapping->host;
fs/splice.c:		if (!page->mapping) {
fs/squashfs/cache.c: * are decompressed and cached in the page-cache in the normal way.  The
fs/squashfs/file.c:	struct inode *inode = page->mapping->host;
fs/squashfs/file.c:	int start_index = page->index & ~mask, end_index = start_index | mask;
fs/squashfs/file.c:		push_page = (i == page->index) ? page :
fs/squashfs/file.c:			grab_cache_page_nowait(page->mapping, i);
fs/squashfs/file.c:		if (i != page->index)
fs/squashfs/file.c:	struct inode *inode = page->mapping->host;
fs/squashfs/file.c:	struct inode *inode = page->mapping->host;
fs/squashfs/file.c:	int index = page->index >> (msblk->block_log - PAGE_SHIFT);
fs/squashfs/file.c:				page->index, squashfs_i(inode)->start);
fs/squashfs/file.c:	if (page->index >= ((i_size_read(inode) + PAGE_SIZE - 1) >>
fs/squashfs/file_cache.c:	struct inode *i = page->mapping->host;
fs/squashfs/file_direct.c:	struct inode *inode = target_page->mapping->host;
fs/squashfs/file_direct.c:	int start_index = target_page->index & ~mask;
fs/squashfs/file_direct.c:		page[i] = (n == target_page->index) ? target_page :
fs/squashfs/file_direct.c:			grab_cache_page_nowait(target_page->mapping, n);
fs/squashfs/file_direct.c:	struct inode *i = target_page->mapping->host;
fs/squashfs/symlink.c:	struct inode *inode = page->mapping->host;
fs/squashfs/symlink.c:	int index = page->index << PAGE_SHIFT;
fs/squashfs/symlink.c:			"%llx, offset %x\n", page->index, block, offset);
fs/sysv/dir.c:	struct address_space *mapping = page->mapping;
fs/sysv/dir.c:	struct inode *inode = page->mapping->host;
fs/sysv/dir.c:	struct inode *dir = page->mapping->host;
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:		inode->i_ino, page->index, i_size, page->flags);
fs/ubifs/file.c:	block = page->index << UBIFS_BLOCKS_PER_PAGE_SHIFT;
fs/ubifs/file.c:			  page->index, inode->i_ino, err);
fs/ubifs/file.c:		inode->i_ino, pos, page->index, len, copied, inode->i_size);
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:		inode->i_ino, page->index, i_size, page->flags);
fs/ubifs/file.c:	if (!i_size || page->index > end_index) {
fs/ubifs/file.c:	page_block = page->index << UBIFS_BLOCKS_PER_PAGE_SHIFT;
fs/ubifs/file.c:	if (end_index == page->index) {
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:	pgoff_t index = page->index, last_page_read = ui->last_page_read;
fs/ubifs/file.c:		      page->index << UBIFS_BLOCKS_PER_PAGE_SHIFT);
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:	ubifs_assert(c, page->index <= ui->synced_i_size >> PAGE_SHIFT);
fs/ubifs/file.c:	block = page->index << UBIFS_BLOCKS_PER_PAGE_SHIFT;
fs/ubifs/file.c:			  page->index, inode->i_ino, err);
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:		inode->i_ino, page->index, page->flags);
fs/ubifs/file.c:	if (page->index > end_index || (page->index == end_index && !len)) {
fs/ubifs/file.c:	if (page->index < end_index) {
fs/ubifs/file.c:		if (page->index >= synced_i_size >> PAGE_SHIFT) {
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:	struct inode *inode = page->mapping->host;
fs/ubifs/file.c:	dbg_gen("ino %lu, pg %lu, i_size %lld",	inode->i_ino, page->index,
fs/ubifs/file.c:	if (unlikely(page->mapping != inode->i_mapping ||
fs/udf/file.c:	struct inode *inode = page->mapping->host;
fs/udf/file.c:	struct inode *inode = page->mapping->host;
fs/udf/file.c:	struct inode *inode = page->mapping->host;
fs/udf/symlink.c:	struct inode *inode = page->mapping->host;
fs/ufs/balloc.c:	cur_index = locked_page->index;
fs/ufs/dir.c:	struct address_space *mapping = page->mapping;
fs/ufs/dir.c:	struct inode *dir = page->mapping->host;
fs/ufs/dir.c:	if ((dir->i_size >> PAGE_SHIFT) == page->index) {
fs/ufs/dir.c:		   dir->i_ino, error, (page->index<<PAGE_SHIFT)+offs,
fs/ufs/dir.c:		   dir->i_ino, (page->index<<PAGE_SHIFT)+offs);
fs/ufs/util.c:		if (unlikely(page->mapping == NULL)) {
fs/userfaultfd.c:	if (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))
fs/xfs/libxfs/xfs_sb.c:	 * Until this is fixed only page-sized or smaller data blocks work.
fs/xfs/xfs_aops.c:	trace_xfs_invalidatepage(page->mapping->host, page, offset, length);
fs/xfs/xfs_aops.c:	struct inode		*inode = page->mapping->host;
fs/xfs/xfs_aops.c:	mapping_set_error(page->mapping, error);
fs/xfs/xfs_aops.c:	struct inode		*inode = page->mapping->host;
fs/xfs/xfs_aops.c:	if (page->index < end_index)
fs/xfs/xfs_aops.c:		end_offset = (xfs_off_t)(page->index + 1) << PAGE_SHIFT;
fs/xfs/xfs_aops.c:		 * via "if (page->index >= end_index + 1)" as "end_index + 1"
fs/xfs/xfs_aops.c:		if (page->index > end_index ||
fs/xfs/xfs_aops.c:		    (page->index == end_index && offset_into_page == 0))
fs/xfs/xfs_aops.c:	trace_xfs_releasepage(page->mapping->host, page, 0, 0);
fs/xfs/xfs_aops.c:	trace_xfs_vm_readpage(page->mapping->host, 1);
fs/xfs/xfs_file.c:	return ___wait_var_event(&page->_refcount,
fs/xfs/xfs_file.c:			atomic_read(&page->_refcount) == 1, TASK_INTERRUPTIBLE,
fs/xfs/xfs_linux.h: * Currently the system supports page-sized i/o.
fs/xfs/xfs_super.c:	 *      page->index << (PAGE_SHIFT - bbits)
include/asm-generic/tlb.h: * gup_fast() and other software pagetable walkers do a lockless page-table
include/asm-generic/vmlinux.lds.h: * on a page boundary and be a page-size multiple in length.
include/drm/drm_vma_manager.h: * @start: Start address (page-based, not byte-based)
include/drm/drm_vma_manager.h: * @pages: Size of object (page-based)
include/drm/drm_vma_manager.h: * drm_vma_node_start() - Return start address for page-based addressing
include/drm/drm_vma_manager.h: * this can only be used for page-based addressing. If you need a proper offset
include/drm/drm_vma_manager.h: * Start address of @node for page-based addressing. 0 if the node does not
include/drm/drm_vma_manager.h: * drm_vma_node_size() - Return size (page-based)
include/linux/balloon_compaction.h: *	            ... page->mapping updates here ...
include/linux/balloon_compaction.h:#include <linux/page-flags.h>
include/linux/balloon_compaction.h: *			 the page->private assignment accordingly.
include/linux/balloon_compaction.h:	list_add(&page->lru, &balloon->pages);
include/linux/balloon_compaction.h: *			 the page->private assignement accordingly.
include/linux/balloon_compaction.h:		list_del(&page->lru);
include/linux/balloon_compaction.h:	list_add(&page->lru, &balloon->pages);
include/linux/balloon_compaction.h:	list_del(&page->lru);
include/linux/balloon_compaction.h:	list_add(&page->lru, pages);
include/linux/balloon_compaction.h:	list_del(&page->lru);
include/linux/buffer_head.h:/* If we *know* page->private refers to buffer_heads */
include/linux/cleancache.h:	return cleancache_fs_enabled_mapping(page->mapping);
include/linux/cleancache.h:	/* careful... page->mapping is NULL sometimes when this is called */
include/linux/dax.h:	 * logical-page-offset into an absolute physical pfn. Return the
include/linux/dax.h:	if (IS_DAX(page->mapping->host))
include/linux/gfp.h: * compaction (which removes fragmentation) and page-out.
include/linux/gfp.h: * There is only one page-allocator function, and two main namespaces to
include/linux/hmm.h:	page->hmm_data = data;
include/linux/hmm.h:	return page->hmm_data;
include/linux/hugetlb.h:/* Return page->index in PAGE_SIZE units */
include/linux/hugetlb.h:		return page->index;
include/linux/hugetlb.h:	return page->index;
include/linux/kernel-page-flags.h:#include <uapi/linux/kernel-page-flags.h>
include/linux/memcontrol.h:#include <linux/page-flags.h>
include/linux/memcontrol.h:	if (page->mem_cgroup)
include/linux/memcontrol.h:		__mod_memcg_state(page->mem_cgroup, idx, val);
include/linux/memcontrol.h:	if (page->mem_cgroup)
include/linux/memcontrol.h:		mod_memcg_state(page->mem_cgroup, idx, val);
include/linux/memcontrol.h:	if (!page->mem_cgroup) {
include/linux/memcontrol.h:	lruvec = mem_cgroup_lruvec(pgdat, page->mem_cgroup);
include/linux/memcontrol.h:	if (page->mem_cgroup)
include/linux/memcontrol.h:		count_memcg_events(page->mem_cgroup, idx, 1);
include/linux/memory_hotplug.h: * walkers which rely on the fully initialized page->flags and others
include/linux/memory_hotplug.h: * Types for free bootmem stored in page->lru.next. These have to be in
include/linux/mm.h: * FIXME: take this include out, include page-flags.h in
include/linux/mm.h:#include <linux/page-flags.h>
include/linux/mm.h: * - cache mapping   (page->mapping)
include/linux/mm.h: * - private data    (page->private)
include/linux/mm.h: * The atomic page->_mapcount, starts from -1: so that transitions
include/linux/mm.h:	return atomic_read(&page->_mapcount) + 1;
include/linux/mm.h: *   page_count() == 0 means the page is free. page->lru is then used for
include/linux/mm.h: * be used through the normal accessor functions. The top bits of page->flags
include/linux/mm.h: * and page->virtual store page management information, but all other fields
include/linux/mm.h: * A page may belong to an inode's memory mapping. In this case, page->mapping
include/linux/mm.h: * is the pointer to the inode, and page->index is the file offset of the page,
include/linux/mm.h: * case PG_swapcache is set, and page->private is an offset into the swapcache.
include/linux/mm.h:	return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;
include/linux/mm.h:	switch (page->pgmap->type) {
include/linux/mm.h:		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
include/linux/mm.h:		page->pgmap->type == MEMORY_DEVICE_PUBLIC;
include/linux/mm.h:		page->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;
include/linux/mm.h:	 * requires to already have an elevated page->_refcount.
include/linux/mm.h:	return (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;
include/linux/mm.h:	return xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);
include/linux/mm.h:	return page->_last_cpupid;
include/linux/mm.h:	page->_last_cpupid = -1 & LAST_CPUPID_MASK;
include/linux/mm.h:	return (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;
include/linux/mm.h:	page->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;
include/linux/mm.h:	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
include/linux/mm.h:	page->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);
include/linux/mm.h:	page->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;
include/linux/mm.h:	page->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);
include/linux/mm.h:	page->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;
include/linux/mm.h:	return (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;
include/linux/mm.h:	page->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);
include/linux/mm.h:	page->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;
include/linux/mm.h:	page->flags &= ~(NODES_MASK << NODES_PGSHIFT);
include/linux/mm.h:	page->flags |= (node & NODES_MASK) << NODES_PGSHIFT;
include/linux/mm.h:	return page->mem_cgroup;
include/linux/mm.h:	return READ_ONCE(page->mem_cgroup);
include/linux/mm.h:	return page->virtual;
include/linux/mm.h:	page->virtual = address;
include/linux/mm.h:	return page->mapping;
include/linux/mm.h:	return page->index;
include/linux/mm.h:	return page->index == -1UL;
include/linux/mm.h:	page->index = -1UL;
include/linux/mm.h:	page->index = 0;
include/linux/mm.h:	struct address_space *check_mapping;	/* Check page->mapping if set */
include/linux/mm.h:	pgoff_t	first_index;			/* Lowest page->index to unmap */
include/linux/mm.h:	pgoff_t last_index;			/* Highest page->index to unmap */
include/linux/mm.h:	return page->ptl;
include/linux/mm.h:	return &page->ptl;
include/linux/mm.h:	 * prep_new_page() initialize page->private (and therefore page->ptl)
include/linux/mm.h:	 * slab code uses page->slab_cache, which share storage with page->ptl.
include/linux/mm.h:	VM_BUG_ON_PAGE(*(unsigned long *)&page->ptl, page);
include/linux/mm.h:	page->pmd_huge_pte = NULL;
include/linux/mm.h:	VM_BUG_ON_PAGE(page->pmd_huge_pte, page);
include/linux/mm.h:/* mm/page-writeback.c */
include/linux/mm_inline.h:	list_add(&page->lru, &lruvec->lists[lru]);
include/linux/mm_inline.h:	list_add_tail(&page->lru, &lruvec->lists[lru]);
include/linux/mm_inline.h:	list_del(&page->lru);
include/linux/mm_types.h:#include <linux/page-flags-layout.h>
include/linux/mm_types.h: * page->mapping, you must restore it to NULL before freeing the page.
include/linux/mm_types.h:			/* See page-flags.h for PAGE_MAPPING_FLAGS */
include/linux/mm_types.h:			pgtable_t pmd_huge_pte; /* protected by page->ptl */
include/linux/mm_types.h:		 * is used for.  See page-flags.h for a list of page types
include/linux/mm_types.h:	 * containing page->_refcount every time we allocate a fragment.
include/linux/mm_types.h: * space that has a special rule for the page-fault handlers (ie a shared
include/linux/mmu_notifier.h:	 * shared page-tables, it not necessary to implement the
include/linux/mmzone.h:#include <linux/page-flags-layout.h>
include/linux/mmzone.h: *   1. All mem_map arrays are page-aligned.
include/linux/mtd/mtd.h: *  pair-0	page-0		page-4
include/linux/mtd/mtd.h: *  pair-1	page-1		page-5
include/linux/mtd/mtd.h: *  pair-2	page-2		page-8
include/linux/mtd/mtd.h: *  pair-127	page-251	page-255
include/linux/nls.h:	charlen = codepage->uni2char(0, tmp, NLS_MAX_CHARSET_SIZE);
include/linux/page-flags-layout.h: * page->flags layout:
include/linux/page-flags-layout.h: * There are five possibilities for how page->flags get laid out.  The first
include/linux/page-flags-layout.h: * The last is when there is insufficient space in page->flags and a separate
include/linux/page-flags.h: * Macros for manipulating and testing page->flags
include/linux/page-flags.h: * Various page->flags bits:
include/linux/page-flags.h: * specific data (which is normally at page->private). It can be used by
include/linux/page-flags.h:	unsigned long head = READ_ONCE(page->compound_head);
include/linux/page-flags.h:	return READ_ONCE(page->compound_head) & 1;
include/linux/page-flags.h:	return test_bit(PG_head, &page->flags) || PageTail(page);
include/linux/page-flags.h:	return page->flags == PAGE_POISON_PATTERN;
include/linux/page-flags.h:	return PageSwapBacked(page) && test_bit(PG_swapcache, &page->flags);
include/linux/page-flags.h: * page->mapping points to its anon_vma, not to a struct address_space;
include/linux/page-flags.h: * bit; and then page->mapping points, not to an anon_vma, but to a private
include/linux/page-flags.h: * page and then page->mapping points a struct address_space.
include/linux/page-flags.h:	return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) != 0;
include/linux/page-flags.h:	return ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;
include/linux/page-flags.h:	return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) ==
include/linux/page-flags.h:	return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) ==
include/linux/page-flags.h:	 * _after_ we've loaded page->flags to check for PageUptodate.
include/linux/page-flags.h:	__set_bit(PG_uptodate, &page->flags);
include/linux/page-flags.h:	set_bit(PG_uptodate, &page->flags);
include/linux/page-flags.h:	WRITE_ONCE(page->compound_head, (unsigned long)head + 1);
include/linux/page-flags.h:	WRITE_ONCE(page->compound_head, 0);
include/linux/page-flags.h: * MMU notifier, otherwise it may result in page->_mapcount < 0 false
include/linux/page-flags.h:	return PageTransCompound(page) && atomic_read(&page->_mapcount) < 0;
include/linux/page-flags.h:	((page->page_type & (PAGE_TYPE_BASE | flag)) == PAGE_TYPE_BASE)
include/linux/page-flags.h:	return (int)page->page_type < PAGE_MAPCOUNT_RESERVE;
include/linux/page-flags.h:	page->page_type &= ~PG_##lname;					\
include/linux/page-flags.h:	page->page_type |= PG_##lname;					\
include/linux/page-flags.h:	return !!(page->flags & PAGE_FLAGS_PRIVATE);
include/linux/page_idle.h:#include <linux/page-flags.h>
include/linux/page_ref.h:#include <linux/page-flags.h>
include/linux/page_ref.h:	return atomic_read(&page->_refcount);
include/linux/page_ref.h:	atomic_set(&page->_refcount, v);
include/linux/page_ref.h:	atomic_add(nr, &page->_refcount);
include/linux/page_ref.h:	atomic_sub(nr, &page->_refcount);
include/linux/page_ref.h:	atomic_inc(&page->_refcount);
include/linux/page_ref.h:	atomic_dec(&page->_refcount);
include/linux/page_ref.h:	int ret = atomic_sub_and_test(nr, &page->_refcount);
include/linux/page_ref.h:	int ret = atomic_inc_return(&page->_refcount);
include/linux/page_ref.h:	int ret = atomic_dec_and_test(&page->_refcount);
include/linux/page_ref.h:	int ret = atomic_dec_return(&page->_refcount);
include/linux/page_ref.h:	int ret = atomic_add_unless(&page->_refcount, nr, u);
include/linux/page_ref.h:	int ret = likely(atomic_cmpxchg(&page->_refcount, count, 0) == count);
include/linux/page_ref.h:	atomic_set_release(&page->_refcount, count);
include/linux/pagemap.h:		return page->index;
include/linux/pagemap.h:		return page->index << compound_order(page);
include/linux/pagemap.h:	return ((loff_t)page->index) << PAGE_SHIFT;
include/linux/pagemap.h:	return (likely(!test_and_set_bit_lock(PG_locked, &page->flags)));
include/linux/pagemap.h: * ie with increased "page->count" so that the page won't
include/linux/pfn.h: * pfn_t: encapsulates a page-frame number that is optionally backed
include/linux/rmap.h:	atomic_inc(compound ? compound_mapcount_ptr(page) : &page->_mapcount);
include/linux/scatterlist.h: * Iterates over sg entries page-by-page.  On each successful iteration,
include/linux/scatterlist.h: * Iterates over sg entries mapping page-by-page.  On each successful
include/linux/slab_def.h:	void *object = x - (x - page->s_mem) % cache->size;
include/linux/slab_def.h:	void *last_object = page->s_mem + (cache->num - 1) * cache->size;
include/linux/slab_def.h:	u32 offset = (obj - page->s_mem);
include/linux/slub_def.h:		(page->objects - 1) * cache->size;
include/linux/sunrpc/svc.h: * if the request is not page-aligned.  So add another '1'.
include/linux/swap.h:#include <linux/page-flags.h>
include/linux/swap.h:#define SWAP_FLAG_DISCARD_PAGES 0x40000 /* discard page-clusters after use */
include/linux/swap.h:	SWP_PAGE_DISCARD = (1 << 10),	/* freed swap page-cluster discards */
include/linux/writeback.h: * mm/page-writeback.c
include/net/page_pool.h: * If page_pool handles DMA mapping (use page->private), then API user
include/trace/events/afs.h: * We use page->private to hold the amount of the page that we've written to,
include/trace/events/btrfs.h:		__entry->index		= page->index;
include/trace/events/btrfs.h:	TP_fast_assign_btrfs(btrfs_sb(page->mapping->host->i_sb),
include/trace/events/btrfs.h:		__entry->ino	= btrfs_ino(BTRFS_I(page->mapping->host));
include/trace/events/btrfs.h:		__entry->index	= page->index;
include/trace/events/btrfs.h:			 BTRFS_I(page->mapping->host)->root->root_key.objectid;
include/trace/events/ext4.h:		__entry->dev	= page->mapping->host->i_sb->s_dev;
include/trace/events/ext4.h:		__entry->ino	= page->mapping->host->i_ino;
include/trace/events/ext4.h:		__entry->index	= page->index;
include/trace/events/ext4.h:		__entry->dev	= page->mapping->host->i_sb->s_dev;
include/trace/events/ext4.h:		__entry->ino	= page->mapping->host->i_ino;
include/trace/events/ext4.h:		__entry->index	= page->index;
include/trace/events/f2fs.h:		__entry->dev		= page->mapping->host->i_sb->s_dev;
include/trace/events/f2fs.h:		__entry->ino		= page->mapping->host->i_ino;
include/trace/events/f2fs.h:		__entry->index		= page->index;
include/trace/events/f2fs.h:	TP_CONDITION(page->mapping)
include/trace/events/f2fs.h:	TP_CONDITION(page->mapping)
include/trace/events/f2fs.h:		__entry->dev	= page->mapping->host->i_sb->s_dev;
include/trace/events/f2fs.h:		__entry->ino	= page->mapping->host->i_ino;
include/trace/events/f2fs.h:		__entry->dir	= S_ISDIR(page->mapping->host->i_mode);
include/trace/events/f2fs.h:		__entry->index	= page->index;
include/trace/events/f2fs.h:		__entry->start	= page->index;
include/trace/events/filemap.h:		__entry->i_ino = page->mapping->host->i_ino;
include/trace/events/filemap.h:		__entry->index = page->index;
include/trace/events/filemap.h:		if (page->mapping->host->i_sb)
include/trace/events/filemap.h:			__entry->s_dev = page->mapping->host->i_sb->s_dev;
include/trace/events/filemap.h:			__entry->s_dev = page->mapping->host->i_rdev;
include/trace/events/fscache.h:		    __entry->page		= page->index;
include/trace/events/fscache.h:		    __entry->page		= page ? page->index : 0;
include/trace/events/fscache.h:		    __entry->page		= page->index;
include/trace/events/page_ref.h:		__entry->flags = page->flags;
include/trace/events/page_ref.h:		__entry->mapping = page->mapping;
include/trace/events/page_ref.h:		__entry->flags = page->flags;
include/trace/events/page_ref.h:		__entry->mapping = page->mapping;
include/trace/events/pagemap.h:	/* Flag format is based on page-types.c formatting for pagemap */
include/trace/events/pagemap.h:	/* Flag format is based on page-types.c formatting for pagemap */
include/trace/events/writeback.h:		__entry->index = page->index;
include/uapi/drm/drm_mode.h:/* page-flip flags are valid, plus: */
include/uapi/drm/exynos_drm.h: *	- this size value would be page-aligned internally.
include/uapi/drm/i915_drm.h:	 * The (page-aligned) allocated size for the object will be returned.
include/uapi/drm/i915_drm.h:	 * The value will be page-aligned.
include/uapi/linux/fpga-dfl.h: * This interface only accepts page-size aligned user memory for dma mapping.
include/uapi/linux/userfaultfd.h:	 * UFFD_FEATURE_SIGBUS feature means no page-fault
include/xen/grant_table.h:#include <linux/page-flags.h>
include/xen/grant_table.h:	return (struct xen_page_foreign *)page->private;
include/xen/grant_table.h:	return (struct xen_page_foreign *)&page->private;
include/xen/interface/grant_table.h: * page-ownership transfers.
include/xen/interface/io/blkif.h: * /local/domain/1/device/vbd/0/ring-page-order = "1"
include/xen/interface/io/kbdif.h: * page-gref
include/xen/interface/io/kbdif.h: * page-ref
include/xen/interface/io/kbdif.h:#define XENKBD_FIELD_RING_GREF		"page-gref"
include/xen/interface/io/kbdif.h:#define XENKBD_FIELD_RING_REF		"page-ref"
include/xen/interface/xen.h: * ptr[:2]  -- Machine address of the page-table entry to modify.
include/xen/interface/xen.h: * mfn: Machine frame number of new page-table base to install in MMU.
include/xen/interface/xen.h: * mfn: Machine frame number of new page-table base to install in MMU
include/xen/interface/xen.h: * linear_addr: Linear address of LDT base (NB. must be page-aligned).
include/xen/interface/xen.h: * DOMID_IO is used to restrict page-table updates to mapping I/O memory.
include/xen/interface/xen.h:	unsigned long mfn_list;     /* VIRTUAL address of page-frame list.    */
init/do_mounts.c:		for (p = page-1; p; p = next) {
init/main.c:	 * Kernel mappings are now finalized - update the userspace page-table
kernel/bounds.c:#include <linux/page-flags.h>
kernel/bpf/cpumap.c:	 * (e.g. ixgbe) recycle tricks based on page-refcnt.
kernel/bpf/cpumap.c:	 * with another CPU on page-refcnt and remaining driver code).
kernel/events/core.c:	vmf->page->mapping = vmf->vma->vm_file->f_mapping;
kernel/events/core.c:	vmf->page->index   = vmf->pgoff;
kernel/events/core.c:		aux_offset = READ_ONCE(rb->user_page->aux_offset);
kernel/events/core.c:		aux_size = READ_ONCE(rb->user_page->aux_size);
kernel/events/ring_buffer.c:	rb->user_page->data_head = head;
kernel/events/ring_buffer.c:		tail = READ_ONCE(rb->user_page->data_tail);
kernel/events/ring_buffer.c:		aux_tail = READ_ONCE(rb->user_page->aux_tail);
kernel/events/ring_buffer.c:	rb->user_page->aux_head = rb->aux_head;
kernel/events/ring_buffer.c:	rb->user_page->aux_head = rb->aux_head;
kernel/events/ring_buffer.c:	page->mapping = NULL;
kernel/events/ring_buffer.c:	page->mapping = NULL;
kernel/events/ring_buffer.c:	page->mapping = NULL;
kernel/events/uprobes.c:	 * and in page-cache. If ->readpage == NULL it must be shmem_mapping(),
kernel/fork.c:			 * If memcg_kmem_charge() fails, page->mem_cgroup
kernel/futex.c: * For shared mappings, it's (page->index, file_inode(vma->vm_file),
kernel/futex.c:	mapping = READ_ONCE(page->mapping);
kernel/futex.c:	 * If page->mapping is NULL, then it cannot be a PageAnon
kernel/futex.c:	 * an unlikely race, but we do need to retry for page->mapping.
kernel/futex.c:		shmem_swizzled = PageSwapCache(page) || page->mapping;
kernel/futex.c:		 * the page->mapping must be traversed. Ordinarily this should
kernel/futex.c:		if (READ_ONCE(page->mapping) != mapping) {
kernel/kexec_core.c:		list_del(&page->lru);
kernel/kexec_core.c:			list_del(&page->lru);
kernel/kexec_core.c:			list_add(&page->lru, &image->unusable_pages);
kernel/kexec_core.c:		list_add(&page->lru, &image->dest_pages);
kernel/locking/qspinlock_paravirt.h:	 * Allocate space from bootmem which should be page-size aligned
kernel/memremap.c:	devmem = container_of(page->pgmap, typeof(*devmem), pagemap);
kernel/memremap.c:		page->pgmap->page_free(page, page->pgmap->data);
kernel/module.c: * These values are always page-aligned (as is base)
kernel/sysctl.c:		.procname	= "page-cluster", 
kernel/sysctl_binary.c:	{ CTL_INT,	VM_PAGE_CLUSTER,		"page-cluster" },
kernel/trace/ring_buffer.c:	local_set(&bpage->commit, 0);
kernel/trace/ring_buffer.c:	return (local_read(&bpage->commit) & ~RB_MISSED_FLAGS)
kernel/trace/ring_buffer.c:	free_page((unsigned long)bpage->page);
kernel/trace/ring_buffer.c:	if ((val & ~RB_FLAG_MASK) != (unsigned long)&page->list)
kernel/trace/ring_buffer.c:	struct list_head *list = page->list.prev;
kernel/trace/ring_buffer.c:	return rb_list_head(list->next) != &page->list;
kernel/trace/ring_buffer.c:			if (rb_is_head_page(cpu_buffer, page, page->list.prev)) {
kernel/trace/ring_buffer.c:	old_write = local_add_return(RB_WRITE_INTCNT, &next_page->write);
kernel/trace/ring_buffer.c:	old_entries = local_add_return(RB_WRITE_INTCNT, &next_page->entries);
kernel/trace/ring_buffer.c:		(void)local_cmpxchg(&next_page->write, old_write, val);
kernel/trace/ring_buffer.c:		(void)local_cmpxchg(&next_page->entries, old_entries, eval);
kernel/trace/ring_buffer.c:		local_set(&next_page->page->commit, 0);
kernel/trace/ring_buffer.c:			       bpage->list.next->prev != &bpage->list))
kernel/trace/ring_buffer.c:			       bpage->list.prev->next != &bpage->list))
kernel/trace/ring_buffer.c:		if (rb_check_list(cpu_buffer, &bpage->list))
kernel/trace/ring_buffer.c:		list_add(&bpage->list, pages);
kernel/trace/ring_buffer.c:		bpage->page = page_address(page);
kernel/trace/ring_buffer.c:		rb_init_page(bpage->page);
kernel/trace/ring_buffer.c:		list_del_init(&bpage->list);
kernel/trace/ring_buffer.c:	bpage->page = page_address(page);
kernel/trace/ring_buffer.c:	rb_init_page(bpage->page);
kernel/trace/ring_buffer.c:	INIT_LIST_HEAD(&cpu_buffer->reader_page->list);
kernel/trace/ring_buffer.c:			list_del_init(&bpage->list);
kernel/trace/ring_buffer.c:	return local_read(&bpage->entries) & RB_WRITE_MASK;
kernel/trace/ring_buffer.c:	return local_read(&bpage->write) & RB_WRITE_MASK;
kernel/trace/ring_buffer.c:	tail_page = &cpu_buffer->tail_page->list;
kernel/trace/ring_buffer.c:		tail_page = rb_list_head(tail_page->next);
kernel/trace/ring_buffer.c:	tail_page->next = (struct list_head *)((unsigned long)next_page |
kernel/trace/ring_buffer.c:	next_page->prev = tail_page;
kernel/trace/ring_buffer.c:	 * 2. We cmpxchg the prev_page->next to point from head page to the
kernel/trace/ring_buffer.c:		prev_page = head_page->prev;
kernel/trace/ring_buffer.c:		last_page->next = head_page_with_bit;
kernel/trace/ring_buffer.c:		first_page->prev = prev_page;
kernel/trace/ring_buffer.c:		r = cmpxchg(&prev_page->next, head_page_with_bit, first_page);
kernel/trace/ring_buffer.c:			head_page->prev = last_page;
kernel/trace/ring_buffer.c:			list_del_init(&bpage->list);
kernel/trace/ring_buffer.c:			list_del_init(&bpage->list);
kernel/trace/ring_buffer.c:	return bpage->page->data + index;
kernel/trace/ring_buffer.c:			       cpu_buffer->reader_page->read);
kernel/trace/ring_buffer.c:	return local_read(&bpage->page->commit);
kernel/trace/ring_buffer.c:	iter->read_stamp = iter->head_page->page->time_stamp;
kernel/trace/ring_buffer.c:			tail_page->real_end = 0;
kernel/trace/ring_buffer.c:		local_sub(length, &tail_page->write);
kernel/trace/ring_buffer.c:	tail_page->real_end = tail;
kernel/trace/ring_buffer.c:		local_sub(length, &tail_page->write);
kernel/trace/ring_buffer.c:	local_sub(length, &tail_page->write);
kernel/trace/ring_buffer.c:	if (rb_is_head_page(cpu_buffer, next_page, &tail_page->list)) {
kernel/trace/ring_buffer.c:	if (bpage->page == (void *)addr && rb_page_write(bpage) == old_index) {
kernel/trace/ring_buffer.c:			local_read(&bpage->write) & ~RB_WRITE_MASK;
kernel/trace/ring_buffer.c:		index = local_cmpxchg(&bpage->write, old_index, new_index);
kernel/trace/ring_buffer.c:		local_set(&cpu_buffer->commit_page->page->commit,
kernel/trace/ring_buffer.c:				cpu_buffer->commit_page->page->time_stamp;
kernel/trace/ring_buffer.c:		local_set(&cpu_buffer->commit_page->page->commit,
kernel/trace/ring_buffer.c:			   local_read(&cpu_buffer->commit_page->page->commit) &
kernel/trace/ring_buffer.c:	return cpu_buffer->commit_page->page == (void *)addr &&
kernel/trace/ring_buffer.c:				cpu_buffer->commit_page->page->time_stamp;
kernel/trace/ring_buffer.c:	write = local_add_return(info->length, &tail_page->write);
kernel/trace/ring_buffer.c:	local_inc(&tail_page->entries);
kernel/trace/ring_buffer.c:		tail_page->page->time_stamp = info->ts;
kernel/trace/ring_buffer.c:	if (likely(bpage->page == (void *)addr)) {
kernel/trace/ring_buffer.c:		local_dec(&bpage->entries);
kernel/trace/ring_buffer.c:		if (bpage->page == (void *)addr) {
kernel/trace/ring_buffer.c:			local_dec(&bpage->entries);
kernel/trace/ring_buffer.c:		ret = bpage->page->time_stamp;
kernel/trace/ring_buffer.c:	iter->head = cpu_buffer->reader_page->read;
kernel/trace/ring_buffer.c:		iter->read_stamp = iter->head_page->page->time_stamp;
kernel/trace/ring_buffer.c:		 head_page->read == commit &&
kernel/trace/ring_buffer.c:	if (cpu_buffer->reader_page->read < rb_page_size(reader))
kernel/trace/ring_buffer.c:		       cpu_buffer->reader_page->read > rb_page_size(reader)))
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->write, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->entries, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->page->commit, 0);
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->real_end = 0;
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->list.next = rb_list_head(reader->list.next);
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->list.prev = reader->list.prev;
kernel/trace/ring_buffer.c:	rb_set_list_to_head(cpu_buffer, &cpu_buffer->reader_page->list);
kernel/trace/ring_buffer.c:	rb_list_head(reader->list.next)->prev = &cpu_buffer->reader_page->list;
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->read = 0;
kernel/trace/ring_buffer.c:		cpu_buffer->read_stamp = reader->page->time_stamp;
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->read += length;
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->head_page->write, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->head_page->entries, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->head_page->page->commit, 0);
kernel/trace/ring_buffer.c:	cpu_buffer->head_page->read = 0;
kernel/trace/ring_buffer.c:	INIT_LIST_HEAD(&cpu_buffer->reader_page->list);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->write, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->entries, 0);
kernel/trace/ring_buffer.c:	local_set(&cpu_buffer->reader_page->page->commit, 0);
kernel/trace/ring_buffer.c:	cpu_buffer->reader_page->read = 0;
kernel/trace/ring_buffer.c:		struct buffer_data_page *rpage = cpu_buffer->reader_page->page;
kernel/trace/ring_buffer.c:			memcpy(bpage->data + pos, rpage->data + rpos, size);
kernel/trace/ring_buffer.c:		local_set(&bpage->commit, pos);
kernel/trace/ring_buffer.c:		bpage->time_stamp = save_timestamp;
kernel/trace/ring_buffer.c:			local_set(&bpage->commit, reader->real_end);
kernel/trace/ring_buffer.c:	commit = local_read(&bpage->commit);
kernel/trace/ring_buffer.c:			memcpy(&bpage->data[commit], &missed_events,
kernel/trace/ring_buffer.c:			local_add(RB_MISSED_STORED, &bpage->commit);
kernel/trace/ring_buffer.c:		local_add(RB_MISSED_EVENTS, &bpage->commit);
kernel/trace/ring_buffer.c:		memset(&bpage->data[commit], 0, BUF_PAGE_SIZE - commit);
kernel/trace/ring_buffer_benchmark.c:		commit = local_read(&rpage->commit) & 0xfffff;
kernel/trace/ring_buffer_benchmark.c:			event = (void *)&rpage->data[i];
kernel/trace/trace.c:	/* Seq buffer is page-sized, exactly what we need. */
lib/Kconfig.debug:	bool "Debug page-flags operations"
lib/bitmap.c: * It is assumed that @buf is a pointer into a PAGE_SIZE, page-aligned
mm/Kconfig:	  Cleancache can be thought of as a page-granularity victim cache
mm/Makefile:			   maccess.o page_alloc.o page-writeback.o \
mm/balloon_compaction.c:	list_del(&page->lru);
mm/balloon_compaction.c:	list_add(&page->lru, &b_dev_info->pages);
mm/cleancache.c:	pool_id = page->mapping->host->i_sb->cleancache_poolid;
mm/cleancache.c:	if (cleancache_get_key(page->mapping->host, &key) < 0)
mm/cleancache.c:	ret = cleancache_ops->get_page(pool_id, key, page->index, page);
mm/cleancache.c:	pool_id = page->mapping->host->i_sb->cleancache_poolid;
mm/cleancache.c:		cleancache_get_key(page->mapping->host, &key) >= 0) {
mm/cleancache.c:		cleancache_ops->put_page(pool_id, key, page->index, page);
mm/cleancache.c:	/* careful... page->mapping is NULL sometimes when this is called */
mm/cleancache.c:					key, page->index);
mm/compaction.c:#include <linux/page-isolation.h>
mm/compaction.c:		list_del(&page->lru);
mm/compaction.c:		list_del(&page->lru);
mm/compaction.c:			list_add(&page->lru, &tmp_list);
mm/compaction.c:	page->mapping = (void *)((unsigned long)mapping | PAGE_MAPPING_MOVABLE);
mm/compaction.c:	page->mapping = (void *)((unsigned long)page->mapping &
mm/compaction.c:		list_add_tail(&page->lru, freelist);
mm/compaction.c:		list_add(&page->lru, &cc->migratepages);
mm/compaction.c:	list_del(&freepage->lru);
mm/compaction.c:	list_add(&page->lru, &cc->freepages);
mm/debug.c:	 * page->_mapcount space in struct page is used by sl[aou]b pages to
mm/debug.c:		  page->mapping, page_to_pgoff(page));
mm/debug.c:	pr_warn("flags: %#lx(%pGp)\n", page->flags, &page->flags);
mm/debug.c:	if (!page_poisoned && page->mem_cgroup)
mm/debug.c:		pr_warn("page->mem_cgroup:%px\n", page->mem_cgroup);
mm/dmapool.c:			blocks += page->in_use;
mm/dmapool.c:		*(int *)(page->vaddr + offset) = next;
mm/dmapool.c:	page->vaddr = dma_alloc_coherent(pool->dev, pool->allocation,
mm/dmapool.c:					 &page->dma, mem_flags);
mm/dmapool.c:	if (page->vaddr) {
mm/dmapool.c:		memset(page->vaddr, POOL_POISON_FREED, pool->allocation);
mm/dmapool.c:		page->in_use = 0;
mm/dmapool.c:		page->offset = 0;
mm/dmapool.c:	return page->in_use != 0;
mm/dmapool.c:	dma_addr_t dma = page->dma;
mm/dmapool.c:	memset(page->vaddr, POOL_POISON_FREED, pool->allocation);
mm/dmapool.c:	dma_free_coherent(pool->dev, pool->allocation, page->vaddr, dma);
mm/dmapool.c:	list_del(&page->page_list);
mm/dmapool.c:					pool->name, page->vaddr);
mm/dmapool.c:				       pool->name, page->vaddr);
mm/dmapool.c:			list_del(&page->page_list);
mm/dmapool.c:		if (page->offset < pool->allocation)
mm/dmapool.c:	list_add(&page->page_list, &pool->page_list);
mm/dmapool.c:	page->in_use++;
mm/dmapool.c:	offset = page->offset;
mm/dmapool.c:	page->offset = *(int *)(page->vaddr + offset);
mm/dmapool.c:	retval = offset + page->vaddr;
mm/dmapool.c:	*handle = offset + page->dma;
mm/dmapool.c:		/* page->offset is stored in first 4 bytes */
mm/dmapool.c:		for (i = sizeof(page->offset); i < pool->size; i++) {
mm/dmapool.c:		if (dma < page->dma)
mm/dmapool.c:		if ((dma - page->dma) < pool->allocation)
mm/dmapool.c:	offset = vaddr - page->vaddr;
mm/dmapool.c:	if ((dma - page->dma) != offset) {
mm/dmapool.c:		unsigned int chain = page->offset;
mm/dmapool.c:				chain = *(int *)(page->vaddr + chain);
mm/dmapool.c:	page->in_use--;
mm/dmapool.c:	*(int *)vaddr = page->offset;
mm/dmapool.c:	page->offset = offset;
mm/early_ioremap.c:	 * Mappings have to be page-aligned
mm/filemap.c: * page-cache, 21.05.1999, Ingo Molnar <mingo@redhat.com>
mm/filemap.c: *    ->zone_lru_lock(zone)	(follow_page->mark_page_accessed)
mm/filemap.c:	XA_STATE(xas, &mapping->i_pages, page->index);
mm/filemap.c:		xas_set_order(&xas, page->index, compound_order(page));
mm/filemap.c:	page->mapping = NULL;
mm/filemap.c:	/* Leave page->index set: truncation lookup relies upon it */
mm/filemap.c:	struct address_space *mapping = page->mapping;
mm/filemap.c:				VM_BUG_ON_PAGE(page->index >
mm/filemap.c:			page->mapping = NULL;
mm/filemap.c:			 * Leave page->index set: truncation lookup relies
mm/filemap.c:			VM_BUG_ON_PAGE(page->index + HPAGE_PMD_NR - tail_pages
mm/filemap.c:	page->mapping = mapping;
mm/filemap.c:	page->index = offset;
mm/filemap.c:	page->mapping = NULL;
mm/filemap.c:	/* Leave page->index set: truncation relies upon it */
mm/filemap.c:	if (wait_page->page != key->page)
mm/filemap.c:	if (wait_page->bit_nr != key->bit_nr)
mm/filemap.c:	if (test_bit(key->bit_nr, &key->page->flags))
mm/filemap.c:		bit_is_set = test_bit(bit_nr, &page->flags);
mm/filemap.c:			if (!test_and_set_bit_lock(bit_nr, &page->flags))
mm/filemap.c:			if (!test_bit(bit_nr, &page->flags))
mm/filemap.c:			 * We can no longer safely access page->flags:
mm/filemap.c:	if (clear_bit_unlock_is_negative_byte(PG_locked, &page->flags))
mm/filemap.c:		if (unlikely(page->mapping != mapping)) {
mm/filemap.c:		VM_BUG_ON_PAGE(page->index != offset, page);
mm/filemap.c:			*start = page->index + 1;
mm/filemap.c:		if (!page->mapping || page_to_pgoff(page) != xas.xa_index) {
mm/filemap.c:			*index = page->index + 1;
mm/filemap.c:			if (!page->mapping)
mm/filemap.c:		if (!page->mapping) {
mm/filemap.c:				if (page->mapping == NULL) {
mm/filemap.c:	if (unlikely(page->mapping != mapping)) {
mm/filemap.c:	VM_BUG_ON_PAGE(page->index != offset, page);
mm/filemap.c:		if (page->mapping != mapping || !PageUptodate(page))
mm/filemap.c:		if (page->index >= max_idx)
mm/filemap.c:	if (page->mapping != inode->i_mapping) {
mm/filemap.c:	if (!page->mapping) {
mm/filemap.c:		 * page-cache pages correctly).
mm/filemap.c: * (presumably at page->private).  If the release was successful, return '1'.
mm/filemap.c:	struct address_space * const mapping = page->mapping;
mm/gup.c:		if (page->mapping && trylock_page(page)) {
mm/highmem.c: * Describes one page->virtual association
mm/hmm.c:	struct hmm_devmem *devmem = page->pgmap->data;
mm/hmm.c:	page->mapping = NULL;
mm/huge_memory.c:	 * we use page->mapping and page->index in second tail page
mm/huge_memory.c:	 * we use page->mapping and page->indexlru in second tail page
mm/huge_memory.c:		if (PageDoubleMap(page) || !page->mapping)
mm/huge_memory.c:		if (page->mapping && !PageDoubleMap(page))
mm/huge_memory.c:		return atomic_read(&page->_mapcount) + 1;
mm/huge_memory.c:		mapcount = atomic_read(&page->_mapcount) + 1;
mm/hugetlb.c:	list_move(&page->lru, &h->hugepage_freelists[nid]);
mm/hugetlb.c:	if (&h->hugepage_freelists[nid] == &page->lru)
mm/hugetlb.c:	list_move(&page->lru, &h->hugepage_activelist);
mm/hugetlb.c:	page->mapping = NULL;
mm/hugetlb.c:		list_del(&page->lru);
mm/hugetlb.c:		list_del(&page->lru);
mm/hugetlb.c:	INIT_LIST_HEAD(&page->lru);
mm/hugetlb.c:			list_del(&page->lru);
mm/hugetlb.c:		list_add(&page->lru, &surplus_list);
mm/hugetlb.c:		list_move(&page->lru, &h->hugepage_activelist);
mm/hugetlb.c:			list_del(&page->lru);
mm/hugetlb.c:	list_move_tail(&page->lru, list);
mm/hugetlb.c:	list_move_tail(&page->lru, &(page_hstate(page))->hugepage_activelist);
mm/hugetlb_cgroup.c:	list_move(&newhpage->lru, &h->hugepage_activelist);
mm/kasan/common.c:		return __kasan_kmalloc(page->slab_cache, object, size,
mm/kasan/common.c:		__kasan_slab_free(page->slab_cache, ptr, ip, false);
mm/kasan/report.c:		struct kmem_cache *cache = page->slab_cache;
mm/khugepaged.c:	new_page->index = start;
mm/khugepaged.c:	new_page->mapping = mapping;
mm/khugepaged.c:		list_add_tail(&page->lru, &pagelist);
mm/khugepaged.c:			while (index < page->index) {
mm/khugepaged.c:			copy_highpage(new_page + (page->index % HPAGE_PMD_NR),
mm/khugepaged.c:			list_del(&page->lru);
mm/khugepaged.c:			page->mapping = NULL;
mm/khugepaged.c:			if (!page || xas.xa_index < page->index) {
mm/khugepaged.c:			VM_BUG_ON_PAGE(page->index != xas.xa_index, page);
mm/khugepaged.c:			list_del(&page->lru);
mm/khugepaged.c:		new_page->mapping = NULL;
mm/ksm.c: * page to reset its page->mapping to NULL, and relies on no other use of
mm/ksm.c: * a page to put something that might look like our key in page->mapping.
mm/ksm.c:	if (READ_ONCE(page->mapping) != expected_mapping)
mm/ksm.c:		 * Another check for page->mapping != expected_mapping would
mm/ksm.c:		 * page->mapping reset to NULL later, in free_pages_prepare().
mm/ksm.c:	if (READ_ONCE(page->mapping) != expected_mapping) {
mm/ksm.c:		if (READ_ONCE(page->mapping) != expected_mapping) {
mm/ksm.c:	 * We come here from above when page->mapping or !PageSwapCache
mm/ksm.c:	page->mapping = (void *)((unsigned long)stable_node | PAGE_MAPPING_KSM);
mm/ksm.c:		 page->index == linear_page_index(vma, address)) {
mm/ksm.c:	VM_BUG_ON_PAGE(newpage->mapping != oldpage->mapping, newpage);
mm/ksm.c:		 * newpage->mapping was set in advance; now we need smp_wmb()
mm/ksm.c:		 * to get_ksm_page() before it can see that oldpage->mapping
mm/list_lru.c:	return page->mem_cgroup;
mm/madvise.c:#include <linux/page-isolation.h>
mm/madvise.c: *  -EINVAL - start + len < 0, start is not page-aligned,
mm/memcontrol.c:#include <linux/page-flags.h>
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c:	memcg = READ_ONCE(page->mem_cgroup);
mm/memcontrol.c: * Obtain a reference on page->memcg and returns it if successful. Otherwise
mm/memcontrol.c:	struct mem_cgroup *memcg = page->mem_cgroup;
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c: * lock_page_memcg - lock a page->mem_cgroup binding
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c:	if (memcg != page->mem_cgroup) {
mm/memcontrol.c: * unlock_page_memcg - unlock a page->mem_cgroup binding
mm/memcontrol.c:	__unlock_page_memcg(page->mem_cgroup);
mm/memcontrol.c:	VM_BUG_ON_PAGE(page->mem_cgroup, page);
mm/memcontrol.c:	 * page->mem_cgroup at this point:
mm/memcontrol.c:	page->mem_cgroup = memcg;
mm/memcontrol.c:	page->mem_cgroup = memcg;
mm/memcontrol.c:	struct mem_cgroup *memcg = page->mem_cgroup;
mm/memcontrol.c:	page->mem_cgroup = NULL;
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c:	/* page is moved even if it's not RSS of this task(page-faulted). */
mm/memcontrol.c:	 * page->mem_cgroup of its source page while we change it.
mm/memcontrol.c:	if (page->mem_cgroup != from)
mm/memcontrol.c:	 * It is safe to change page->mem_cgroup here because the page
mm/memcontrol.c:	page->mem_cgroup = to;
mm/memcontrol.c:		if (page->mem_cgroup == mc.from) {
mm/memcontrol.c:	if (page->mem_cgroup == mc.from) {
mm/memcontrol.c: * After page->mapping has been set up, the caller must finalize the
mm/memcontrol.c: * after page->mapping has been set up.  This must happen atomically
mm/memcontrol.c:	VM_BUG_ON_PAGE(!page->mapping, page);
mm/memcontrol.c:	if (!page->mem_cgroup)
mm/memcontrol.c:	 * page->mem_cgroup at this point, we have fully
mm/memcontrol.c:	if (ug->memcg != page->mem_cgroup) {
mm/memcontrol.c:		ug->memcg = page->mem_cgroup;
mm/memcontrol.c:	page->mem_cgroup = NULL;
mm/memcontrol.c:	 * Note that the list can be a single page->lru; hence the
mm/memcontrol.c:		next = page->lru.next;
mm/memcontrol.c:	/* Don't touch page->lru of any random page, pre-check: */
mm/memcontrol.c:	if (!page->mem_cgroup)
mm/memcontrol.c: * Both pages must be locked, @newpage->mapping must be set up.
mm/memcontrol.c:	if (newpage->mem_cgroup)
mm/memcontrol.c:	memcg = oldpage->mem_cgroup;
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c:	page->mem_cgroup = NULL;
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memcontrol.c:	memcg = page->mem_cgroup;
mm/memory-failure.c: *   tools/vm/page-types when running a real workload.
mm/memory-failure.c:#include <linux/page-flags.h>
mm/memory-failure.c:#include <linux/kernel-page-flags.h>
mm/memory-failure.c:#include <linux/page-isolation.h>
mm/memory-failure.c:	struct address_space *mapping = page->mapping;
mm/memory-failure.c:	if (!page->mapping)
mm/memory-failure.c: * A page state is defined by its current page->flags bits.
mm/memory-failure.c:		start = (page->index << PAGE_SHIFT) & ~(size - 1);
mm/memory-failure.c:		unmap_mapping_range(page->mapping, start, start + size, 0);
mm/memory-failure.c:		page_flags = hpage->flags;
mm/memory-failure.c:				pfn, page->flags, &page->flags);
mm/memory-failure.c:			pfn, ret, page->flags, &page->flags);
mm/memory-failure.c:		list_add(&page->lru, &pagelist);
mm/memory-failure.c:				pfn, ret, page->flags, &page->flags);
mm/memory-failure.c:			pfn, ret, page_count(page), page->flags, &page->flags);
mm/memory.c:#warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
mm/memory.c: * function from other places, for example from page-fault handler.
mm/memory.c:	 * You *really* shouldn't map things that aren't page-aligned,
mm/memory.c:		if (!page->mapping) {
mm/memory.c: *   relevant references. This includes dropping the reference the page-table
mm/memory.c:	page_ptl_cachep = kmem_cache_create("page->ptl", sizeof(spinlock_t), 0,
mm/memory.c:	page->ptl = ptl;
mm/memory.c:	kmem_cache_free(page_ptl_cachep, page->ptl);
mm/memory_hotplug.c:#include <linux/page-isolation.h>
mm/memory_hotplug.c:	page->freelist = (void *)type;
mm/memory_hotplug.c:	type = (unsigned long) page->freelist;
mm/memory_hotplug.c:		page->freelist = NULL;
mm/memory_hotplug.c:		INIT_LIST_HEAD(&page->lru);
mm/memory_hotplug.c:			list_add_tail(&page->lru, &source);
mm/migrate.c:	/* Driver shouldn't use PG_isolated bit of page->flags */
mm/migrate.c:		list_del(&page->lru);
mm/migrate.c:			new = page - pvmw.page->index +
mm/migrate.c:		newpage->index = page->index;
mm/migrate.c:		newpage->mapping = page->mapping;
mm/migrate.c:	newpage->index = page->index;
mm/migrate.c:	newpage->mapping = page->mapping;
mm/migrate.c:	newpage->index = page->index;
mm/migrate.c:	newpage->mapping = page->mapping;
mm/migrate.c:	 * When successful, old pagecache page->mapping must be cleared before
mm/migrate.c:		 * Anonymous and movable page->mapping will be cleard by
mm/migrate.c:			page->mapping = NULL;
mm/migrate.c:	 * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,
mm/migrate.c:	 * Calling try_to_unmap() against a page->mapping==NULL page will
mm/migrate.c:	if (!page->mapping) {
mm/migrate.c:		list_del(&page->lru);
mm/migrate.c:	list_add(&page->lru, &migratepages);
mm/migrate.c:			list_del(&page->lru);
mm/migrate.c:	/* anon mapping, we can simply copy page->mapping to the new page: */
mm/migrate.c:	new_page->mapping = page->mapping;
mm/migrate.c:	new_page->index = page->index;
mm/migrate.c:		if (!page || !page->mapping || PageTransCompound(page)) {
mm/mincore.c: * and is up to date; i.e. that no page-in operation would be required
mm/mincore.c:	/* Check the start address: needs to be page-aligned.. */
mm/mm_init.c:		"location: %d -> %d layout %d -> %d unused %d -> %d page-flags\n",
mm/mmu_gather.c:	 * Invalidate page-table caches used by hardware walkers. Then we still
mm/mmu_gather.c:	 * This isn't an RCU grace period and hence the page-tables cannot be
mm/mmu_gather.c:	 * It is however sufficient for software page-table walkers that rely on
mm/mmu_gather.c: * tlb_gather_mmu - initialize an mmu_gather structure for page-table tear-down
mm/mmu_gather.c: * @start: start of the region that will be removed from the page-table
mm/mmu_gather.c: * @end: end of the region that will be removed from the page-table
mm/mmu_gather.c: * Called to initialize an (on-stack) mmu_gather structure for page-table
mm/mmzone.c:		old_flags = flags = page->flags;
mm/mmzone.c:	} while (unlikely(cmpxchg(&page->flags, old_flags, flags) != old_flags));
mm/nommu.c:	 * - note that this may not return a page-aligned address if the object
mm/page-writeback.c: * mm/page-writeback.c
mm/page-writeback.c: * multiple pages in ascending page->index order, and looping back to the start
mm/page-writeback.c:			done_index = page->index;
mm/page-writeback.c:			if (unlikely(page->mapping != mapping)) {
mm/page-writeback.c:					done_index = page->index + 1;
mm/page-writeback.c:	struct address_space *mapping = page->mapping;
mm/page-writeback.c:	struct address_space *mapping = page->mapping;
mm/page-writeback.c: * page->mapping->host, and if the page is unlocked.  This is because another
mm/page-writeback.c:	if (bdi_cap_stable_pages_required(inode_to_bdi(page->mapping->host)))
mm/page_alloc.c:#include <linux/page-isolation.h>
mm/page_alloc.c:	return page->index;
mm/page_alloc.c:	page->index = migratetype;
mm/page_alloc.c:	bad_flags &= page->flags;
mm/page_alloc.c: * in bit 0 of page->compound_head. The rest of bits is pointer to head page.
mm/page_alloc.c:	INIT_LIST_HEAD(&page->lru);
mm/page_alloc.c:	VM_BUG_ON_PAGE(page->flags & PAGE_FLAGS_CHECK_AT_PREP, page);
mm/page_alloc.c:			list_add_tail(&page->lru,
mm/page_alloc.c:	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
mm/page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
mm/page_alloc.c:	if (unlikely((unsigned long)page->mapping |
mm/page_alloc.c:			(unsigned long)page->mem_cgroup |
mm/page_alloc.c:			(page->flags & check_flags)))
mm/page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
mm/page_alloc.c:	if (unlikely(page->mapping != NULL))
mm/page_alloc.c:	if (unlikely(page->flags & PAGE_FLAGS_CHECK_AT_FREE)) {
mm/page_alloc.c:	if (unlikely(page->mem_cgroup))
mm/page_alloc.c:	 * We rely page->lru.next never has bit 0 set, unless the page
mm/page_alloc.c:		if (page->mapping != TAIL_MAPPING) {
mm/page_alloc.c:	page->mapping = NULL;
mm/page_alloc.c:		page->mapping = NULL;
mm/page_alloc.c:	page->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
mm/page_alloc.c:			list_del(&page->lru);
mm/page_alloc.c:			list_add_tail(&page->lru, &head);
mm/page_alloc.c:	 * page->lru.next will not point to original list.
mm/page_alloc.c:	INIT_LIST_HEAD(&page->lru);
mm/page_alloc.c:			INIT_LIST_HEAD(&page->lru);
mm/page_alloc.c:	if (unlikely(atomic_read(&page->_mapcount) != -1))
mm/page_alloc.c:	if (unlikely(page->mapping != NULL))
mm/page_alloc.c:	if (unlikely(page->flags & __PG_HWPOISON)) {
mm/page_alloc.c:	if (unlikely(page->flags & PAGE_FLAGS_CHECK_AT_PREP)) {
mm/page_alloc.c:	if (unlikely(page->mem_cgroup))
mm/page_alloc.c:		list_del(&page->lru);
mm/page_alloc.c:		list_move(&page->lru,
mm/page_alloc.c:	list_move(&page->lru, &area->free_list[start_type]);
mm/page_alloc.c:		list_add_tail(&page->lru, list);
mm/page_alloc.c:	list_add(&page->lru, &pcp->lists[migratetype]);
mm/page_alloc.c:			list_del(&page->lru);
mm/page_alloc.c:	list_del(&page->lru);
mm/page_alloc.c:		list_del(&page->lru);
mm/page_alloc.c:		page->pgmap = pgmap;
mm/page_alloc.c:		page->hmm_data = 0;
mm/page_alloc.c:		 * because their page->_refcount is zero at all time.
mm/page_alloc.c:		list_del(&page->lru);
mm/page_ext.c:		 * page->flags of out of node pages are not initialized.  So we
mm/page_isolation.c:#include <linux/page-isolation.h>
mm/page_isolation.c: * start_isolate_page_range() - make page-allocation-type of range of pages to
mm/page_isolation.c: * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in
mm/page_owner.c:			page->flags, &page->flags);
mm/percpu.c:	page->index = (unsigned long)pcpu;
mm/percpu.c:	return (struct pcpu_chunk *)page->index;
mm/percpu.c: * This is a helper to ease setting up page-remapped first percpu
mm/percpu.c: * page-by-page into vmalloc area.
mm/readahead.c:		page->mapping = mapping;
mm/readahead.c:		page->mapping = NULL;
mm/readahead.c:		list_del(&page->lru);
mm/readahead.c:		if (add_to_page_cache_lru(page, mapping, page->index,
mm/readahead.c:		list_del(&page->lru);
mm/readahead.c:		if (!add_to_page_cache_lru(page, mapping, page->index, gfp))
mm/readahead.c:		page->index = page_offset;
mm/readahead.c:		list_add(&page->lru, &page_pool);
mm/readahead.c: * @offset: start offset into @mapping, in pagecache page-sized units
mm/readahead.c: * @offset: start offset into @mapping, in pagecache page-sized units
mm/rmap.c: *     page->flags PG_locked (lock_page)
mm/rmap.c: * that the anon_vma pointer from page->mapping is valid if there is a
mm/rmap.c:	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
mm/rmap.c:	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
mm/rmap.c:	} else if (page->mapping) {
mm/rmap.c:		if (!vma->vm_file || vma->vm_file->f_mapping != page->mapping)
mm/rmap.c:	WRITE_ONCE(page->mapping, (struct address_space *) anon_vma);
mm/rmap.c:	page->mapping = (struct address_space *) anon_vma;
mm/rmap.c:	page->index = linear_page_index(vma, address);
mm/rmap.c:		first = atomic_inc_and_test(&page->_mapcount);
mm/rmap.c:		atomic_set(&page->_mapcount, 0);
mm/rmap.c:		if (!atomic_inc_and_test(&page->_mapcount))
mm/rmap.c:		if (!atomic_add_negative(-1, &page->_mapcount))
mm/rmap.c:	if (!atomic_add_negative(-1, &page->_mapcount))
mm/rmap.c:	 * The page lock not only makes sure that page->mapping cannot
mm/shmem.c:	page->mapping = mapping;
mm/shmem.c:	page->index = index;
mm/shmem.c:		page->mapping = NULL;
mm/shmem.c:	struct address_space *mapping = page->mapping;
mm/shmem.c:	error = shmem_replace_entry(mapping, page->index, page, radswap);
mm/shmem.c:	page->mapping = NULL;
mm/shmem.c:	mapping = page->mapping;
mm/shmem.c:	index = page->index;
mm/slab.c:	return page->slab_cache;
mm/slab.c:	return page->s_mem + cache->size * idx;
mm/slab.c:	page->mapping = NULL;
mm/slab.c:	cachep = page->slab_cache;
mm/slab.c:		poison_obj(cachep, page->freelist - obj_offset(cachep),
mm/slab.c:	freelist = page->freelist;
mm/slab.c:		call_rcu(&page->rcu_head, kmem_rcu_free);
mm/slab.c:		list_del(&page->lru);
mm/slab.c:		list_del(&page->lru);
mm/slab.c:	page->s_mem = addr + colour_off;
mm/slab.c:	page->active = 0;
mm/slab.c:	return ((freelist_idx_t *)page->freelist)[idx];
mm/slab.c:	((freelist_idx_t *)(page->freelist))[idx] = val;
mm/slab.c:	swap(((freelist_idx_t *)page->freelist)[a],
mm/slab.c:		((freelist_idx_t *)page->freelist)[b]);
mm/slab.c:		page->freelist = index_to_obj(cachep, page, objfreelist) +
mm/slab.c:		page->freelist = index_to_obj(cachep, page, cachep->num - 1) +
mm/slab.c:	objp = index_to_obj(cachep, page, get_free_obj(page, page->active));
mm/slab.c:	page->active++;
mm/slab.c:	for (i = page->active; i < cachep->num; i++) {
mm/slab.c:	page->active--;
mm/slab.c:	if (!page->freelist)
mm/slab.c:		page->freelist = objp + obj_offset(cachep);
mm/slab.c:	set_free_obj(page, page->active, objnr);
mm/slab.c:	page->slab_cache = cache;
mm/slab.c:	page->freelist = freelist;
mm/slab.c:	INIT_LIST_HEAD(&page->lru);
mm/slab.c:	if (!page->active) {
mm/slab.c:		list_add_tail(&page->lru, &(n->slabs_free));
mm/slab.c:	n->free_objects += cachep->num - page->active;
mm/slab.c:	list_del(&page->lru);
mm/slab.c:	if (page->active == cachep->num) {
mm/slab.c:		list_add(&page->lru, &n->slabs_full);
mm/slab.c:				void **objp = page->freelist;
mm/slab.c:			page->freelist = NULL;
mm/slab.c:		list_add(&page->lru, &n->slabs_partial);
mm/slab.c:	list_del(&page->lru);
mm/slab.c:	if (!page->active) {
mm/slab.c:		list_add_tail(&page->lru, &n->slabs_free);
mm/slab.c:		list_add_tail(&page->lru, &n->slabs_partial);
mm/slab.c:	BUG_ON(page->active >= cachep->num);
mm/slab.c:	while (page->active < cachep->num && batchcount--) {
mm/slab.c:	BUG_ON(page->active == cachep->num);
mm/slab.c:		list_del(&page->lru);
mm/slab.c:		if (page->active == 0) {
mm/slab.c:			list_add(&page->lru, &n->slabs_free);
mm/slab.c:			list_add_tail(&page->lru, &n->slabs_partial);
mm/slab.c:		list_move(&page->lru, list);
mm/slab.c:			BUG_ON(page->active);
mm/slab.c:	for (i = 0, p = page->s_mem; i < c->num; i++, p += c->size) {
mm/slab.c:		for (j = page->active; j < c->num; j++) {
mm/slab.c:	cachep = page->slab_cache;
mm/slab.h:	cachep = page->slab_cache;
mm/slob.c: * alignment. Again, objects of page-size or greater are allocated by
mm/slub.c: *	A. page->freelist	-> List of object free in a page
mm/slub.c: *	B. page->inuse		-> Number of objects in use
mm/slub.c: *	C. page->objects	-> Number of objects in page
mm/slub.c: *	D. page->frozen		-> frozen state
mm/slub.c:	bit_spin_lock(PG_locked, &page->flags);
mm/slub.c:	__bit_spin_unlock(PG_locked, &page->flags);
mm/slub.c:		if (cmpxchg_double(&page->freelist, &page->counters,
mm/slub.c:		if (page->freelist == freelist_old &&
mm/slub.c:					page->counters == counters_old) {
mm/slub.c:			page->freelist = freelist_new;
mm/slub.c:			page->counters = counters_new;
mm/slub.c:		if (cmpxchg_double(&page->freelist, &page->counters,
mm/slub.c:		if (page->freelist == freelist_old &&
mm/slub.c:					page->counters == counters_old) {
mm/slub.c:			page->freelist = freelist_new;
mm/slub.c:			page->counters = counters_new;
mm/slub.c:	for (p = page->freelist; p; p = get_freepointer(s, p))
mm/slub.c:	if (object < base || object >= base + page->objects * s->size ||
mm/slub.c:	       page, page->objects, page->inuse, page->freelist, page->flags);
mm/slub.c:	if (page->objects > maxobj) {
mm/slub.c:			page->objects, maxobj);
mm/slub.c:	if (page->inuse > page->objects) {
mm/slub.c:			page->inuse, page->objects);
mm/slub.c:	fp = page->freelist;
mm/slub.c:	while (fp && nr <= page->objects) {
mm/slub.c:				page->freelist = NULL;
mm/slub.c:				page->inuse = page->objects;
mm/slub.c:	if (page->objects != max_objects) {
mm/slub.c:			 page->objects, max_objects);
mm/slub.c:		page->objects = max_objects;
mm/slub.c:	if (page->inuse != page->objects - nr) {
mm/slub.c:			 page->inuse, page->objects - nr);
mm/slub.c:		page->inuse = page->objects - nr;
mm/slub.c:			object, page->inuse,
mm/slub.c:			page->freelist);
mm/slub.c:	list_add(&page->lru, &n->full);
mm/slub.c:	list_del(&page->lru);
mm/slub.c:		page->inuse = page->objects;
mm/slub.c:		page->freelist = NULL;
mm/slub.c:	if (unlikely(s != page->slab_cache)) {
mm/slub.c:		} else if (!page->slab_cache) {
mm/slub.c:	if (page->objects < 2 || !s->random_seq)
mm/slub.c:	page_limit = page->objects * s->size;
mm/slub.c:	page->freelist = cur;
mm/slub.c:	for (idx = 1; idx < page->objects; idx++) {
mm/slub.c:	page->objects = oo_objects(oo);
mm/slub.c:	page->slab_cache = s;
mm/slub.c:		page->freelist = start;
mm/slub.c:		for (idx = 0, p = start; idx < page->objects - 1; idx++) {
mm/slub.c:	page->inuse = page->objects;
mm/slub.c:	page->frozen = 1;
mm/slub.c:	inc_slabs_node(s, page_to_nid(page), page->objects);
mm/slub.c:						page->objects)
mm/slub.c:	page->mapping = NULL;
mm/slub.c:	__free_slab(page->slab_cache, page);
mm/slub.c:		call_rcu(&page->rcu_head, rcu_free_slab);
mm/slub.c:	dec_slabs_node(s, page_to_nid(page), page->objects);
mm/slub.c:		list_add_tail(&page->lru, &n->partial);
mm/slub.c:		list_add(&page->lru, &n->partial);
mm/slub.c:	list_del(&page->lru);
mm/slub.c:	freelist = page->freelist;
mm/slub.c:	counters = page->counters;
mm/slub.c:		new.inuse = page->objects;
mm/slub.c:	if (page->freelist) {
mm/slub.c:			prior = page->freelist;
mm/slub.c:			counters = page->counters;
mm/slub.c:	old.freelist = page->freelist;
mm/slub.c:	old.counters = page->counters;
mm/slub.c:		c->partial = page->next;
mm/slub.c:			old.freelist = page->freelist;
mm/slub.c:			old.counters = page->counters;
mm/slub.c:			page->next = discard_page;
mm/slub.c:		discard_page = discard_page->next;
mm/slub.c:			pobjects = oldpage->pobjects;
mm/slub.c:			pages = oldpage->pages;
mm/slub.c:		pobjects += page->objects - page->inuse;
mm/slub.c:		page->pages = pages;
mm/slub.c:		page->pobjects = pobjects;
mm/slub.c:		page->next = oldpage;
mm/slub.c:	return page->objects - page->inuse;
mm/slub.c:		freelist = page->freelist;
mm/slub.c:		page->freelist = NULL;
mm/slub.c: * Check the page->freelist of a page and either transfer the freelist to the
mm/slub.c:		freelist = page->freelist;
mm/slub.c:		counters = page->counters;
mm/slub.c:		new.inuse = page->objects;
mm/slub.c:	VM_BUG_ON(!c->page->frozen);
mm/slub.c:		prior = page->freelist;
mm/slub.c:		counters = page->counters;
mm/slub.c:		df->s = page->slab_cache;
mm/slub.c:	n = page->freelist;
mm/slub.c:	page->freelist = get_freepointer(kmem_cache_node, n);
mm/slub.c:	page->inuse = 1;
mm/slub.c:	page->frozen = 0;
mm/slub.c:	inc_slabs_node(kmem_cache_node, node, page->objects);
mm/slub.c:	unsigned long *map = bitmap_zalloc(page->objects, GFP_ATOMIC);
mm/slub.c:	for_each_object(p, s, addr, page->objects) {
mm/slub.c:		if (!page->inuse) {
mm/slub.c:			list_add(&page->lru, &discard);
mm/slub.c:	s = page->slab_cache;
mm/slub.c:	return slab_ksize(page->slab_cache);
mm/slub.c:	slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
mm/slub.c:		 * list_lock. page->inuse here is the upper limit.
mm/slub.c:			int free = page->objects - page->inuse;
mm/slub.c:			/* Do not reread page->inuse */
mm/slub.c:			if (free == page->objects) {
mm/slub.c:				list_move(&page->lru, &discard);
mm/slub.c:				list_move(&page->lru, promote + free - 1);
mm/slub.c:	return page->inuse;
mm/slub.c:	return page->objects;
mm/slub.c:	bitmap_zero(map, page->objects);
mm/slub.c:	for_each_object(p, s, addr, page->objects) {
mm/slub.c:	for_each_object(p, s, addr, page->objects)
mm/slub.c:	bitmap_zero(map, page->objects);
mm/slub.c:	for_each_object(p, s, addr, page->objects)
mm/slub.c:				x = page->objects;
mm/slub.c:				x = page->inuse;
mm/slub.c:					x = page->pages;
mm/slub.c:			pages += page->pages;
mm/slub.c:			objects += page->pobjects;
mm/slub.c:				page->pobjects, page->pages);
mm/sparse.c:		magic = (unsigned long) page->freelist;
mm/swap.c:		put_dev_pagemap(page->pgmap);
mm/swap.c: * @pages: list of pages threaded on page->lru
mm/swap.c:		list_move_tail(&page->lru, &lruvec->lists[lru]);
mm/swap.c:		list_add(&page->lru, &pages_to_free);
mm/swap.c:		list_add_tail(&page_tail->lru, &page->lru);
mm/swap.c: * passed on to page-only pagevec operations.
mm/swapfile.c:		 * perform discards for released swap page-clusters.
mm/swapfile.c:	list_add_tail(&page->lru, &head->lru);
mm/swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
mm/swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
mm/swapfile.c:		page = list_entry(page->lru.prev, struct page, lru);
mm/swapfile.c:			page = list_entry(page->lru.prev, struct page, lru);
mm/swapfile.c:			page = list_entry(page->lru.next, struct page, lru);
mm/swapfile.c:		page = list_entry(page->lru.prev, struct page, lru);
mm/swapfile.c:			page = list_entry(page->lru.prev, struct page, lru);
mm/swapfile.c:				list_del(&page->lru);
mm/truncate.c:	invalidatepage = page->mapping->a_ops->invalidatepage;
mm/truncate.c: * We need to bale out if page->mapping is no longer equal to the original
mm/truncate.c:		unmap_mapping_pages(mapping, page->index, nr, false);
mm/truncate.c:	if (page->mapping != mapping)
mm/truncate.c:	if (page->mapping != mapping)
mm/truncate.c:			/* We rely upon deletion not changing page->index */
mm/truncate.c:			if (page->mapping != mapping) {
mm/truncate.c:			/* We rely upon deletion not changing page->index */
mm/truncate.c:			/* We rely upon deletion not changing page->index */
mm/truncate.c:	if (page->mapping != mapping)
mm/truncate.c:	if (page->mapping != mapping || mapping->a_ops->launder_page == NULL)
mm/truncate.c:			/* We rely upon deletion not changing page->index */
mm/truncate.c:			if (page->mapping != mapping) {
mm/util.c:	mapping = (unsigned long)page->mapping;
mm/util.c:/* Neutral page->mapping pointer to address_space or anon_vma or other */
mm/util.c:		return atomic_read(&page->_mapcount) >= 0;
mm/util.c:	mapping = (unsigned long)page->mapping;
mm/util.c:	mapping = page->mapping;
mm/util.c:	ret = atomic_read(&page->_mapcount) + 1;
mm/util.c:	 * For file THP page->_mapcount contains total number of mapping
mm/vmscan.c:			prev = lru_to_page(&(_page->lru));		\
mm/vmscan.c:			prev = lru_to_page(&(_page->lru));		\
mm/vmscan.c:	 * heads at page->private.
mm/vmscan.c:		 * page->mapping == NULL while being dirty with clean buffers.
mm/vmscan.c:	 * escape unnoticed. The smp_rmb is needed to ensure the page->flags
mm/vmscan.c:	 * load is not satisfied before that of page->_refcount.
mm/vmscan.c:		list_del(&page->lru);
mm/vmscan.c:				list_add_tail(&page->lru, page_list);
mm/vmscan.c:			list_add(&page->lru, &free_pages);
mm/vmscan.c:		list_add(&page->lru, &ret_pages);
mm/vmscan.c:			list_move(&page->lru, &clean_pages);
mm/vmscan.c:			list_move(&page->lru, &pages_skipped);
mm/vmscan.c:			list_move(&page->lru, dst);
mm/vmscan.c:			list_move(&page->lru, src);
mm/vmscan.c:		list_del(&page->lru);
mm/vmscan.c:				list_add(&page->lru, &pages_to_free);
mm/vmscan.c: * The downside is that we have to touch page->_refcount against each page.
mm/vmscan.c: * But we had to alter page->flags anyway.
mm/vmscan.c:		list_move(&page->lru, &lruvec->lists[lru]);
mm/vmscan.c:				list_add(&page->lru, pages_to_free);
mm/vmscan.c:		list_del(&page->lru);
mm/vmscan.c:				list_add(&page->lru, &l_active);
mm/vmscan.c:		list_add(&page->lru, &l_inactive);
mm/vmscan.c: * This is the direct reclaim path, for page-allocating processes.  We only
mm/workingset.c:	/* Page is fully exclusive and pins page->mem_cgroup */
mm/z3fold.c:	INIT_LIST_HEAD(&page->lru);
mm/z3fold.c:	clear_bit(PAGE_HEADLESS, &page->private);
mm/z3fold.c:	clear_bit(MIDDLE_CHUNK_MAPPED, &page->private);
mm/z3fold.c:	clear_bit(NEEDS_COMPACTING, &page->private);
mm/z3fold.c:	clear_bit(PAGE_STALE, &page->private);
mm/z3fold.c:	clear_bit(PAGE_CLAIMED, &page->private);
mm/z3fold.c:	set_bit(PAGE_STALE, &page->private);
mm/z3fold.c:	clear_bit(NEEDS_COMPACTING, &page->private);
mm/z3fold.c:	if (!list_empty(&page->lru))
mm/z3fold.c:		list_del(&page->lru);
mm/z3fold.c:		if (WARN_ON(!test_bit(PAGE_STALE, &page->private)))
mm/z3fold.c:	if (test_bit(MIDDLE_CHUNK_MAPPED, &page->private))
mm/z3fold.c:	if (WARN_ON(!test_and_clear_bit(NEEDS_COMPACTING, &page->private))) {
mm/z3fold.c:			if (test_bit(NEEDS_COMPACTING, &page->private)) {
mm/z3fold.c:		set_bit(PAGE_HEADLESS, &page->private);
mm/z3fold.c:	if (!list_empty(&page->lru))
mm/z3fold.c:		list_del(&page->lru);
mm/z3fold.c:	list_add(&page->lru, &pool->lru);
mm/z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private)) {
mm/z3fold.c:		if (!test_and_set_bit(PAGE_CLAIMED, &page->private)) {
mm/z3fold.c:			list_del(&page->lru);
mm/z3fold.c:	if (test_bit(PAGE_CLAIMED, &page->private)) {
mm/z3fold.c:	if (test_and_set_bit(NEEDS_COMPACTING, &page->private)) {
mm/z3fold.c:			if (test_and_set_bit(PAGE_CLAIMED, &page->private))
mm/z3fold.c:			if (test_bit(PAGE_HEADLESS, &page->private))
mm/z3fold.c:		list_del_init(&page->lru);
mm/z3fold.c:		if (!test_bit(PAGE_HEADLESS, &page->private)) {
mm/z3fold.c:		if (test_bit(PAGE_HEADLESS, &page->private)) {
mm/z3fold.c:			list_add(&page->lru, &pool->lru);
mm/z3fold.c:			clear_bit(PAGE_CLAIMED, &page->private);
mm/z3fold.c:			list_add(&page->lru, &pool->lru);
mm/z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private))
mm/z3fold.c:		set_bit(MIDDLE_CHUNK_MAPPED, &page->private);
mm/z3fold.c:	if (test_bit(PAGE_HEADLESS, &page->private))
mm/z3fold.c:		clear_bit(MIDDLE_CHUNK_MAPPED, &page->private);
mm/zsmalloc.c: *	page->private: points to zspage
mm/zsmalloc.c: *	page->freelist(index): links together all component pages of a zspage
mm/zsmalloc.c: *	page->units: first object offset in a subpage of zspage
mm/zsmalloc.c: * For every zspage, zspage->freeobj gives head of this list.
mm/zsmalloc.c:	return zspage->isolated;
mm/zsmalloc.c:	return zspage->inuse;
mm/zsmalloc.c:	zspage->inuse = val;
mm/zsmalloc.c:	zspage->inuse += val;
mm/zsmalloc.c:	struct page *first_page = zspage->first_page;
mm/zsmalloc.c:	return page->units;
mm/zsmalloc.c:	page->units = offset;
mm/zsmalloc.c:	return zspage->freeobj;
mm/zsmalloc.c:	zspage->freeobj = obj;
mm/zsmalloc.c:	BUG_ON(zspage->magic != ZSPAGE_MAGIC);
mm/zsmalloc.c:	*fullness = zspage->fullness;
mm/zsmalloc.c:	*class_idx = zspage->class;
mm/zsmalloc.c:	zspage->class = class_idx;
mm/zsmalloc.c:	zspage->fullness = fullness;
mm/zsmalloc.c:			list_add(&zspage->list, &head->list);
mm/zsmalloc.c:	list_add(&zspage->list, &class->fullness_list[fullness]);
mm/zsmalloc.c:	list_del_init(&zspage->list);
mm/zsmalloc.c:	struct zspage *zspage = (struct zspage *)page->private;
mm/zsmalloc.c:	BUG_ON(zspage->magic != ZSPAGE_MAGIC);
mm/zsmalloc.c:	return page->freelist;
mm/zsmalloc.c:		return page->index;
mm/zsmalloc.c:	page->freelist = NULL;
mm/zsmalloc.c:	VM_BUG_ON(list_empty(&zspage->list));
mm/zsmalloc.c:	 * 1. all pages are linked together using page->freelist
mm/zsmalloc.c:	 * 2. each sub-page point to zspage using page->private
mm/zsmalloc.c:		page->freelist = NULL;
mm/zsmalloc.c:			zspage->first_page = page;
mm/zsmalloc.c:			prev_page->freelist = page;
mm/zsmalloc.c:	zspage->magic = ZSPAGE_MAGIC;
mm/zsmalloc.c:		/* record handle to page->index */
mm/zsmalloc.c:		zspage->first_page->index = handle;
mm/zsmalloc.c:	rwlock_init(&zspage->lock);
mm/zsmalloc.c:	read_lock(&zspage->lock);
mm/zsmalloc.c:	read_unlock(&zspage->lock);
mm/zsmalloc.c:	write_lock(&zspage->lock);
mm/zsmalloc.c:	write_unlock(&zspage->lock);
mm/zsmalloc.c:	zspage->isolated++;
mm/zsmalloc.c:	zspage->isolated--;
mm/zsmalloc.c:		newpage->index = oldpage->index;
mm/zsmalloc.c:	if (list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
mm/zsmalloc.c:	if (!list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {
mm/zsmalloc.c:		list_del(&zspage->list);
mm/zswap.c:#include <linux/page-flags.h>
net/9p/trans_xen.c:					      "max-ring-page-order", 0);
net/ceph/messenger.c:	BUG_ON(list_is_last(&cursor->page->lru, &pagelist->head));
net/ceph/pagelist.c:		list_del(&page->lru);
net/ceph/pagelist.c:		list_del(&page->lru);
net/ceph/pagelist.c:	list_add_tail(&page->lru, &pl->head);
net/ceph/pagelist.c:		list_add_tail(&page->lru, &pl->free_list);
net/ceph/pagelist.c:		list_del(&page->lru);
net/ceph/pagelist.c:		list_move_tail(&page->lru, &pl->free_list);
net/core/page_pool.c:#include <linux/page-flags.h>
net/core/page_pool.c:	/* Setup DMA mapping: use page->private for DMA-addr
net/core/page_pool.c:	set_page_private(page, dma); /* page->private = dma; */
net/core/page_pool.c:	 * knowing page is not part of page-cache (thus avoiding a
net/rds/ib_rdma.c:			WARN_ON(!page->mapping && irqs_disabled());
net/rds/rdma.c:			WARN_ON(!page->mapping && irqs_disabled());
net/sunrpc/xdr.c: * xdr_read_pages - Ensure page-based XDR data to decode is aligned at current pointer position
net/sunrpc/xprtrdma/xprt_rdma.h:/* Maximum number of page-sized "segments" per chunk list to be
samples/vfio-mdev/mbochs.c:		dev_info_ratelimited(dev, "%s: framebuffer not page-aligned\n",
scripts/kconfig/lxdialog/textbox.c:		page--;
scripts/kconfig/lxdialog/textbox.c:			page--;
scripts/leaking_addresses.pl:	--page-offset-32-bit=o		Page offset (for 32-bit kernel 0xABCD1234).
scripts/leaking_addresses.pl:	'page-offset-32-bit=o'	=> \$page_offset_32bit,
scripts/leaking_addresses.pl:	printf("\n\t--32-bit or --page-offset-32-bit=<page offset>\n\n");
scripts/leaking_addresses.pl:	# Allow --32-bit or --page-offset-32-bit to override
scripts/leaking_addresses.pl:       # Allow --page-offset-32bit to override.
sound/pci/echoaudio/darla20_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/darla20_dsp.c:	chip->comm_page->gd_clock_state = clock_state;
sound/pci/echoaudio/darla20_dsp.c:	chip->comm_page->gd_spdif_status = spdif_status;
sound/pci/echoaudio/darla20_dsp.c:	chip->comm_page->gd_resampler_state = 3;	/* magic number - should always be 3 */
sound/pci/echoaudio/darla24_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/darla24_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP ? */
sound/pci/echoaudio/darla24_dsp.c:	chip->comm_page->gd_clock_state = clock;
sound/pci/echoaudio/echo3g_dsp.c:	chip->comm_page->e3g_frq_register =
sound/pci/echoaudio/echo3g_dsp.c:	u32 control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/echo3g_dsp.c:				 le32_to_cpu(chip->comm_page->e3g_frq_register),
sound/pci/echoaudio/echoaudio.c:	memcpy(&commpage->audio_format, &commpage_bak->audio_format,
sound/pci/echoaudio/echoaudio.c:		sizeof(commpage->audio_format));
sound/pci/echoaudio/echoaudio.c:	memcpy(&commpage->sglist_addr, &commpage_bak->sglist_addr,
sound/pci/echoaudio/echoaudio.c:		sizeof(commpage->sglist_addr));
sound/pci/echoaudio/echoaudio.c:	memcpy(&commpage->midi_output, &commpage_bak->midi_output,
sound/pci/echoaudio/echoaudio.c:		sizeof(commpage->midi_output));
sound/pci/echoaudio/echoaudio.h:	chip->comm_page->handshake = 0;
sound/pci/echoaudio/echoaudio_3g.c:	chip->comm_page->ext_box_status = cpu_to_le32(E3G_ASIC_NOT_LOADED);
sound/pci/echoaudio/echoaudio_3g.c:	box_status = le32_to_cpu(chip->comm_page->ext_box_status);
sound/pci/echoaudio/echoaudio_3g.c:	return le32_to_cpu(chip->comm_page->e3g_frq_register);
sound/pci/echoaudio/echoaudio_3g.c:	if (ctl_reg != chip->comm_page->control_register ||
sound/pci/echoaudio/echoaudio_3g.c:	    frq_reg != chip->comm_page->e3g_frq_register || force) {
sound/pci/echoaudio/echoaudio_3g.c:		chip->comm_page->e3g_frq_register = frq_reg;
sound/pci/echoaudio/echoaudio_3g.c:		chip->comm_page->control_register = ctl_reg;
sound/pci/echoaudio/echoaudio_3g.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/echoaudio_3g.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/echoaudio_3g.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/echoaudio_3g.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/echoaudio_3g.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/echoaudio_3g.c:	control_reg = le32_to_cpu(chip->comm_page->control_register) &
sound/pci/echoaudio/echoaudio_3g.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/echoaudio_3g.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/echoaudio_dsp.c:		if (chip->comm_page->handshake) {
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->nominal_level_mask |= cpu_to_le32(1 << index);
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->nominal_level_mask &= ~cpu_to_le32(1 << index);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->line_out_level[channel] = gain;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->monitors[monitor_index(chip, output, input)] = gain;
sound/pci/echoaudio/echoaudio_dsp.c:		memset((s8 *)chip->comm_page->vu_meter, ECHOGAIN_MUTED,
sound/pci/echoaudio/echoaudio_dsp.c:		memset((s8 *)chip->comm_page->peak_meter, ECHOGAIN_MUTED,
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->vu_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->peak_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->vu_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->peak_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->vu_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:		meters[n++] = chip->comm_page->peak_meter[m];
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->gd_clock_state = GD_CLOCK_UNDEF;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->gd_spdif_status = GD_SPDIF_STATUS_UNDEF;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->handshake = cpu_to_le32(0xffffffff);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->audio_format[pipe_index] = cpu_to_le16(dsp_format);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->cmd_start |= cpu_to_le32(channel_mask);
sound/pci/echoaudio/echoaudio_dsp.c:	if (chip->comm_page->cmd_start) {
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->cmd_start = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->cmd_stop |= cpu_to_le32(channel_mask);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->cmd_reset = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	if (chip->comm_page->cmd_stop) {
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->cmd_stop = 0;
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->cmd_reset = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->cmd_stop |= cpu_to_le32(channel_mask);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->cmd_reset |= cpu_to_le32(channel_mask);
sound/pci/echoaudio/echoaudio_dsp.c:	if (chip->comm_page->cmd_reset) {
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->cmd_stop = 0;
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->cmd_reset = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->comm_size =
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->handshake = cpu_to_le32(0xffffffff);
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->midi_out_free_count =
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(44100);
sound/pci/echoaudio/echoaudio_dsp.c:	memset(chip->comm_page->monitors, ECHOGAIN_MUTED, MONITOR_ARRAY_SIZE);
sound/pci/echoaudio/echoaudio_dsp.c:	memset(chip->comm_page->vmixer, ECHOGAIN_MUTED, VMIXER_ARRAY_SIZE);
sound/pci/echoaudio/echoaudio_dsp.c:		if (chip->comm_page->midi_input[0])	/* The count is at index 0 */
sound/pci/echoaudio/echoaudio_dsp.c:		chip->comm_page->midi_input[0] = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->position[pipe_index] = 0;
sound/pci/echoaudio/echoaudio_dsp.c:	pipe->dma_counter = (__le32 *)&chip->comm_page->position[pipe_index];
sound/pci/echoaudio/echoaudio_dsp.c:	chip->comm_page->sglist_addr[pipe->index].addr =
sound/pci/echoaudio/echoaudio_gml.c:	if (reg_value != chip->comm_page->control_register || force) {
sound/pci/echoaudio/echoaudio_gml.c:		chip->comm_page->control_register = reg_value;
sound/pci/echoaudio/echoaudio_gml.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/gina20_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/gina20_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/gina20_dsp.c:	chip->comm_page->gd_clock_state = clock_state;
sound/pci/echoaudio/gina20_dsp.c:	chip->comm_page->gd_spdif_status = spdif_status;
sound/pci/echoaudio/gina20_dsp.c:	chip->comm_page->gd_resampler_state = 3;	/* magic number - should always be 3 */
sound/pci/echoaudio/gina20_dsp.c:		chip->comm_page->gd_clock_state = GD_CLOCK_SPDIFIN;
sound/pci/echoaudio/gina20_dsp.c:		chip->comm_page->gd_spdif_status = GD_SPDIF_STATUS_NOCHANGE;
sound/pci/echoaudio/gina20_dsp.c:	chip->comm_page->line_in_level[input] = gain;
sound/pci/echoaudio/gina20_dsp.c:		chip->comm_page->flags |=
sound/pci/echoaudio/gina20_dsp.c:		chip->comm_page->flags &=
sound/pci/echoaudio/gina24_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/gina24_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/gina24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/gina24_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/gina24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register) &
sound/pci/echoaudio/gina24_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/gina24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/indigo_dsp.c:	if (control_reg != le32_to_cpu(chip->comm_page->control_register)) {
sound/pci/echoaudio/indigo_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/indigo_dsp.c:		chip->comm_page->control_register = cpu_to_le32(control_reg);
sound/pci/echoaudio/indigo_dsp.c:	chip->comm_page->vmixer[index] = gain;
sound/pci/echoaudio/indigo_express_dsp.c:	old_control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/indigo_express_dsp.c:		chip->comm_page->control_register = cpu_to_le32(control_reg);
sound/pci/echoaudio/indigo_express_dsp.c:	chip->comm_page->vmixer[index] = gain;
sound/pci/echoaudio/indigodj_dsp.c:	if (control_reg != le32_to_cpu(chip->comm_page->control_register)) {
sound/pci/echoaudio/indigodj_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/indigodj_dsp.c:		chip->comm_page->control_register = cpu_to_le32(control_reg);
sound/pci/echoaudio/indigodj_dsp.c:	chip->comm_page->vmixer[index] = gain;
sound/pci/echoaudio/indigoio_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/indigoio_dsp.c:	chip->comm_page->vmixer[index] = gain;
sound/pci/echoaudio/layla20_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/layla20_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/layla20_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/layla20_dsp.c:	chip->comm_page->input_clock = cpu_to_le16(clock);
sound/pci/echoaudio/layla20_dsp.c:	chip->comm_page->output_clock = cpu_to_le16(clock);
sound/pci/echoaudio/layla20_dsp.c:	chip->comm_page->line_in_level[input] = gain;
sound/pci/echoaudio/layla20_dsp.c:		chip->comm_page->flags |=
sound/pci/echoaudio/layla20_dsp.c:		chip->comm_page->flags &=
sound/pci/echoaudio/layla24_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/layla24_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/layla24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/layla24_dsp.c:		chip->comm_page->sample_rate =
sound/pci/echoaudio/layla24_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP ? */
sound/pci/echoaudio/layla24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register) &
sound/pci/echoaudio/layla24_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/layla24_dsp.c:		monitors = kmemdup(chip->comm_page->monitors,
sound/pci/echoaudio/layla24_dsp.c:		memset(chip->comm_page->monitors, ECHOGAIN_MUTED,
sound/pci/echoaudio/layla24_dsp.c:			memcpy(chip->comm_page->monitors, monitors,
sound/pci/echoaudio/layla24_dsp.c:		memcpy(chip->comm_page->monitors, monitors, MONITOR_ARRAY_SIZE);
sound/pci/echoaudio/layla24_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/mia_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/mia_dsp.c:	if (control_reg != le32_to_cpu(chip->comm_page->control_register)) {
sound/pci/echoaudio/mia_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/mia_dsp.c:		chip->comm_page->control_register = cpu_to_le32(control_reg);
sound/pci/echoaudio/mia_dsp.c:	chip->comm_page->vmixer[index] = gain;
sound/pci/echoaudio/mia_dsp.c:		chip->comm_page->flags |=
sound/pci/echoaudio/mia_dsp.c:		chip->comm_page->flags &=
sound/pci/echoaudio/midi.c:		chip->comm_page->flags |=
sound/pci/echoaudio/midi.c:		chip->comm_page->flags &=
sound/pci/echoaudio/midi.c:	chip->comm_page->midi_output[0] = bytes;
sound/pci/echoaudio/midi.c:	memcpy(&chip->comm_page->midi_output[1], data, bytes);
sound/pci/echoaudio/midi.c:	chip->comm_page->midi_out_free_count = 0;
sound/pci/echoaudio/midi.c:	count = le16_to_cpu(chip->comm_page->midi_input[0]);
sound/pci/echoaudio/midi.c:		midi_byte = le16_to_cpu(chip->comm_page->midi_input[i]);
sound/pci/echoaudio/mona_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/mona_dsp.c:		chip->comm_page->sample_rate = cpu_to_le32(rate);
sound/pci/echoaudio/mona_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/echoaudio/mona_dsp.c:	chip->comm_page->sample_rate = cpu_to_le32(rate);	/* ignored by the DSP */
sound/pci/echoaudio/mona_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register) &
sound/pci/echoaudio/mona_dsp.c:	clocks_from_dsp = le32_to_cpu(chip->comm_page->status_clocks);
sound/pci/echoaudio/mona_dsp.c:	control_reg = le32_to_cpu(chip->comm_page->control_register);
sound/pci/emu10k1/memory.c:			last_page--; /* last page was already allocated */
sound/xen/xen_snd_front_evtchnl.c:	prod = page->in_prod;
sound/xen/xen_snd_front_evtchnl.c:	if (prod == page->in_cons)
sound/xen/xen_snd_front_evtchnl.c:	for (cons = page->in_cons; cons != prod; cons++) {
sound/xen/xen_snd_front_evtchnl.c:	page->in_cons = cons;
tools/arch/s390/include/uapi/asm/sie.h:	{ 0x258, "DIAG (0x258) page-reference services" },	\
tools/include/uapi/drm/i915_drm.h:	 * The (page-aligned) allocated size for the object will be returned.
tools/include/uapi/drm/i915_drm.h:	 * The value will be page-aligned.
tools/perf/Documentation/Makefile:MANPAGE_XSL = manpage-normal.xsl
tools/perf/Documentation/Makefile:MANPAGE_XSL = manpage-1.72.xsl
tools/perf/Documentation/Makefile:XMLTO_EXTRA += -m manpage-bold-literal.xsl
tools/perf/Documentation/Makefile:XMLTO_EXTRA += -m manpage-suppress-sp.xsl
tools/perf/Documentation/asciidoc.conf:# Usage: linkperf:command[manpage-section]
tools/perf/Documentation/manpage-1.72.xsl:<!-- manpage-1.72.xsl:
tools/perf/Documentation/manpage-1.72.xsl:<xsl:import href="manpage-base.xsl"/>
tools/perf/Documentation/manpage-base.xsl:<!-- manpage-base.xsl:
tools/perf/Documentation/manpage-bold-literal.xsl:<!-- manpage-bold-literal.xsl:
tools/perf/Documentation/manpage-normal.xsl:<!-- manpage-normal.xsl:
tools/perf/Documentation/manpage-normal.xsl:<xsl:import href="manpage-base.xsl"/>
tools/perf/Documentation/manpage-suppress-sp.xsl:<!-- manpage-suppress-sp.xsl:
tools/perf/Documentation/perf-stat.txt:           3,228,188      page-faults:u             #    0.039 M/sec
tools/perf/builtin-stat.c:            17,318 page-faults               #    0.010 M/sec
tools/perf/pmu-events/arch/x86/bonnell/virtual-memory.json:        "BriefDescription": "Number of page-walks executed."
tools/perf/pmu-events/arch/x86/bonnell/virtual-memory.json:        "BriefDescription": "Duration of page-walks in core cycles"
tools/perf/pmu-events/arch/x86/broadwell/pipeline.json:        "PublicDescription": "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide.",
tools/perf/pmu-events/arch/x86/broadwellde/pipeline.json:        "PublicDescription": "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide.",
tools/perf/pmu-events/arch/x86/broadwellx/pipeline.json:        "PublicDescription": "This event counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:\n - preceding store conflicts with the load (incomplete overlap);\n - store forwarding is impossible due to u-arch limitations;\n - preceding lock RMW operations are not forwarded;\n - store has the no-forward bit set (uncacheable/page-split/masked stores);\n - all-blocking stores are used (mostly, fences and port I/O);\nand others.\nThe most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events.\nSee the table of not supported store forwards in the Optimization Guide.",
tools/perf/pmu-events/arch/x86/cascadelakex/pipeline.json:        "PublicDescription": "Counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:a. preceding store conflicts with the load (incomplete overlap),b. store forwarding is impossible due to u-arch limitations,c. preceding lock RMW operations are not forwarded,d. store has the no-forward bit set (uncacheable/page-split/masked stores),e. all-blocking stores are used (mostly, fences and port I/O), and others.The most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events. See the table of not supported store forwards in the Optimization Guide.",
tools/perf/pmu-events/arch/x86/goldmont/virtual-memory.json:        "BriefDescription": "Duration of D-side page-walks in cycles"
tools/perf/pmu-events/arch/x86/goldmont/virtual-memory.json:        "PublicDescription": "Counts every core cycle a page-walk is in progress due to either a data memory operation or an instruction fetch.",
tools/perf/pmu-events/arch/x86/goldmont/virtual-memory.json:        "BriefDescription": "Duration of page-walks in cycles"
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "BriefDescription": "D-side page-walks",
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "PublicDescription": "This event counts every cycle when a D-side (walks due to a load) page walk is in progress. Page walk duration divided by number of page walks is the average duration of page-walks.",
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "BriefDescription": "Duration of D-side page-walks in core cycles"
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "BriefDescription": "I-side page-walks",
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "PublicDescription": "This event counts every cycle when a I-side (walks due to an instruction fetch) page walk is in progress. Page walk duration divided by number of page walks is the average duration of page-walks.",
tools/perf/pmu-events/arch/x86/silvermont/virtual-memory.json:        "BriefDescription": "Duration of I-side page-walks in core cycles"
tools/perf/pmu-events/arch/x86/skylake/pipeline.json:        "PublicDescription": "Counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:a. preceding store conflicts with the load (incomplete overlap),b. store forwarding is impossible due to u-arch limitations,c. preceding lock RMW operations are not forwarded,d. store has the no-forward bit set (uncacheable/page-split/masked stores),e. all-blocking stores are used (mostly, fences and port I/O), and others.The most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events. See the table of not supported store forwards in the Optimization Guide.",
tools/perf/pmu-events/arch/x86/skylakex/pipeline.json:        "PublicDescription": "Counts how many times the load operation got the true Block-on-Store blocking code preventing store forwarding. This includes cases when:a. preceding store conflicts with the load (incomplete overlap),b. store forwarding is impossible due to u-arch limitations,c. preceding lock RMW operations are not forwarded,d. store has the no-forward bit set (uncacheable/page-split/masked stores),e. all-blocking stores are used (mostly, fences and port I/O), and others.The most common case is a load blocked due to its address range overlapping with a preceding smaller uncompleted store. Note: This event does not take into account cases of out-of-SW-control (for example, SbTailHit), unknown physical STA, and cases of blocking loads on store due to being non-WB memory type or a lock. These cases are covered by other events. See the table of not supported store forwards in the Optimization Guide.",
tools/perf/util/evsel.c:	"page-faults",
tools/perf/util/parse-events.c:		.symbol = "page-faults",
tools/perf/util/parse-events.l:page-faults|faults				{ return sym(yyscanner, PERF_TYPE_SOFTWARE, PERF_COUNT_SW_PAGE_FAULTS); }
tools/testing/radix-tree/regression1.c:		pthread_mutex_lock(&page->lock);
tools/testing/radix-tree/regression1.c:		if (!page->count)
tools/testing/radix-tree/regression1.c:		pthread_mutex_unlock(&page->lock);
tools/testing/radix-tree/regression1.c:		pthread_mutex_unlock(&page->lock);
tools/testing/radix-tree/test.c:/* Use the same pattern as tag_pages_for_writeback() in mm/page-writeback.c */
tools/testing/selftests/memfd/fuse_test.c:	 * implement page-replacements or other fancy ways to avoid racing
tools/testing/selftests/memfd/fuse_test.c:	 * then the kernel did a page-replacement or canceled the read() (or
tools/testing/selftests/vm/.gitignore:hugepage-mmap
tools/testing/selftests/vm/.gitignore:hugepage-shm
tools/testing/selftests/vm/Makefile:TEST_GEN_FILES += hugepage-mmap
tools/testing/selftests/vm/Makefile:TEST_GEN_FILES += hugepage-shm
tools/testing/selftests/vm/hugepage-mmap.c: * hugepage-mmap:
tools/testing/selftests/vm/hugepage-shm.c: * hugepage-shm:
tools/testing/selftests/vm/run_vmtests:echo "running hugepage-mmap"
tools/testing/selftests/vm/run_vmtests:./hugepage-mmap
tools/testing/selftests/vm/run_vmtests:echo "running hugepage-shm"
tools/testing/selftests/vm/run_vmtests:./hugepage-shm
tools/testing/selftests/vm/userfaultfd.c:	uffd_test_ops->alias_mapping(&uffdio_zeropage->range.start,
tools/testing/selftests/vm/userfaultfd.c:				     uffdio_zeropage->range.len,
tools/testing/selftests/vm/userfaultfd.c:		if (uffdio_zeropage->zeropage != -EEXIST)
tools/testing/selftests/vm/userfaultfd.c:				uffdio_zeropage->zeropage), exit(1);
tools/testing/selftests/vm/userfaultfd.c:			uffdio_zeropage->zeropage), exit(1);
tools/testing/selftests/x86/protection_keys.c:	 * huge-page-aligned sizes when operating on hugetlbfs.
tools/testing/selftests/x86/protection_keys.c:	/* lots_o_noops_around_write should be page-aligned already */
tools/vm/.gitignore:page-types
tools/vm/Makefile:TARGETS=page-types slabinfo page_owner_sort
tools/vm/Makefile:	$(RM) page-types slabinfo page_owner_sort
tools/vm/page-types.c: * page-types: Tool for querying page flags
tools/vm/page-types.c:#include "../../include/uapi/linux/kernel-page-flags.h"
tools/vm/page-types.c:	printf("             flags\tpage-count       MB"
tools/vm/page-types.c:"page-types [options]\n"
